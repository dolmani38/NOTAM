{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNRG1CGmkTZ96g1cI/V5IBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/NOTAM/blob/main/albert_notam_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tlIhtCjJia"
      },
      "source": [
        "# Albert NOTAM 언어 모델 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc1weyK_jPwQ",
        "outputId": "a1465530-2f76-426d-a9e1-65b40839182b"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5DRavVNRey3"
      },
      "source": [
        "## 참조\n",
        "\n",
        "https://colab.research.google.com/github/parmarsuraj99/suraj-parmar/blob/master/_notebooks/2020-05-02-SanskritALBERT.ipynb#scrollTo=VNAOMXjpMHZD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038IYm33Muol"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oVCIQF4UMmyF",
        "outputId": "1033826f-7ac4-4877-ed82-493198b78864"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/.\n",
        "!pip install sentencepiece==0.1.95\n",
        "!pip install datasets==1.8.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 80215, done.\u001b[K\n",
            "remote: Counting objects: 100% (1086/1086), done.\u001b[K\n",
            "remote: Compressing objects: 100% (565/565), done.\u001b[K\n",
            "remote: Total 80215 (delta 652), reused 777 (delta 452), pack-reused 79129\u001b[K\n",
            "Receiving objects: 100% (80215/80215), 63.35 MiB | 28.34 MiB/s, done.\n",
            "Resolving deltas: 100% (57253/57253), done.\n",
            "Processing ./transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.6.3)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 15.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.10.0.dev0-py3-none-any.whl size=2644547 sha256=0c89d77331a0d39e2d94cce594c32adf93b0b72de72a941d30ab4de58e77d2ea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eotp5bfy/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.15 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0.dev0\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 9.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting datasets==1.8.0\n",
            "  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (4.6.3)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 87.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.1.5)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.0.15)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.3.4)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 95.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.8.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.8.0) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.8.0) (1.15.0)\n",
            "Installing collected packages: tqdm, xxhash, fsspec, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed datasets-1.8.0 fsspec-2021.7.0 tqdm-4.49.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_K0rsDlckzd"
      },
      "source": [
        "# NOTAM Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWN53GjXcqSa",
        "outputId": "231e68f7-4f68-4c98-bfe1-f759c0ee571e"
      },
      "source": [
        "%%time\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/NOTAM/data/TRAIN_210812.csv')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 27.4 ms, sys: 12 ms, total: 39.4 ms\n",
            "Wall time: 533 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "MPBJvnYscutM",
        "outputId": "1c587e02-ae30-4c46-a232-222c4a63615f"
      },
      "source": [
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSG_ID</th>\n",
              "      <th>UPDATED_BY</th>\n",
              "      <th>UPDATED_DATE</th>\n",
              "      <th>E_SCORE</th>\n",
              "      <th>AE_SCORE</th>\n",
              "      <th>ALL_SCORE</th>\n",
              "      <th>Q_LINE</th>\n",
              "      <th>A_LINE</th>\n",
              "      <th>B_LINE</th>\n",
              "      <th>C_LINE</th>\n",
              "      <th>D_LINE</th>\n",
              "      <th>E_LINE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6824038</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:08</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6824038</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6816851</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:09</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6816851</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:18</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6816333</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:11</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KZOB/QMXLC/IV/BO/A/000/999/4213N08321W005</td>\n",
              "      <td>KDTW</td>\n",
              "      <td>1905211704</td>\n",
              "      <td>1905211800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY K5 CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10410</th>\n",
              "      <td>15966850</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-08-01 10:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZNY/QMXLC/IV/BO/A/000/999/4038N07347W005</td>\n",
              "      <td>KJFK</td>\n",
              "      <td>1912311421</td>\n",
              "      <td>1912311600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY C BTN TWY C1 AND TWY V CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10411</th>\n",
              "      <td>15947262</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:06</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>LCCC/QWFLW/IV/M  /W /210/230/3335N03202E133</td>\n",
              "      <td>LCCC</td>\n",
              "      <td>2001040820</td>\n",
              "      <td>2001051150</td>\n",
              "      <td>0820-1150</td>\n",
              "      <td>AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10412</th>\n",
              "      <td>15962821</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:48</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZAU/QMRLC/IV/NBO/A/000/999/4257N08754W005</td>\n",
              "      <td>KMKE</td>\n",
              "      <td>1912310929</td>\n",
              "      <td>2001012359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 01L/19R CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10413</th>\n",
              "      <td>15965583</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-06 13:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>2001071500</td>\n",
              "      <td>2001072000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10414</th>\n",
              "      <td>15962570</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-08 14:49</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>1912311430</td>\n",
              "      <td>1912312100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10415 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         MSG_ID  ...                                             E_LINE\n",
              "0       6824038  ...  AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...\n",
              "1       6824038  ...  AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...\n",
              "2       6816851  ...  LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...\n",
              "3       6816851  ...  LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...\n",
              "4       6816333  ...                                        TWY K5 CLSD\n",
              "...         ...  ...                                                ...\n",
              "10410  15966850  ...                    TWY C BTN TWY C1 AND TWY V CLSD\n",
              "10411  15947262  ...  AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...\n",
              "10412  15962821  ...                                   RWY 01L/19R CLSD\n",
              "10413  15965583  ...  TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...\n",
              "10414  15962570  ...  TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...\n",
              "\n",
              "[10415 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAyAjbU1eBWU"
      },
      "source": [
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    #txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    #txt = txt.replace('..','')\n",
        "    #txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmFGrzRQdi_l"
      },
      "source": [
        "notam_contents = []\n",
        "\n",
        "for row in df.iterrows():\n",
        "    doc_cont = str(row[1][11])\n",
        "    notam_contents.append(clean_text(doc_cont.lower()))\n",
        "    notam_contents.append(clean_text(str(row[1][6]).lower()))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfnVFX03d1Dn",
        "outputId": "957a4dd7-2999-41ae-8406-d519a2f76664"
      },
      "source": [
        "len(notam_contents)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20830"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f16NAw2Id3ei",
        "outputId": "583257a0-a9a2-4fc3-b1e3-7b11fdce2a2e"
      },
      "source": [
        "notam_contents[3]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yuxx/qoaxx/iv/bo/e/000/999/2537s13421e005'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vr5SWcZpZ00"
      },
      "source": [
        "f = open('/content/drive/MyDrive/NOTAM/train_tokenizer2.txt', 'r')\n",
        "while True:\n",
        "    line = f.readline()\n",
        "    if not line: break\n",
        "    notam_contents.append(clean_text(line.lower()))\n",
        "f.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm0Z7smipl3J",
        "outputId": "58f6a75f-29d9-4ecc-a53a-8ecb3f02738b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(notam_contents)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37602"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzHDt-FVdW6P",
        "outputId": "6deaae33-ba1a-4b84-e370-f735768d5255"
      },
      "source": [
        "%%time\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('/content/drive/MyDrive/NOTAM/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in notam_contents:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 16 ms, sys: 4.81 ms, total: 20.8 ms\n",
            "Wall time: 34 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bNszLMewp4"
      },
      "source": [
        "# Tokenizer 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGBDbPNwy1X-",
        "outputId": "b8048d4d-c915-4aeb-e77e-4e504c4b3794"
      },
      "source": [
        "%%time\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "# spm_train --input=data/train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
        "\n",
        "input_file = '/content/drive/MyDrive/NOTAM/train_tokenizer.txt'\n",
        "vocab_size = 21770\n",
        "\n",
        "sp_model_root='/content/drive/MyDrive/NOTAM/albert_tokenizer_model'\n",
        "if not os.path.isdir(sp_model_root):\n",
        "    os.mkdir(sp_model_root)\n",
        "sp_model_name = 'spiece'\n",
        "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
        "model_type = 'unigram'  # 학습할 모델 선택, unigram이 더 성능이 좋음'bpe'\n",
        "character_coverage  = 1.0  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
        "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "print('train done')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train done\n",
            "CPU times: user 2min 9s, sys: 6.75 s, total: 2min 15s\n",
            "Wall time: 2min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23ogFfIztOR",
        "outputId": "fdf7073c-969f-41a8-d272-2f7522ce9336"
      },
      "source": [
        "## check\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(sp_model_path))\n",
        "\n",
        "tokens = sp.encode_as_pieces(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "ids = sp.encode_as_ids(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "\n",
        "print(ids)\n",
        "print(tokens)\n",
        "\n",
        "tokens = sp.decode_pieces(tokens)\n",
        "ids = sp.decode_ids(ids)\n",
        "\n",
        "print(ids)\n",
        "print(tokens)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2960, 158, 325, 150, 934, 121, 6471, 6891, 4465, 287, 5526, 227, 537, 784, 2603, 727, 158, 722, 13632, 11285, 124, 156, 482, 1503, 123, 3055, 203, 124, 9543, 934, 121, 6471, 6891, 4465, 287, 5526, 227, 537, 784, 2603, 727, 124, 9543, 244, 158, 722, 13632, 11285, 124, 156, 727, 309, 762, 542, 230, 2960, 133, 3530, 16070, 1695, 123, 493, 295, 217, 123]\n",
            "['▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'au', 'sda', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'au', 'sda', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'af', '▁intran', 'et', '.', '▁mil', '▁use', '▁only', '.']\n",
            "ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\n",
            "ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXWYH5FRyu1"
      },
      "source": [
        "# 여기서부터 다시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55T0J0MDFtrm",
        "outputId": "3fc06257-d285-45b8-d04d-6078e8e297f9"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF_Q_0VTFzPv",
        "outputId": "c60154dd-ac85-4874-f4a1-65f9ca1bd2a9"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "#'/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "albet_tokenizer_model = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model'\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file /content/drive/MyDrive/NOTAM/albert_tokenizer_model/config.json not found\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "file /content/drive/MyDrive/NOTAM/albert_tokenizer_model/config.json not found\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "QB0SlhA4GMQQ",
        "outputId": "a9decc68-bc23-4540-9563-65e3535dd759"
      },
      "source": [
        "op = tokenizer.encode(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "print(op)\n",
        "tokenizer.decode(op)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 2960, 158, 325, 150, 934, 121, 6471, 6891, 4465, 287, 5526, 227, 537, 784, 2603, 727, 158, 722, 13632, 11285, 124, 156, 482, 1503, 123, 3055, 203, 124, 9543, 934, 121, 6471, 6891, 4465, 287, 5526, 227, 537, 784, 2603, 727, 124, 9543, 244, 158, 722, 13632, 11285, 124, 156, 727, 309, 762, 542, 230, 2960, 133, 3530, 16070, 1695, 123, 493, 295, 217, 123, 6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMDfH8GZF_mK",
        "outputId": "2962a51b-0181-4b2d-948b-b61e0c42ad13"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer.all_special_tokens[:20])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inr3cDwQG6Fs",
        "outputId": "24e8c214-c7f9-4b92-f01a-ad7f90e38a7e"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer.save_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/spiece.model',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/added_tokens.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhlWrXXHr6L"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHhYsZbH3zr",
        "outputId": "6ec5dc83-6d22-461c-a45e-a45218e5b5c6"
      },
      "source": [
        "op = tokenizer(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    5,  2960,   158,   325,   150,   934,   121,  6471,  6891,  4465,\n",
            "           287,  5526,   227,   537,   784,  2603,   727,   158,   722, 13632,\n",
            "         11285,   124,   156,   482,  1503,   123,  3055,   203,   124,  9543,\n",
            "           934,   121,  6471,  6891,  4465,   287,  5526,   227,   537,   784,\n",
            "          2603,   727,   124,  9543,   244,   158,   722, 13632, 11285,   124,\n",
            "           156,   727,   309,   762,   542,   230,  2960,   133,  3530, 16070,\n",
            "          1695,   123,   493,   295,   217,   123,     6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'au', 'sda', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'au', 'sda', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'af', '▁intran', 'et', '.', '▁mil', '▁use', '▁only', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 2960, 158, 325, 150, 934, 121, 6471, 6891, 4465, 287, 5526, 227, 537, 784, 2603, 727, 158, 722, 13632, 11285, 124, 156, 482, 1503, 123, 3055, 203, 124, 9543, 934, 121, 6471, 6891, 4465, 287, 5526, 227, 537, 784, 2603, 727, 124, 9543, 244, 158, 722, 13632, 11285, 124, 156, 727, 309, 762, 542, 230, 2960, 133, 3530, 16070, 1695, 123, 493, 295, 217, 123, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paElX_hhJAoh",
        "outputId": "4edbd5ae-d225-423b-a521-384161c4ad40"
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9RPI1UerOSCB",
        "outputId": "8c6ca1f2-2f2f-47da-d7bd-cbd87b821cfe"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "        \"AlbertModel\"\n",
        "    ],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": vocab_size\n",
        "}\n",
        "with open(albet_tokenizer_model + \"_special/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "\n",
        "#Configuration for tokenizer.\n",
        "#Note: I set do_lower_case: False, and keep_accents:True\n",
        "# Opening JSON file\n",
        "f = open(albet_tokenizer_model+ \"_special/tokenizer_config.json\")\n",
        "   \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "tokenizer_config = json.load(f)\n",
        "\n",
        "tokenizer_config['max_len'] = 512\n",
        "tokenizer_config['model_type'] = 'albert'\n",
        "tokenizer_config['do_lower_case'] = False\n",
        "tokenizer_config['keep_accents'] = True\n",
        "\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as outfile:\n",
        "    json.dump(tokenizer_config, outfile)\n",
        "'''\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"do_lower_case\":False, \n",
        "\t\"keep_accents\":True\n",
        "}\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntokenizer_config = {\\n\\t\"max_len\": 512,\\n\\t\"model_type\": \"albert\",\\n\\t\"do_lower_case\":False, \\n\\t\"keep_accents\":True\\n}\\nwith open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", \\'w\\') as fp:\\n    json.dump(tokenizer_config, fp)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdjOOoG6QX2p",
        "outputId": "66293c57-c5bc-4499-847a-be9407eb158f"
      },
      "source": [
        "\n",
        "#To train from scratch\n",
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special \\\n",
        "        --tokenizer_name /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special \\\n",
        "        --train_file /content/drive/MyDrive/NOTAM/train_tokenizer.txt \\\n",
        "        --output_dir /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model \\\n",
        "        --use_fast_tokenizer \\\n",
        "        --do_train \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 500 \\\n",
        "        --logging_steps 500 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --num_train_epochs 20 \\\n",
        "        --seed 108 \\\n",
        "        --overwrite_output_dir \\\n",
        "        --logging_dir /content/drive/MyDrive/Tokenizer_train/logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-14 08:05:50.300670: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "08/14/2021 08:05:52 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/14/2021 08:05:52 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/Tokenizer_train/logs,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=20.0,\n",
            "output_dir=/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=model,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=108,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "08/14/2021 08:05:52 - WARNING - datasets.builder - Using custom data configuration default-da24a5138541ad5b\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374126992 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374126992 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374126800 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "100% 1/1 [00:00<00:00, 3013.15it/s]\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 1/1 [00:00<00:00, 187.78it/s]\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/14/2021 08:05:52 - INFO - datasets.builder - Generating split train\n",
            "08/14/2021 08:05:52 - INFO - datasets.arrow_writer - Done writing 37602 examples in 16034584 bytes /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.incomplete/text-train.arrow.\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885382897424 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.incomplete.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885382897424 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.incomplete.lock\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374126800 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.builder - Constructing Dataset for split train, from /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "100% 1/1 [00:00<00:00, 494.96it/s]\n",
            "08/14/2021 08:05:52 - WARNING - datasets.builder - Using custom data configuration default-da24a5138541ad5b\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374127760 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "08/14/2021 08:05:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374127760 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374793168 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "08/14/2021 08:05:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/14/2021 08:05:52 - INFO - datasets.utils.filelock - Lock 139885374793168 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:52 - INFO - datasets.builder - Constructing Dataset for split train[:5%], from /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/14/2021 08:05:53 - WARNING - datasets.builder - Using custom data configuration default-da24a5138541ad5b\n",
            "08/14/2021 08:05:53 - INFO - datasets.utils.filelock - Lock 139885221601680 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:53 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "08/14/2021 08:05:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/14/2021 08:05:53 - INFO - datasets.utils.filelock - Lock 139885221601680 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:53 - INFO - datasets.utils.filelock - Lock 139885374101328 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:53 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "08/14/2021 08:05:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/14/2021 08:05:53 - INFO - datasets.utils.filelock - Lock 139885374101328 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-da24a5138541ad5b_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/14/2021 08:05:53 - INFO - datasets.builder - Constructing Dataset for split train[5%:], from /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "[INFO|configuration_utils.py:543] 2021-08-14 08:05:53,070 >> loading configuration file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/config.json\n",
            "[INFO|configuration_utils.py:581] 2021-08-14 08:05:53,071 >> Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 21770\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-14 08:05:53,075 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-14 08:05:53,075 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-14 08:05:53,075 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-14 08:05:53,076 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-14 08:05:53,076 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer_config.json\n",
            "08/14/2021 08:05:53 - INFO - __main__ - Training new model from scratch\n",
            "Running tokenizer on dataset line_by_line:   0% 0/36 [00:00<?, ?ba/s]08/14/2021 08:05:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-c19bb37cb452973c.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 36/36 [00:03<00:00, 11.52ba/s]\n",
            "08/14/2021 08:05:56 - INFO - datasets.arrow_writer - Done writing 35716 examples in 8598514 bytes /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/tmpw77zf4fs.\n",
            "Running tokenizer on dataset line_by_line:   0% 0/2 [00:00<?, ?ba/s]08/14/2021 08:05:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-69386f0b4e3580bb.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 2/2 [00:00<00:00, 10.49ba/s]\n",
            "08/14/2021 08:05:56 - INFO - datasets.arrow_writer - Done writing 1880 examples in 315436 bytes /root/.cache/huggingface/datasets/text/default-da24a5138541ad5b/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/tmpvaeldl2c.\n",
            "[INFO|trainer.py:528] 2021-08-14 08:06:02,996 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1171] 2021-08-14 08:06:03,002 >> ***** Running training *****\n",
            "[INFO|trainer.py:1172] 2021-08-14 08:06:03,002 >>   Num examples = 35716\n",
            "[INFO|trainer.py:1173] 2021-08-14 08:06:03,002 >>   Num Epochs = 20\n",
            "[INFO|trainer.py:1174] 2021-08-14 08:06:03,002 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1175] 2021-08-14 08:06:03,002 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1176] 2021-08-14 08:06:03,002 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1177] 2021-08-14 08:06:03,002 >>   Total optimization steps = 89300\n",
            "{'loss': 8.5644, 'learning_rate': 4.972004479283315e-05, 'epoch': 0.11}\n",
            "  1% 500/89300 [00:28<1:10:53, 20.88it/s][INFO|trainer.py:1928] 2021-08-14 08:06:31,753 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:06:31,757 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:06:31,874 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:06:31,877 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:06:31,880 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:06:32,149 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48500] due to args.save_total_limit\n",
            "{'loss': 7.0711, 'learning_rate': 4.944008958566629e-05, 'epoch': 0.22}\n",
            "  1% 1000/89300 [00:55<1:09:14, 21.25it/s][INFO|trainer.py:1928] 2021-08-14 08:06:58,655 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:06:58,658 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:06:58,774 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:06:58,778 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:06:58,781 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:06:59,057 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49000] due to args.save_total_limit\n",
            "{'loss': 6.7743, 'learning_rate': 4.916013437849945e-05, 'epoch': 0.34}\n",
            "  2% 1500/89300 [01:23<1:37:44, 14.97it/s][INFO|trainer.py:1928] 2021-08-14 08:07:26,892 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:07:26,896 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:07:27,009 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:07:27,013 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:07:27,015 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:07:27,289 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 6.5057, 'learning_rate': 4.888017917133259e-05, 'epoch': 0.45}\n",
            "  2% 2000/89300 [01:52<1:33:04, 15.63it/s][INFO|trainer.py:1928] 2021-08-14 08:07:56,215 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:07:56,220 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:07:56,337 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:07:56,341 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:07:56,343 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:07:56,626 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 6.3856, 'learning_rate': 4.860022396416574e-05, 'epoch': 0.56}\n",
            "  3% 2500/89300 [02:20<1:10:25, 20.54it/s][INFO|trainer.py:1928] 2021-08-14 08:08:24,200 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:08:24,204 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:08:24,334 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:08:24,337 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:08:24,340 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:08:24,604 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 6.1973, 'learning_rate': 4.832026875699888e-05, 'epoch': 0.67}\n",
            "  3% 3000/89300 [02:49<1:06:37, 21.59it/s][INFO|trainer.py:1928] 2021-08-14 08:08:52,416 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:08:52,420 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:08:52,539 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:08:52,543 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:08:52,546 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:08:52,844 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 6.0643, 'learning_rate': 4.804031354983203e-05, 'epoch': 0.78}\n",
            "  4% 3500/89300 [03:17<1:09:52, 20.47it/s][INFO|trainer.py:1928] 2021-08-14 08:09:20,441 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:09:20,445 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:09:20,560 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:09:20,563 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:09:20,566 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:09:20,843 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 5.9159, 'learning_rate': 4.7760358342665176e-05, 'epoch': 0.9}\n",
            "  4% 4000/89300 [03:45<1:35:12, 14.93it/s][INFO|trainer.py:1928] 2021-08-14 08:09:48,541 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:09:48,546 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:09:48,660 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:09:48,664 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:09:48,666 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:09:48,928 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 5.7965, 'learning_rate': 4.7480403135498324e-05, 'epoch': 1.01}\n",
            "  5% 4500/89300 [04:14<1:31:57, 15.37it/s][INFO|trainer.py:1928] 2021-08-14 08:10:17,347 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:10:17,351 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:10:17,467 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:10:17,471 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:10:17,474 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:10:17,759 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 5.6027, 'learning_rate': 4.720044792833147e-05, 'epoch': 1.12}\n",
            "  6% 5000/89300 [04:43<1:23:11, 16.89it/s][INFO|trainer.py:1928] 2021-08-14 08:10:46,571 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:10:46,575 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:10:46,693 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:10:46,696 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:10:46,700 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:10:46,975 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 5.5689, 'learning_rate': 4.6920492721164614e-05, 'epoch': 1.23}\n",
            "  6% 5500/89300 [05:10<1:00:52, 22.95it/s][INFO|trainer.py:1928] 2021-08-14 08:11:14,188 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:11:14,192 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:11:14,313 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:11:14,317 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:11:14,319 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:11:14,597 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 5.4363, 'learning_rate': 4.664053751399776e-05, 'epoch': 1.34}\n",
            "  7% 6000/89300 [05:39<1:34:40, 14.66it/s][INFO|trainer.py:1928] 2021-08-14 08:11:42,564 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:11:42,568 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:11:42,675 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:11:42,679 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:11:42,682 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:11:42,948 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 5.2127, 'learning_rate': 4.6360582306830904e-05, 'epoch': 1.46}\n",
            "  7% 6500/89300 [06:06<1:22:11, 16.79it/s][INFO|trainer.py:1928] 2021-08-14 08:12:09,866 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:12:09,870 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:12:09,987 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:12:09,991 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:12:09,994 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:12:10,263 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500] due to args.save_total_limit\n",
            "{'loss': 5.2242, 'learning_rate': 4.608062709966405e-05, 'epoch': 1.57}\n",
            "  8% 7000/89300 [06:34<1:18:46, 17.41it/s][INFO|trainer.py:1928] 2021-08-14 08:12:38,274 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:12:38,278 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:12:38,405 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:12:38,409 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:12:38,412 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:12:38,693 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 5.0992, 'learning_rate': 4.580067189249721e-05, 'epoch': 1.68}\n",
            "  8% 7500/89300 [07:04<1:10:29, 19.34it/s][INFO|trainer.py:1928] 2021-08-14 08:13:07,412 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:13:07,417 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:13:07,536 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:13:07,540 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:13:07,543 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:13:07,827 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500] due to args.save_total_limit\n",
            "{'loss': 4.9712, 'learning_rate': 4.552071668533035e-05, 'epoch': 1.79}\n",
            "  9% 8000/89300 [07:31<1:07:45, 20.00it/s][INFO|trainer.py:1928] 2021-08-14 08:13:34,359 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:13:34,363 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:13:34,475 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:13:34,479 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:13:34,483 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:13:34,765 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000] due to args.save_total_limit\n",
            "{'loss': 4.9237, 'learning_rate': 4.52407614781635e-05, 'epoch': 1.9}\n",
            " 10% 8500/89300 [07:58<1:22:35, 16.31it/s][INFO|trainer.py:1928] 2021-08-14 08:14:02,033 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:14:02,038 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:14:02,156 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:14:02,160 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:14:02,164 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:14:02,428 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500] due to args.save_total_limit\n",
            "{'loss': 4.8959, 'learning_rate': 4.496080627099664e-05, 'epoch': 2.02}\n",
            " 10% 9000/89300 [08:28<1:21:04, 16.51it/s][INFO|trainer.py:1928] 2021-08-14 08:14:31,702 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:14:31,706 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:14:31,819 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:14:31,823 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:14:31,826 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:14:32,085 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 4.813, 'learning_rate': 4.468085106382979e-05, 'epoch': 2.13}\n",
            " 11% 9500/89300 [08:56<1:03:04, 21.09it/s][INFO|trainer.py:1928] 2021-08-14 08:14:59,865 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:14:59,870 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:15:00,005 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:15:00,009 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:15:00,012 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:15:00,297 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500] due to args.save_total_limit\n",
            "{'loss': 4.695, 'learning_rate': 4.4400895856662936e-05, 'epoch': 2.24}\n",
            " 11% 10000/89300 [09:23<1:06:49, 19.78it/s][INFO|trainer.py:1928] 2021-08-14 08:15:26,834 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:15:26,839 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:15:26,954 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:15:26,957 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:15:26,960 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:15:27,239 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000] due to args.save_total_limit\n",
            "{'loss': 4.6752, 'learning_rate': 4.4120940649496084e-05, 'epoch': 2.35}\n",
            " 12% 10500/89300 [09:51<1:14:19, 17.67it/s][INFO|trainer.py:1928] 2021-08-14 08:15:54,895 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:15:54,899 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:15:55,007 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:15:55,011 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:15:55,013 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:15:55,290 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500] due to args.save_total_limit\n",
            "{'loss': 4.5474, 'learning_rate': 4.384098544232923e-05, 'epoch': 2.46}\n",
            " 12% 11000/89300 [10:20<1:11:50, 18.16it/s][INFO|trainer.py:1928] 2021-08-14 08:16:23,979 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:16:23,984 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:16:24,093 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:16:24,096 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:16:24,099 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:16:24,366 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 4.5115, 'learning_rate': 4.3561030235162374e-05, 'epoch': 2.58}\n",
            " 13% 11500/89300 [10:48<1:07:30, 19.21it/s][INFO|trainer.py:1928] 2021-08-14 08:16:52,286 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:16:52,291 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:16:52,405 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:16:52,408 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:16:52,411 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:16:52,676 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500] due to args.save_total_limit\n",
            "{'loss': 4.4858, 'learning_rate': 4.328107502799552e-05, 'epoch': 2.69}\n",
            " 13% 12000/89300 [11:17<1:17:12, 16.69it/s][INFO|trainer.py:1928] 2021-08-14 08:17:20,952 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:17:20,956 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:17:21,089 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:17:21,093 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:17:21,096 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:17:21,380 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000] due to args.save_total_limit\n",
            "{'loss': 4.3805, 'learning_rate': 4.3001119820828664e-05, 'epoch': 2.8}\n",
            " 14% 12500/89300 [11:45<54:15, 23.59it/s][INFO|trainer.py:1928] 2021-08-14 08:17:48,527 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:17:48,531 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:17:48,637 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:17:48,641 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:17:48,644 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:17:48,927 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500] due to args.save_total_limit\n",
            "{'loss': 4.4486, 'learning_rate': 4.272116461366182e-05, 'epoch': 2.91}\n",
            " 15% 13000/89300 [12:14<1:27:13, 14.58it/s][INFO|trainer.py:1928] 2021-08-14 08:18:17,429 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:18:17,433 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:18:17,547 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:18:17,551 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:18:17,555 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:18:17,853 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 4.3554, 'learning_rate': 4.244120940649496e-05, 'epoch': 3.02}\n",
            " 15% 13500/89300 [12:42<1:14:29, 16.96it/s][INFO|trainer.py:1928] 2021-08-14 08:18:46,031 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:18:46,035 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:18:46,147 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:18:46,151 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:18:46,154 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:18:46,430 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500] due to args.save_total_limit\n",
            "{'loss': 4.2783, 'learning_rate': 4.216125419932811e-05, 'epoch': 3.14}\n",
            " 16% 14000/89300 [13:10<1:18:06, 16.07it/s][INFO|trainer.py:1928] 2021-08-14 08:19:14,075 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:19:14,079 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:19:14,194 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:19:14,198 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:19:14,201 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:19:14,479 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000] due to args.save_total_limit\n",
            "{'loss': 4.237, 'learning_rate': 4.188129899216126e-05, 'epoch': 3.25}\n",
            " 16% 14500/89300 [13:38<1:19:59, 15.59it/s][INFO|trainer.py:1928] 2021-08-14 08:19:42,147 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:19:42,167 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:19:42,285 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:19:42,289 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:19:42,292 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:19:42,554 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500] due to args.save_total_limit\n",
            "{'loss': 4.186, 'learning_rate': 4.16013437849944e-05, 'epoch': 3.36}\n",
            " 17% 15000/89300 [14:07<1:06:54, 18.51it/s][INFO|trainer.py:1928] 2021-08-14 08:20:10,616 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:20:10,621 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:20:10,744 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:20:10,748 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:20:10,750 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:20:11,031 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 4.1182, 'learning_rate': 4.1321388577827555e-05, 'epoch': 3.47}\n",
            " 17% 15500/89300 [14:36<1:16:12, 16.14it/s][INFO|trainer.py:1928] 2021-08-14 08:20:39,783 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:20:39,787 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:20:39,899 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:20:39,902 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:20:39,905 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:20:40,181 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500] due to args.save_total_limit\n",
            "{'loss': 4.1086, 'learning_rate': 4.1041433370660696e-05, 'epoch': 3.58}\n",
            " 18% 16000/89300 [15:05<1:03:02, 19.38it/s][INFO|trainer.py:1928] 2021-08-14 08:21:08,648 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:21:08,652 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:21:08,771 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:21:08,775 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:21:08,777 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:21:09,078 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 4.0681, 'learning_rate': 4.0761478163493845e-05, 'epoch': 3.7}\n",
            " 18% 16500/89300 [15:33<56:19, 21.54it/s][INFO|trainer.py:1928] 2021-08-14 08:21:36,367 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:21:36,371 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:21:36,483 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:21:36,487 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:21:36,490 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:21:36,757 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500] due to args.save_total_limit\n",
            "{'loss': 4.0486, 'learning_rate': 4.0481522956326986e-05, 'epoch': 3.81}\n",
            " 19% 17000/89300 [16:01<1:04:42, 18.62it/s][INFO|trainer.py:1928] 2021-08-14 08:22:05,097 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:22:05,101 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:22:05,220 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:22:05,224 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:22:05,227 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:22:05,512 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 3.9896, 'learning_rate': 4.0201567749160135e-05, 'epoch': 3.92}\n",
            " 20% 17500/89300 [16:29<1:15:43, 15.80it/s][INFO|trainer.py:1928] 2021-08-14 08:22:32,794 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:22:32,799 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:22:32,909 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:22:32,926 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:22:32,931 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:22:33,197 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500] due to args.save_total_limit\n",
            "{'loss': 3.977, 'learning_rate': 3.992161254199328e-05, 'epoch': 4.03}\n",
            " 20% 18000/89300 [16:58<1:05:25, 18.16it/s][INFO|trainer.py:1928] 2021-08-14 08:23:01,420 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:23:01,425 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:23:01,547 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:23:01,552 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:23:01,555 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:23:01,869 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000] due to args.save_total_limit\n",
            "{'loss': 3.8959, 'learning_rate': 3.964165733482643e-05, 'epoch': 4.14}\n",
            " 21% 18500/89300 [17:25<1:11:39, 16.47it/s][INFO|trainer.py:1928] 2021-08-14 08:23:29,067 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:23:29,071 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:23:29,196 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:23:29,199 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:23:29,202 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:23:29,495 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500] due to args.save_total_limit\n",
            "{'loss': 3.8237, 'learning_rate': 3.936170212765958e-05, 'epoch': 4.26}\n",
            " 21% 19000/89300 [17:53<1:18:04, 15.01it/s][INFO|trainer.py:1928] 2021-08-14 08:23:56,892 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:23:56,897 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:23:57,007 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:23:57,011 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:23:57,014 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:23:57,305 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000] due to args.save_total_limit\n",
            "{'loss': 3.8768, 'learning_rate': 3.908174692049272e-05, 'epoch': 4.37}\n",
            " 22% 19500/89300 [18:22<1:24:03, 13.84it/s][INFO|trainer.py:1928] 2021-08-14 08:24:25,610 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:24:25,614 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:24:25,733 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:24:25,737 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:24:25,740 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:24:26,010 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500] due to args.save_total_limit\n",
            "{'loss': 3.7709, 'learning_rate': 3.880179171332587e-05, 'epoch': 4.48}\n",
            " 22% 20000/89300 [18:50<55:55, 20.65it/s][INFO|trainer.py:1928] 2021-08-14 08:24:53,975 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:24:53,980 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:24:54,118 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:24:54,124 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:24:54,127 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:24:54,414 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000] due to args.save_total_limit\n",
            "{'loss': 3.8169, 'learning_rate': 3.852183650615902e-05, 'epoch': 4.59}\n",
            " 23% 20500/89300 [19:20<1:31:14, 12.57it/s][INFO|trainer.py:1928] 2021-08-14 08:25:23,847 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:25:23,852 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:25:23,962 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:25:23,966 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:25:23,968 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:25:24,259 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500] due to args.save_total_limit\n",
            "{'loss': 3.8744, 'learning_rate': 3.824188129899216e-05, 'epoch': 4.7}\n",
            " 24% 21000/89300 [19:50<1:18:10, 14.56it/s][INFO|trainer.py:1928] 2021-08-14 08:25:53,422 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:25:53,427 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:25:53,549 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:25:53,553 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:25:53,556 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:25:53,845 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 3.7381, 'learning_rate': 3.7961926091825315e-05, 'epoch': 4.82}\n",
            " 24% 21500/89300 [20:17<1:13:03, 15.47it/s][INFO|trainer.py:1928] 2021-08-14 08:26:21,292 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:26:21,296 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:26:21,406 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:26:21,410 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:26:21,413 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:26:21,670 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500] due to args.save_total_limit\n",
            "{'loss': 3.6851, 'learning_rate': 3.7681970884658456e-05, 'epoch': 4.93}\n",
            " 25% 22000/89300 [20:45<1:05:15, 17.19it/s][INFO|trainer.py:1928] 2021-08-14 08:26:48,839 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000\n",
            "[INFO|configuration_utils.py:379] 2021-08-14 08:26:48,844 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-14 08:26:48,958 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-14 08:26:48,962 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-14 08:26:48,965 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-14 08:26:49,233 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000] due to args.save_total_limit\n",
            " 25% 22443/89300 [21:10<52:53, 21.07it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QGY7F6YUQPe"
      },
      "source": [
        "# language model 만들었으면... 여기서 부터~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj-fQYF-L4gB"
      },
      "source": [
        "#model_name = 'albert-base-v2'\n",
        "model_name = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhnMlOQ-9YiH",
        "outputId": "d2b1d070-dee1-43e3-c67c-be79d66bb71f"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "op = tokenizer(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   5, 3174,  157,  159,  151,  468,  121, 3250, 3367, 2698,  121,  304,\n",
            "         2824,  345,  377,  842,  184, 2154, 1075,  157,  304, 4565, 5232,  126,\n",
            "          158, 1075,  151, 1224,  127, 4404,  884,  126, 5033,  468,  121, 3250,\n",
            "         3367, 2698,  121,  304, 2824,  345,  377,  842,  184, 2154, 1075,  126,\n",
            "         5033,  260,  157,  304, 4565, 5232,  126,  158, 1075,  327,  706,  864,\n",
            "          379, 3174,  140, 1271,  982,  378,  121,  614,  127,  296,  461,  182,\n",
            "          127,    6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aero', 'n', 'au', 'tical', '▁flight', '▁information', '▁file', '▁e', 'dition', '▁8', '▁(', 'au', 'sda', 'fif', '▁', 'ed', '▁8', ')', '▁withdrawn', '.', '▁replace', 'ment', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aero', 'n', 'au', 'tical', '▁flight', '▁information', '▁file', '▁e', 'dition', '▁8', '▁', 'version', '▁2', '▁(', 'au', 'sda', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'af', '▁int', 'ra', 'n', 'et', '.', '▁mil', '▁use', '▁only', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 3174, 157, 159, 151, 468, 121, 3250, 3367, 2698, 121, 304, 2824, 345, 377, 842, 184, 2154, 1075, 157, 304, 4565, 5232, 126, 158, 1075, 151, 1224, 127, 4404, 884, 126, 5033, 468, 121, 3250, 3367, 2698, 121, 304, 2824, 345, 377, 842, 184, 2154, 1075, 126, 5033, 260, 157, 304, 4565, 5232, 126, 158, 1075, 327, 706, 864, 379, 3174, 140, 1271, 982, 378, 121, 614, 127, 296, 461, 182, 127, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBgbBd_YBXFK"
      },
      "source": [
        "# Trainset, Testset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frtHb3-FEion"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj6IsYhuBaYC",
        "outputId": "55042650-d626-47de-df24-79278486b90c"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "# For every sentence...\n",
        "for row in df.iterrows():\n",
        "    # E section\n",
        "    sent = clean_text(str(row[1][11]).lower()) \n",
        "    # label for E Section\n",
        "    lb = row[1][3]+0\n",
        "    if lb == 6:\n",
        "        lb = 5\n",
        "    labels.append(lb)\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation = True,\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', notam_contents[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Training & Validation Split\n",
        "# Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )   "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2187: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\n",
            "Token IDs: tensor([    5,  3174,   157,   159,   151,   468,   121,  3250,  3367,  2698,\n",
            "          121,   304,  2824,   345,   377,   842,   184,  2154,  1075,   157,\n",
            "          304,  4565,  5232,   126,   158,  1075,   151,  1224,   127,  4404,\n",
            "          884,   126,  5033,   468,   121,  3250,  3367,  2698,   121,   304,\n",
            "         2824,   345,   377,   842,   184,  2154,  1075,   126,  5033,   260,\n",
            "          157,   304,  4565,  5232,   126,   158,  1075,   327,   706,   864,\n",
            "          379,  3174,   140,  1271,   982,   378,   121,   614,   127,   296,\n",
            "          461,   182,   127,     6, 11866, 11866, 11866, 11866, 11866, 11866,\n",
            "        11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866,\n",
            "        11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866,\n",
            "        11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866,\n",
            "        11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866,\n",
            "        11866, 11866, 11866, 11866, 11866, 11866, 11866, 11866])\n",
            "9,373 training samples\n",
            "1,042 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK41iend2mO9"
      },
      "source": [
        "# classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDzOhrAs2oRX",
        "outputId": "d872f921-d2c9-48ab-8326-36d2a8737258"
      },
      "source": [
        "from transformers import AlbertForSequenceClassification\n",
        "\n",
        "model = AlbertForSequenceClassification.from_pretrained(\n",
        "                                    model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL9-inxrHi5J",
        "outputId": "7d7681e3-a4ce-4f86-e335-1d2cf42ddef5"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcFa0AUzReC4"
      },
      "source": [
        "import os\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def save_notam_model(notam_model, output_dir = '/content/drive/MyDrive/NOTAM/notam_model'):\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = notam_model.module if hasattr(notam_model, 'module') else notam_model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "def load_notam_model(input_dir = '/content/drive/MyDrive/NOTAM/notam_model'):\n",
        "    print('Loading Albert notam model...')\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(input_dir)\n",
        "    model = AlbertForSequenceClassification.from_pretrained(input_dir)\n",
        "    return tokenizer, model\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cwwyGE-FpZQ",
        "outputId": "756b17cf-daef-4f14-c7c3-34e65ab4e1f8"
      },
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        " \n",
        "print(model)\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "#epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "    \n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        #print(b_input_ids)\n",
        "        #print(b_input_mask)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "\n",
        "        outputs =  model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask, \n",
        "                            labels=b_labels)\n",
        "        '''\n",
        "        loss, logits = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask, \n",
        "                            labels=b_labels)\n",
        "        '''\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        for l in batch[2]:\n",
        "            true_labels.append(l.item())        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        for li in logits:\n",
        "            pred_labels.append(np.argmax(li))\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    print(\"  classification_report   \")    \n",
        "    print(classification_report(true_labels,pred_labels))\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    save_notam_model(model,output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section')\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(11967, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.16      0.23        31\n",
            "           1       0.59      0.62      0.60       185\n",
            "           2       0.56      0.41      0.47        95\n",
            "           3       0.56      0.36      0.44       110\n",
            "           4       0.83      0.84      0.84       164\n",
            "           5       0.75      0.86      0.80       457\n",
            "\n",
            "    accuracy                           0.70      1042\n",
            "   macro avg       0.61      0.54      0.56      1042\n",
            "weighted avg       0.68      0.70      0.69      1042\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_E_Section\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:56 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjxTaVBdxNev"
      },
      "source": [
        "# Combining Categorical and Numerical Features with Text in BERT\n",
        "\n",
        "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW8qqTp8xSXM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}