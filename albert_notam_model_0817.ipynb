{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN+zDFUrTSaphU9xcWeTOpV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/NOTAM/blob/main/albert_notam_model_0817.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tlIhtCjJia"
      },
      "source": [
        "# Albert NOTAM 언어 모델 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc1weyK_jPwQ",
        "outputId": "be29b897-5770-4691-9df7-6b9f4cecacd8"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5DRavVNRey3"
      },
      "source": [
        "## 참조\n",
        "\n",
        "https://colab.research.google.com/github/parmarsuraj99/suraj-parmar/blob/master/_notebooks/2020-05-02-SanskritALBERT.ipynb#scrollTo=VNAOMXjpMHZD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038IYm33Muol"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oVCIQF4UMmyF",
        "outputId": "9508d5cd-caf4-4d3d-decf-371111aeb4ef"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/.\n",
        "!pip install sentencepiece==0.1.95\n",
        "!pip install datasets==1.8.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 80853, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 80853 (delta 35), reused 38 (delta 8), pack-reused 80769\u001b[K\n",
            "Receiving objects: 100% (80853/80853), 63.09 MiB | 29.47 MiB/s, done.\n",
            "Resolving deltas: 100% (57885/57885), done.\n",
            "Processing ./transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.6.3)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.10.0.dev0-py3-none-any.whl size=2666966 sha256=88e30ff454b48ef878c813e45e569cb87bc01438a8d096a53e79410c8ac6cfc3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jqgz8msb/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.15 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0.dev0\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting datasets==1.8.0\n",
            "  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2.23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 40.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (21.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (4.6.3)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.0.15)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.19.5)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (3.0.0)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.8.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.8.0) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.8.0) (1.15.0)\n",
            "Installing collected packages: tqdm, xxhash, fsspec, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed datasets-1.8.0 fsspec-2021.7.0 tqdm-4.49.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_K0rsDlckzd"
      },
      "source": [
        "# NOTAM Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWN53GjXcqSa",
        "outputId": "94e4907f-2e6f-4d33-f941-45d8520e6fbf"
      },
      "source": [
        "%%time\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/NOTAM/data/TRAIN_210812.csv')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 35.3 ms, sys: 17.9 ms, total: 53.2 ms\n",
            "Wall time: 655 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "MPBJvnYscutM",
        "outputId": "b0c19646-5906-4123-8231-d1df11196001"
      },
      "source": [
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSG_ID</th>\n",
              "      <th>UPDATED_BY</th>\n",
              "      <th>UPDATED_DATE</th>\n",
              "      <th>E_SCORE</th>\n",
              "      <th>AE_SCORE</th>\n",
              "      <th>ALL_SCORE</th>\n",
              "      <th>Q_LINE</th>\n",
              "      <th>A_LINE</th>\n",
              "      <th>B_LINE</th>\n",
              "      <th>C_LINE</th>\n",
              "      <th>D_LINE</th>\n",
              "      <th>E_LINE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6824038</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:08</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6824038</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6816851</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:09</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6816851</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:18</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6816333</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:11</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KZOB/QMXLC/IV/BO/A/000/999/4213N08321W005</td>\n",
              "      <td>KDTW</td>\n",
              "      <td>1905211704</td>\n",
              "      <td>1905211800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY K5 CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10410</th>\n",
              "      <td>15966850</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-08-01 10:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZNY/QMXLC/IV/BO/A/000/999/4038N07347W005</td>\n",
              "      <td>KJFK</td>\n",
              "      <td>1912311421</td>\n",
              "      <td>1912311600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY C BTN TWY C1 AND TWY V CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10411</th>\n",
              "      <td>15947262</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:06</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>LCCC/QWFLW/IV/M  /W /210/230/3335N03202E133</td>\n",
              "      <td>LCCC</td>\n",
              "      <td>2001040820</td>\n",
              "      <td>2001051150</td>\n",
              "      <td>0820-1150</td>\n",
              "      <td>AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10412</th>\n",
              "      <td>15962821</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:48</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZAU/QMRLC/IV/NBO/A/000/999/4257N08754W005</td>\n",
              "      <td>KMKE</td>\n",
              "      <td>1912310929</td>\n",
              "      <td>2001012359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 01L/19R CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10413</th>\n",
              "      <td>15965583</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-06 13:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>2001071500</td>\n",
              "      <td>2001072000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10414</th>\n",
              "      <td>15962570</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-08 14:49</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>1912311430</td>\n",
              "      <td>1912312100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10415 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         MSG_ID  ...                                             E_LINE\n",
              "0       6824038  ...  AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...\n",
              "1       6824038  ...  AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...\n",
              "2       6816851  ...  LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...\n",
              "3       6816851  ...  LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...\n",
              "4       6816333  ...                                        TWY K5 CLSD\n",
              "...         ...  ...                                                ...\n",
              "10410  15966850  ...                    TWY C BTN TWY C1 AND TWY V CLSD\n",
              "10411  15947262  ...  AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...\n",
              "10412  15962821  ...                                   RWY 01L/19R CLSD\n",
              "10413  15965583  ...  TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...\n",
              "10414  15962570  ...  TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...\n",
              "\n",
              "[10415 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAyAjbU1eBWU"
      },
      "source": [
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    #txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    #txt = txt.replace('..','')\n",
        "    #txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmFGrzRQdi_l"
      },
      "source": [
        "notam_contents = []\n",
        "\n",
        "for row in df.iterrows():\n",
        "    doc_cont = str(row[1][11])\n",
        "    # Q Section\n",
        "    notam_contents.append(clean_text(str(row[1][6]).lower()))\n",
        "    # A Section\n",
        "    notam_contents.append(clean_text(str(row[1][7]).lower()))\n",
        "    # E Section\n",
        "    notam_contents.append(clean_text(doc_cont.lower()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfnVFX03d1Dn",
        "outputId": "802f8202-ecc1-4f29-aabf-1156b818f751"
      },
      "source": [
        "len(notam_contents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31245"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f16NAw2Id3ei",
        "outputId": "cc7abae4-9eaa-4ae6-d888-92cb3b2b9efc"
      },
      "source": [
        "notam_contents[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yuxx/qoaxx/iv/bo/e/000/999/2537s13421e005'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vr5SWcZpZ00"
      },
      "source": [
        "f = open('/content/drive/MyDrive/NOTAM/train_tokenizer2.txt', 'r')\n",
        "while True:\n",
        "    line = f.readline()\n",
        "    if not line: break\n",
        "    notam_contents.append(clean_text(line.lower()))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm0Z7smipl3J",
        "outputId": "64da4b4f-c0d3-4227-b149-73766aab4a36"
      },
      "source": [
        "len(notam_contents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzHDt-FVdW6P",
        "outputId": "788e0f2b-2571-448d-dc18-e622b72a7721"
      },
      "source": [
        "%%time\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('/content/drive/MyDrive/NOTAM/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in notam_contents:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 17.5 ms, sys: 4.07 ms, total: 21.6 ms\n",
            "Wall time: 33 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bNszLMewp4"
      },
      "source": [
        "# Tokenizer 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGBDbPNwy1X-",
        "outputId": "2e2db08d-3d1f-4465-d79b-5c8f206987ef"
      },
      "source": [
        "%%time\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "# spm_train --input=data/train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
        "\n",
        "input_file = '/content/drive/MyDrive/NOTAM/train_tokenizer.txt'\n",
        "vocab_size = 12000\n",
        "\n",
        "sp_model_root='/content/drive/MyDrive/NOTAM/albert_tokenizer_model'\n",
        "if not os.path.isdir(sp_model_root):\n",
        "    os.mkdir(sp_model_root)\n",
        "sp_model_name = 'spiece'\n",
        "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
        "model_type = 'unigram'  # 학습할 모델 선택, unigram이 더 성능이 좋음'bpe'\n",
        "character_coverage  = 1.0  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
        "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "print('train done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train done\n",
            "CPU times: user 2min 18s, sys: 20 s, total: 2min 38s\n",
            "Wall time: 2min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23ogFfIztOR",
        "outputId": "9ca710be-aad3-4f86-a7f1-a1984352b212"
      },
      "source": [
        "## check\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(sp_model_path))\n",
        "\n",
        "tokens = sp.encode_as_pieces(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "ids = sp.encode_as_ids(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "\n",
        "print(ids)\n",
        "print(tokens)\n",
        "\n",
        "tokens = sp.decode_pieces(tokens)\n",
        "ids = sp.decode_ids(ids)\n",
        "\n",
        "print(ids)\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2656, 160, 334, 146, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 160, 3732, 1368, 8997, 124, 156, 491, 1549, 123, 3097, 174, 124, 9387, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 124, 9387, 248, 160, 3732, 1368, 8997, 124, 156, 702, 327, 800, 549, 234, 2656, 134, 136, 328, 182, 2699, 1635, 123, 493, 305, 217, 123]\n",
            "['▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'a', 'f', '▁in', 'tra', 'net', '.', '▁mil', '▁use', '▁only', '.']\n",
            "ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\n",
            "ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXWYH5FRyu1"
      },
      "source": [
        "# 여기서부터 다시 -> NOTAM Language model 만들기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55T0J0MDFtrm",
        "outputId": "523429cf-a20e-4e2e-bdb5-c6e52ccd8f19"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF_Q_0VTFzPv",
        "outputId": "3de4b561-8845-466b-b3d5-6023a75f48ba"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "#'/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "albet_tokenizer_model = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model'\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file /content/drive/MyDrive/NOTAM/albert_tokenizer_model/config.json not found\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "file /content/drive/MyDrive/NOTAM/albert_tokenizer_model/config.json not found\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "QB0SlhA4GMQQ",
        "outputId": "bad04afc-72e6-4d20-e383-9045d626fa9b"
      },
      "source": [
        "op = tokenizer.encode(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "print(op)\n",
        "tokenizer.decode(op)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 2656, 160, 334, 146, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 160, 3732, 1368, 8997, 124, 156, 491, 1549, 123, 3097, 174, 124, 9387, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 124, 9387, 248, 160, 3732, 1368, 8997, 124, 156, 702, 327, 800, 549, 234, 2656, 134, 136, 328, 182, 2699, 1635, 123, 493, 305, 217, 123, 6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMDfH8GZF_mK",
        "outputId": "5e2a451a-324e-4ee4-dc12-ea007fd81b0d"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer.all_special_tokens[:20])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check special tokens : ['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inr3cDwQG6Fs",
        "outputId": "3f8863aa-7916-45e9-b4ee-acb618636201"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer.save_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/spiece.model',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/added_tokens.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhlWrXXHr6L"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHhYsZbH3zr",
        "outputId": "0dc3bfa8-a6aa-4b96-c1bf-ea1272d06e0f"
      },
      "source": [
        "op = tokenizer(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   5, 2656,  160,  334,  146,  953,  121, 5844, 6340, 4549,  258, 6473,\n",
            "          227,  545,  781, 2677,  702,  160, 3732, 1368, 8997,  124,  156,  491,\n",
            "         1549,  123, 3097,  174,  124, 9387,  953,  121, 5844, 6340, 4549,  258,\n",
            "         6473,  227,  545,  781, 2677,  702,  124, 9387,  248,  160, 3732, 1368,\n",
            "         8997,  124,  156,  702,  327,  800,  549,  234, 2656,  134,  136,  328,\n",
            "          182, 2699, 1635,  123,  493,  305,  217,  123,    6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'a', 'f', '▁in', 'tra', 'net', '.', '▁mil', '▁use', '▁only', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 2656, 160, 334, 146, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 160, 3732, 1368, 8997, 124, 156, 491, 1549, 123, 3097, 174, 124, 9387, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 124, 9387, 248, 160, 3732, 1368, 8997, 124, 156, 702, 327, 800, 549, 234, 2656, 134, 136, 328, 182, 2699, 1635, 123, 493, 305, 217, 123, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paElX_hhJAoh",
        "outputId": "80a38b34-09ab-495c-c842-791ef218993d"
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9RPI1UerOSCB",
        "outputId": "cbe95071-f0b4-4df3-e383-90bd4d9a5ecb"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "        \"AlbertModel\"\n",
        "    ],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": vocab_size\n",
        "}\n",
        "with open(albet_tokenizer_model + \"_special/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "\n",
        "#Configuration for tokenizer.\n",
        "#Note: I set do_lower_case: False, and keep_accents:True\n",
        "# Opening JSON file\n",
        "f = open(albet_tokenizer_model+ \"_special/tokenizer_config.json\")\n",
        "   \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "tokenizer_config = json.load(f)\n",
        "\n",
        "tokenizer_config['max_len'] = 512\n",
        "tokenizer_config['model_type'] = 'albert'\n",
        "tokenizer_config['do_lower_case'] = False\n",
        "tokenizer_config['keep_accents'] = True\n",
        "\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as outfile:\n",
        "    json.dump(tokenizer_config, outfile)\n",
        "'''\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"do_lower_case\":False, \n",
        "\t\"keep_accents\":True\n",
        "}\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "'''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntokenizer_config = {\\n\\t\"max_len\": 512,\\n\\t\"model_type\": \"albert\",\\n\\t\"do_lower_case\":False, \\n\\t\"keep_accents\":True\\n}\\nwith open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", \\'w\\') as fp:\\n    json.dump(tokenizer_config, fp)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdjOOoG6QX2p",
        "outputId": "cad20b61-1b73-49a3-d446-87d9320bf903"
      },
      "source": [
        "\n",
        "#To train from scratch\n",
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special \\\n",
        "        --tokenizer_name /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special \\\n",
        "        --train_file /content/drive/MyDrive/NOTAM/train_tokenizer.txt \\\n",
        "        --output_dir /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model \\\n",
        "        --use_fast_tokenizer \\\n",
        "        --do_train \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 500 \\\n",
        "        --logging_steps 500 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --num_train_epochs 20 \\\n",
        "        --seed 108 \\\n",
        "        --overwrite_output_dir \\\n",
        "        --logging_dir /content/drive/MyDrive/Tokenizer_train/logs"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/17/2021 23:35:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/17/2021 23:35:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/Tokenizer_train/logs,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=20.0,\n",
            "output_dir=/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=model,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=108,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "08/17/2021 23:35:07 - WARNING - datasets.builder - Using custom data configuration default-38715558c88f2163\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.filelock - Lock 139792058711760 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.filelock - Lock 139792058711760 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.filelock - Lock 139792058340304 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:07 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "100% 1/1 [00:00<00:00, 2978.91it/s]\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 1/1 [00:00<00:00, 128.11it/s]\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "08/17/2021 23:35:07 - INFO - datasets.builder - Generating split train\n",
            "08/17/2021 23:35:07 - INFO - datasets.arrow_writer - Done writing 48017 examples in 20477445 bytes /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.incomplete/text-train.arrow.\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.filelock - Lock 139792058342544 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.incomplete.lock\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.filelock - Lock 139792058342544 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.incomplete.lock\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "08/17/2021 23:35:07 - INFO - datasets.utils.filelock - Lock 139792058340304 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:07 - INFO - datasets.builder - Constructing Dataset for split train, from /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "100% 1/1 [00:00<00:00, 374.93it/s]\n",
            "08/17/2021 23:35:08 - WARNING - datasets.builder - Using custom data configuration default-38715558c88f2163\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058709968 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "08/17/2021 23:35:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058709968 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058339536 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "08/17/2021 23:35:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058339536 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - INFO - datasets.builder - Constructing Dataset for split train[:5%], from /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/17/2021 23:35:08 - WARNING - datasets.builder - Using custom data configuration default-38715558c88f2163\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058340368 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "08/17/2021 23:35:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058340368 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058254416 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
            "08/17/2021 23:35:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "08/17/2021 23:35:08 - INFO - datasets.utils.filelock - Lock 139792058254416 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_text_default-38715558c88f2163_0.0.0_e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5.lock\n",
            "08/17/2021 23:35:08 - INFO - datasets.builder - Constructing Dataset for split train[5%:], from /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\n",
            "[INFO|configuration_utils.py:543] 2021-08-17 23:35:08,221 >> loading configuration file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/config.json\n",
            "[INFO|configuration_utils.py:581] 2021-08-17 23:35:08,222 >> Model config AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 12000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-17 23:35:08,229 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-17 23:35:08,229 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-17 23:35:08,229 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-17 23:35:08,229 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1728] 2021-08-17 23:35:08,229 >> loading file /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer_config.json\n",
            "08/17/2021 23:35:08 - INFO - __main__ - Training new model from scratch\n",
            "Running tokenizer on dataset line_by_line:   0% 0/46 [00:00<?, ?ba/s]08/17/2021 23:35:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-a0ed221101b1b69c.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 46/46 [00:04<00:00, 10.45ba/s]\n",
            "08/17/2021 23:35:13 - INFO - datasets.arrow_writer - Done writing 45610 examples in 9093632 bytes /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/tmpw04rxtdt.\n",
            "Running tokenizer on dataset line_by_line:   0% 0/3 [00:00<?, ?ba/s]08/17/2021 23:35:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-4700229eda36448b.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 3/3 [00:00<00:00, 24.01ba/s]\n",
            "08/17/2021 23:35:13 - INFO - datasets.arrow_writer - Done writing 2401 examples in 303715 bytes /root/.cache/huggingface/datasets/text/default-38715558c88f2163/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/tmplmzeu7me.\n",
            "[INFO|trainer.py:528] 2021-08-17 23:35:19,792 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1171] 2021-08-17 23:35:19,800 >> ***** Running training *****\n",
            "[INFO|trainer.py:1172] 2021-08-17 23:35:19,800 >>   Num examples = 45610\n",
            "[INFO|trainer.py:1173] 2021-08-17 23:35:19,800 >>   Num Epochs = 20\n",
            "[INFO|trainer.py:1174] 2021-08-17 23:35:19,800 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1175] 2021-08-17 23:35:19,800 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1176] 2021-08-17 23:35:19,800 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1177] 2021-08-17 23:35:19,800 >>   Total optimization steps = 114040\n",
            "{'loss': 8.1615, 'learning_rate': 4.9780778674149426e-05, 'epoch': 0.09}\n",
            "  0% 500/114040 [00:25<1:53:17, 16.70it/s][INFO|trainer.py:1928] 2021-08-17 23:35:45,448 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:35:45,661 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:35:46,317 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:35:46,545 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:35:46,682 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:35:48,282 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500] due to args.save_total_limit\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:35:48,284 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000] due to args.save_total_limit\n",
            "{'loss': 6.9846, 'learning_rate': 4.956155734829884e-05, 'epoch': 0.18}\n",
            "  1% 1000/114040 [00:54<1:38:12, 19.18it/s][INFO|trainer.py:1928] 2021-08-17 23:36:14,210 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:36:14,216 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:36:14,345 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:36:14,350 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:36:14,354 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:36:14,656 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500] due to args.save_total_limit\n",
            "{'loss': 6.6211, 'learning_rate': 4.9342336022448266e-05, 'epoch': 0.26}\n",
            "  1% 1500/114040 [01:19<1:32:12, 20.34it/s][INFO|trainer.py:1928] 2021-08-17 23:36:39,295 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:36:39,301 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:36:39,435 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:36:39,440 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:36:39,445 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:36:39,733 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 6.4483, 'learning_rate': 4.912311469659769e-05, 'epoch': 0.35}\n",
            "  2% 2000/114040 [01:45<1:42:31, 18.21it/s][INFO|trainer.py:1928] 2021-08-17 23:37:06,074 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:37:06,079 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:37:06,198 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:37:06,203 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:37:06,207 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:37:06,510 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 6.3061, 'learning_rate': 4.8903893370747106e-05, 'epoch': 0.44}\n",
            "  2% 2500/114040 [02:13<1:48:35, 17.12it/s][INFO|trainer.py:1928] 2021-08-17 23:37:33,950 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:37:33,955 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:37:34,080 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:37:34,085 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:37:34,089 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:37:34,399 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 6.1453, 'learning_rate': 4.868467204489653e-05, 'epoch': 0.53}\n",
            "  3% 3000/114040 [02:39<1:37:57, 18.89it/s][INFO|trainer.py:1928] 2021-08-17 23:38:00,025 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:38:00,030 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:38:00,157 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:38:00,162 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:38:00,167 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:38:00,457 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 6.0384, 'learning_rate': 4.846545071904595e-05, 'epoch': 0.61}\n",
            "  3% 3500/114040 [03:07<1:34:37, 19.47it/s][INFO|trainer.py:1928] 2021-08-17 23:38:27,211 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:38:27,217 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:38:27,345 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:38:27,350 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:38:27,354 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:38:27,643 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-2500] due to args.save_total_limit\n",
            "{'loss': 5.8604, 'learning_rate': 4.8246229393195376e-05, 'epoch': 0.7}\n",
            "  4% 4000/114040 [03:34<1:50:38, 16.58it/s][INFO|trainer.py:1928] 2021-08-17 23:38:54,380 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:38:54,385 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:38:54,509 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:38:54,529 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:38:54,535 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:38:54,822 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 5.7909, 'learning_rate': 4.802700806734479e-05, 'epoch': 0.79}\n",
            "  4% 4500/114040 [04:01<2:20:35, 12.99it/s][INFO|trainer.py:1928] 2021-08-17 23:39:21,852 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:39:21,859 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:39:21,983 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:39:21,988 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:39:21,992 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:39:22,307 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-3500] due to args.save_total_limit\n",
            "{'loss': 5.6394, 'learning_rate': 4.7807786741494216e-05, 'epoch': 0.88}\n",
            "  4% 5000/114040 [04:27<1:36:58, 18.74it/s][INFO|trainer.py:1928] 2021-08-17 23:39:47,933 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:39:47,938 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:39:48,063 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:39:48,068 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:39:48,072 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:39:48,377 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 5.5764, 'learning_rate': 4.758856541564364e-05, 'epoch': 0.96}\n",
            "  5% 5500/114040 [04:55<1:33:11, 19.41it/s][INFO|trainer.py:1928] 2021-08-17 23:40:15,477 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:40:15,483 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:40:15,607 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:40:15,612 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:40:15,615 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:40:15,894 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 5.4242, 'learning_rate': 4.7369344089793056e-05, 'epoch': 1.05}\n",
            "  5% 6000/114040 [05:22<1:31:38, 19.65it/s][INFO|trainer.py:1928] 2021-08-17 23:40:43,035 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:40:43,040 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:40:43,156 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:40:43,161 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:40:43,164 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:40:43,448 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 5.4131, 'learning_rate': 4.715012276394248e-05, 'epoch': 1.14}\n",
            "  6% 6500/114040 [05:50<1:40:24, 17.85it/s][INFO|trainer.py:1928] 2021-08-17 23:41:10,460 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:41:10,466 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:41:10,595 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:41:10,600 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:41:10,622 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:41:10,927 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-5500] due to args.save_total_limit\n",
            "{'loss': 5.2271, 'learning_rate': 4.69309014380919e-05, 'epoch': 1.23}\n",
            "  6% 7000/114040 [06:16<1:43:09, 17.29it/s][INFO|trainer.py:1928] 2021-08-17 23:41:36,617 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:41:36,622 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:41:36,743 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:41:36,749 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:41:36,753 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:41:37,067 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 5.1335, 'learning_rate': 4.671168011224132e-05, 'epoch': 1.32}\n",
            "  7% 7500/114040 [06:42<1:15:28, 23.53it/s][INFO|trainer.py:1928] 2021-08-17 23:42:02,784 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:42:02,790 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:42:02,920 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:42:02,925 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:42:02,929 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:42:03,253 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-6500] due to args.save_total_limit\n",
            "{'loss': 5.043, 'learning_rate': 4.649245878639074e-05, 'epoch': 1.4}\n",
            "  7% 8000/114040 [07:09<1:19:40, 22.18it/s][INFO|trainer.py:1928] 2021-08-17 23:42:29,397 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:42:29,403 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:42:29,526 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:42:29,531 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:42:29,536 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:42:29,829 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7000] due to args.save_total_limit\n",
            "{'loss': 5.0099, 'learning_rate': 4.6273237460540166e-05, 'epoch': 1.49}\n",
            "  7% 8500/114040 [07:34<1:34:59, 18.52it/s][INFO|trainer.py:1928] 2021-08-17 23:42:55,052 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:42:55,058 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:42:55,186 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:42:55,191 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:42:55,195 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:42:55,467 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-7500] due to args.save_total_limit\n",
            "{'loss': 5.011, 'learning_rate': 4.605401613468959e-05, 'epoch': 1.58}\n",
            "  8% 9000/114040 [08:01<1:50:38, 15.82it/s][INFO|trainer.py:1928] 2021-08-17 23:43:21,636 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:43:21,641 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:43:21,772 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:43:21,796 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:43:21,801 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:43:22,094 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 4.9247, 'learning_rate': 4.5834794808839006e-05, 'epoch': 1.67}\n",
            "  8% 9500/114040 [08:27<1:19:19, 21.96it/s][INFO|trainer.py:1928] 2021-08-17 23:43:48,054 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:43:48,060 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:43:48,183 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:43:48,188 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:43:48,192 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:43:48,504 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-8500] due to args.save_total_limit\n",
            "{'loss': 4.7669, 'learning_rate': 4.561557348298843e-05, 'epoch': 1.75}\n",
            "  9% 10000/114040 [08:53<1:21:47, 21.20it/s][INFO|trainer.py:1928] 2021-08-17 23:44:14,056 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:44:14,061 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:44:14,184 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:44:14,189 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:44:14,193 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:44:14,512 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9000] due to args.save_total_limit\n",
            "{'loss': 4.7579, 'learning_rate': 4.539635215713785e-05, 'epoch': 1.84}\n",
            "  9% 10500/114040 [09:21<1:34:39, 18.23it/s][INFO|trainer.py:1928] 2021-08-17 23:44:41,316 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:44:41,321 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:44:41,449 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:44:41,454 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:44:41,458 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:44:41,742 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-9500] due to args.save_total_limit\n",
            "{'loss': 4.5964, 'learning_rate': 4.517713083128727e-05, 'epoch': 1.93}\n",
            " 10% 11000/114040 [09:48<1:27:57, 19.52it/s][INFO|trainer.py:1928] 2021-08-17 23:45:08,136 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:45:08,142 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:45:08,269 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:45:08,274 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:45:08,278 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:45:08,567 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 4.5825, 'learning_rate': 4.495790950543669e-05, 'epoch': 2.02}\n",
            " 10% 11500/114040 [10:14<1:19:08, 21.59it/s][INFO|trainer.py:1928] 2021-08-17 23:45:34,460 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:45:34,467 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:45:34,605 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:45:34,611 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:45:34,614 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:45:34,896 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-10500] due to args.save_total_limit\n",
            "{'loss': 4.4944, 'learning_rate': 4.4738688179586116e-05, 'epoch': 2.1}\n",
            " 11% 12000/114040 [10:41<1:46:03, 16.04it/s][INFO|trainer.py:1928] 2021-08-17 23:46:01,738 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:46:01,744 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:46:01,869 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:46:01,873 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:46:01,877 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:46:02,196 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11000] due to args.save_total_limit\n",
            "{'loss': 4.5068, 'learning_rate': 4.451946685373553e-05, 'epoch': 2.19}\n",
            " 11% 12500/114040 [11:08<1:18:12, 21.64it/s][INFO|trainer.py:1928] 2021-08-17 23:46:28,305 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:46:28,310 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:46:28,431 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:46:28,436 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:46:28,440 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:46:28,746 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-11500] due to args.save_total_limit\n",
            "{'loss': 4.518, 'learning_rate': 4.4300245527884956e-05, 'epoch': 2.28}\n",
            " 11% 13000/114040 [11:35<1:58:34, 14.20it/s][INFO|trainer.py:1928] 2021-08-17 23:46:55,619 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:46:55,626 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:46:55,743 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:46:55,748 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:46:55,752 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:46:56,042 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 4.414, 'learning_rate': 4.408102420203438e-05, 'epoch': 2.37}\n",
            " 12% 13500/114040 [12:02<1:32:30, 18.11it/s][INFO|trainer.py:1928] 2021-08-17 23:47:22,178 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:47:22,183 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:47:22,311 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:47:22,317 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:47:22,320 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:47:22,618 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-12500] due to args.save_total_limit\n",
            "{'loss': 4.4084, 'learning_rate': 4.38618028761838e-05, 'epoch': 2.46}\n",
            " 12% 14000/114040 [12:29<1:39:39, 16.73it/s][INFO|trainer.py:1928] 2021-08-17 23:47:49,783 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:47:49,788 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:47:49,912 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:47:49,917 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:47:49,921 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:47:50,218 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13000] due to args.save_total_limit\n",
            "{'loss': 4.3762, 'learning_rate': 4.364258155033322e-05, 'epoch': 2.54}\n",
            " 13% 14500/114040 [12:55<1:16:07, 21.80it/s][INFO|trainer.py:1928] 2021-08-17 23:48:15,476 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:48:15,481 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:48:15,605 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:48:15,629 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:48:15,633 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:48:15,911 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-13500] due to args.save_total_limit\n",
            "{'loss': 4.2689, 'learning_rate': 4.342336022448264e-05, 'epoch': 2.63}\n",
            " 13% 15000/114040 [13:22<1:31:08, 18.11it/s][INFO|trainer.py:1928] 2021-08-17 23:48:42,476 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:48:42,481 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:48:42,601 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:48:42,606 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:48:42,610 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:48:42,930 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 4.2515, 'learning_rate': 4.3204138898632066e-05, 'epoch': 2.72}\n",
            " 14% 15500/114040 [13:49<1:44:07, 15.77it/s][INFO|trainer.py:1928] 2021-08-17 23:49:09,291 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:49:09,297 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:49:09,421 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:49:09,426 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:49:09,430 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:49:09,745 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-14500] due to args.save_total_limit\n",
            "{'loss': 4.0958, 'learning_rate': 4.298491757278148e-05, 'epoch': 2.81}\n",
            " 14% 16000/114040 [14:15<1:24:24, 19.36it/s][INFO|trainer.py:1928] 2021-08-17 23:49:35,495 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:49:35,499 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:49:35,613 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:49:35,618 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:49:35,621 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:49:35,911 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 4.1728, 'learning_rate': 4.2765696246930906e-05, 'epoch': 2.89}\n",
            " 14% 16500/114040 [14:41<1:11:56, 22.60it/s][INFO|trainer.py:1928] 2021-08-17 23:50:01,771 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:50:01,776 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:50:01,897 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:50:01,902 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:50:01,905 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:50:02,197 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-15500] due to args.save_total_limit\n",
            "{'loss': 4.1075, 'learning_rate': 4.254647492108033e-05, 'epoch': 2.98}\n",
            " 15% 17000/114040 [15:09<1:23:16, 19.42it/s][INFO|trainer.py:1928] 2021-08-17 23:50:29,112 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:50:29,117 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:50:29,234 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:50:29,242 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:50:29,246 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:50:29,537 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 4.1572, 'learning_rate': 4.2327253595229746e-05, 'epoch': 3.07}\n",
            " 15% 17500/114040 [15:36<1:46:55, 15.05it/s][INFO|trainer.py:1928] 2021-08-17 23:50:56,332 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:50:56,337 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:50:56,453 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:50:56,457 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:50:56,478 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:50:56,768 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-16500] due to args.save_total_limit\n",
            "{'loss': 4.0181, 'learning_rate': 4.210803226937917e-05, 'epoch': 3.16}\n",
            " 16% 18000/114040 [16:02<1:15:03, 21.33it/s][INFO|trainer.py:1928] 2021-08-17 23:51:22,775 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:51:22,780 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:51:22,905 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:51:22,910 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:51:22,914 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:51:23,202 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17000] due to args.save_total_limit\n",
            "{'loss': 4.0189, 'learning_rate': 4.188881094352859e-05, 'epoch': 3.24}\n",
            " 16% 18500/114040 [16:29<1:05:21, 24.36it/s][INFO|trainer.py:1928] 2021-08-17 23:51:49,453 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:51:49,459 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:51:49,592 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:51:49,598 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:51:49,602 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:51:49,930 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-17500] due to args.save_total_limit\n",
            "{'loss': 4.0089, 'learning_rate': 4.166958961767801e-05, 'epoch': 3.33}\n",
            " 17% 19000/114040 [16:56<1:25:32, 18.52it/s][INFO|trainer.py:1928] 2021-08-17 23:52:16,256 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:52:16,262 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:52:16,387 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:52:16,392 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:52:16,397 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:52:16,676 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18000] due to args.save_total_limit\n",
            "{'loss': 3.8342, 'learning_rate': 4.145036829182743e-05, 'epoch': 3.42}\n",
            " 17% 19500/114040 [17:22<1:25:06, 18.51it/s][INFO|trainer.py:1928] 2021-08-17 23:52:42,935 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:52:42,941 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:52:43,057 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:52:43,062 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:52:43,066 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:52:43,365 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-18500] due to args.save_total_limit\n",
            "{'loss': 4.0196, 'learning_rate': 4.123114696597685e-05, 'epoch': 3.51}\n",
            " 18% 20000/114040 [17:49<1:10:25, 22.26it/s][INFO|trainer.py:1928] 2021-08-17 23:53:09,798 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:53:09,819 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:53:09,946 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:53:09,951 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:53:09,955 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:53:10,239 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19000] due to args.save_total_limit\n",
            "{'loss': 3.8019, 'learning_rate': 4.101192564012627e-05, 'epoch': 3.6}\n",
            " 18% 20500/114040 [18:15<1:20:21, 19.40it/s][INFO|trainer.py:1928] 2021-08-17 23:53:35,566 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:53:35,571 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:53:35,690 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:53:35,696 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:53:35,700 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:53:36,001 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-19500] due to args.save_total_limit\n",
            "{'loss': 3.8105, 'learning_rate': 4.0792704314275696e-05, 'epoch': 3.68}\n",
            " 18% 21000/114040 [18:42<1:05:23, 23.71it/s][INFO|trainer.py:1928] 2021-08-17 23:54:02,636 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:54:02,645 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:54:02,763 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:54:02,768 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:54:02,771 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:54:03,087 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 3.8496, 'learning_rate': 4.057348298842511e-05, 'epoch': 3.77}\n",
            " 19% 21500/114040 [19:08<1:10:13, 21.96it/s][INFO|trainer.py:1928] 2021-08-17 23:54:29,037 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:54:29,042 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:54:29,168 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:54:29,174 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:54:29,178 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:54:29,484 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-20500] due to args.save_total_limit\n",
            "{'loss': 3.8513, 'learning_rate': 4.0354261662574536e-05, 'epoch': 3.86}\n",
            " 19% 22000/114040 [19:35<1:16:58, 19.93it/s][INFO|trainer.py:1928] 2021-08-17 23:54:55,509 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:54:55,517 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:54:55,635 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:54:55,639 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:54:55,643 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:54:55,941 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21000] due to args.save_total_limit\n",
            "{'loss': 3.6875, 'learning_rate': 4.013504033672396e-05, 'epoch': 3.95}\n",
            " 20% 22500/114040 [20:01<1:04:49, 23.53it/s][INFO|trainer.py:1928] 2021-08-17 23:55:22,043 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:55:22,048 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:55:22,164 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:55:22,169 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:55:22,173 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:55:22,455 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-21500] due to args.save_total_limit\n",
            "{'loss': 3.7988, 'learning_rate': 3.9915819010873376e-05, 'epoch': 4.03}\n",
            " 20% 23000/114040 [20:28<1:34:49, 16.00it/s][INFO|trainer.py:1928] 2021-08-17 23:55:48,800 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:55:48,821 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:55:48,947 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:55:48,952 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:55:48,956 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:55:49,254 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22000] due to args.save_total_limit\n",
            "{'loss': 3.7059, 'learning_rate': 3.96965976850228e-05, 'epoch': 4.12}\n",
            " 21% 23500/114040 [20:55<1:09:06, 21.84it/s][INFO|trainer.py:1928] 2021-08-17 23:56:15,812 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:56:15,817 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:56:15,941 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:56:15,946 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:56:15,950 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:56:16,259 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-22500] due to args.save_total_limit\n",
            "{'loss': 3.6174, 'learning_rate': 3.947737635917222e-05, 'epoch': 4.21}\n",
            " 21% 24000/114040 [21:22<1:30:36, 16.56it/s][INFO|trainer.py:1928] 2021-08-17 23:56:42,418 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:56:42,424 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:56:42,547 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:56:42,552 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:56:42,556 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:56:42,874 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23000] due to args.save_total_limit\n",
            "{'loss': 3.5552, 'learning_rate': 3.925815503332164e-05, 'epoch': 4.3}\n",
            " 21% 24500/114040 [21:49<1:16:08, 19.60it/s][INFO|trainer.py:1928] 2021-08-17 23:57:09,228 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:57:09,235 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:57:09,360 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:57:09,365 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:57:09,369 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:57:09,670 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-23500] due to args.save_total_limit\n",
            "{'loss': 3.6925, 'learning_rate': 3.903893370747106e-05, 'epoch': 4.38}\n",
            " 22% 25000/114040 [22:16<1:51:30, 13.31it/s][INFO|trainer.py:1928] 2021-08-17 23:57:36,694 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:57:36,700 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:57:36,829 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:57:36,835 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:57:36,839 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:57:37,132 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24000] due to args.save_total_limit\n",
            "{'loss': 3.7664, 'learning_rate': 3.8819712381620486e-05, 'epoch': 4.47}\n",
            " 22% 25500/114040 [22:43<1:09:51, 21.12it/s][INFO|trainer.py:1928] 2021-08-17 23:58:03,395 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:58:03,401 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:58:03,526 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:58:03,531 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:58:03,535 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:58:03,826 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-24500] due to args.save_total_limit\n",
            "{'loss': 3.5882, 'learning_rate': 3.86004910557699e-05, 'epoch': 4.56}\n",
            " 23% 26000/114040 [23:10<1:25:14, 17.21it/s][INFO|trainer.py:1928] 2021-08-17 23:58:30,470 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:58:30,476 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:58:30,602 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:58:30,624 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:58:30,628 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:58:30,917 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25000] due to args.save_total_limit\n",
            "{'loss': 3.5647, 'learning_rate': 3.8381269729919326e-05, 'epoch': 4.65}\n",
            " 23% 26500/114040 [23:37<1:28:53, 16.41it/s][INFO|trainer.py:1928] 2021-08-17 23:58:57,479 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:58:57,485 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:58:57,599 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:58:57,603 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:58:57,608 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:58:57,927 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-25500] due to args.save_total_limit\n",
            "{'loss': 3.6017, 'learning_rate': 3.816204840406875e-05, 'epoch': 4.74}\n",
            " 24% 27000/114040 [24:03<1:20:15, 18.07it/s][INFO|trainer.py:1928] 2021-08-17 23:59:23,894 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27000\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:59:23,901 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:59:24,025 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:59:24,031 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:59:24,035 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:59:24,352 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26000] due to args.save_total_limit\n",
            "{'loss': 3.4588, 'learning_rate': 3.7942827078218166e-05, 'epoch': 4.82}\n",
            " 24% 27500/114040 [24:29<1:12:31, 19.89it/s][INFO|trainer.py:1928] 2021-08-17 23:59:49,462 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27500\n",
            "[INFO|configuration_utils.py:379] 2021-08-17 23:59:49,468 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-17 23:59:49,594 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-17 23:59:49,599 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-17 23:59:49,603 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-17 23:59:49,890 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-26500] due to args.save_total_limit\n",
            "{'loss': 3.5834, 'learning_rate': 3.772360575236759e-05, 'epoch': 4.91}\n",
            " 25% 28000/114040 [24:55<1:32:30, 15.50it/s][INFO|trainer.py:1928] 2021-08-18 00:00:16,096 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:00:16,102 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:00:16,223 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:00:16,228 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:00:16,232 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:00:16,507 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27000] due to args.save_total_limit\n",
            "{'loss': 3.6395, 'learning_rate': 3.750438442651701e-05, 'epoch': 5.0}\n",
            " 25% 28500/114040 [25:22<1:23:48, 17.01it/s][INFO|trainer.py:1928] 2021-08-18 00:00:42,938 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:00:42,945 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:00:43,078 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:00:43,083 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:00:43,087 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:00:43,364 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-27500] due to args.save_total_limit\n",
            "{'loss': 3.4941, 'learning_rate': 3.7285163100666436e-05, 'epoch': 5.09}\n",
            " 25% 29000/114040 [25:48<54:47, 25.87it/s][INFO|trainer.py:1928] 2021-08-18 00:01:08,511 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:01:08,517 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:01:08,641 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:01:08,645 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:01:08,666 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:01:08,945 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28000] due to args.save_total_limit\n",
            "{'loss': 3.4761, 'learning_rate': 3.706594177481585e-05, 'epoch': 5.17}\n",
            " 26% 29500/114040 [26:15<1:14:06, 19.01it/s][INFO|trainer.py:1928] 2021-08-18 00:01:35,677 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:01:35,682 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:01:35,807 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:01:35,813 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:01:35,817 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:01:36,130 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-28500] due to args.save_total_limit\n",
            "{'loss': 3.5413, 'learning_rate': 3.6846720448965276e-05, 'epoch': 5.26}\n",
            " 26% 30000/114040 [26:42<1:32:12, 15.19it/s][INFO|trainer.py:1928] 2021-08-18 00:02:02,465 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:02:02,471 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:02:02,596 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:02:02,602 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:02:02,606 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:02:02,907 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29000] due to args.save_total_limit\n",
            "{'loss': 3.2852, 'learning_rate': 3.66274991231147e-05, 'epoch': 5.35}\n",
            " 27% 30500/114040 [27:08<1:11:12, 19.56it/s][INFO|trainer.py:1928] 2021-08-18 00:02:28,279 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:02:28,284 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:02:28,405 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:02:28,411 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:02:28,415 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:02:28,695 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-29500] due to args.save_total_limit\n",
            "{'loss': 3.415, 'learning_rate': 3.6408277797264116e-05, 'epoch': 5.44}\n",
            " 27% 31000/114040 [27:35<1:06:28, 20.82it/s][INFO|trainer.py:1928] 2021-08-18 00:02:55,175 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:02:55,180 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:02:55,302 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:02:55,306 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:02:55,311 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:02:55,602 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30000] due to args.save_total_limit\n",
            "{'loss': 3.4228, 'learning_rate': 3.618905647141354e-05, 'epoch': 5.52}\n",
            " 28% 31500/114040 [28:02<1:45:00, 13.10it/s][INFO|trainer.py:1928] 2021-08-18 00:03:22,485 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:03:22,503 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:03:22,623 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:03:22,628 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:03:22,632 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:03:22,931 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-30500] due to args.save_total_limit\n",
            "{'loss': 3.3438, 'learning_rate': 3.596983514556296e-05, 'epoch': 5.61}\n",
            " 28% 32000/114040 [28:29<58:25, 23.40it/s][INFO|trainer.py:1928] 2021-08-18 00:03:49,502 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:03:49,507 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:03:49,624 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:03:49,631 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:03:49,654 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:03:49,949 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31000] due to args.save_total_limit\n",
            "{'loss': 3.3974, 'learning_rate': 3.575061381971238e-05, 'epoch': 5.7}\n",
            " 28% 32500/114040 [28:56<1:07:17, 20.20it/s][INFO|trainer.py:1928] 2021-08-18 00:04:17,000 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:04:17,006 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:04:17,129 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:04:17,134 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:04:17,138 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:04:17,453 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-31500] due to args.save_total_limit\n",
            "{'loss': 3.3036, 'learning_rate': 3.55313924938618e-05, 'epoch': 5.79}\n",
            " 29% 33000/114040 [29:23<1:02:13, 21.71it/s][INFO|trainer.py:1928] 2021-08-18 00:04:44,103 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:04:44,108 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:04:44,231 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:04:44,236 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:04:44,240 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:04:44,542 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32000] due to args.save_total_limit\n",
            "{'loss': 3.359, 'learning_rate': 3.5312171168011226e-05, 'epoch': 5.88}\n",
            " 29% 33500/114040 [29:50<59:02, 22.74it/s][INFO|trainer.py:1928] 2021-08-18 00:05:11,031 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:05:11,036 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:05:11,157 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:05:11,162 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:05:11,166 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:05:11,466 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-32500] due to args.save_total_limit\n",
            "{'loss': 3.2543, 'learning_rate': 3.509294984216065e-05, 'epoch': 5.96}\n",
            " 30% 34000/114040 [30:18<1:10:45, 18.85it/s][INFO|trainer.py:1928] 2021-08-18 00:05:38,186 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:05:38,192 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:05:38,324 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:05:38,329 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:05:38,334 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:05:38,637 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33000] due to args.save_total_limit\n",
            "{'loss': 3.2732, 'learning_rate': 3.4873728516310066e-05, 'epoch': 6.05}\n",
            " 30% 34500/114040 [30:45<1:30:52, 14.59it/s][INFO|trainer.py:1928] 2021-08-18 00:06:05,926 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:06:05,947 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:06:06,068 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:06:06,073 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:06:06,077 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:06:06,372 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-33500] due to args.save_total_limit\n",
            "{'loss': 3.1647, 'learning_rate': 3.465450719045949e-05, 'epoch': 6.14}\n",
            " 31% 35000/114040 [31:11<1:09:50, 18.86it/s][INFO|trainer.py:1928] 2021-08-18 00:06:31,893 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:06:31,899 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:06:32,025 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:06:32,032 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:06:32,036 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:06:32,342 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34000] due to args.save_total_limit\n",
            "{'loss': 3.1599, 'learning_rate': 3.443528586460891e-05, 'epoch': 6.23}\n",
            " 31% 35500/114040 [31:38<1:03:41, 20.55it/s][INFO|trainer.py:1928] 2021-08-18 00:06:58,571 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:06:58,576 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:06:58,704 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:06:58,709 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:06:58,714 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:06:59,028 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-34500] due to args.save_total_limit\n",
            "{'loss': 3.098, 'learning_rate': 3.421606453875833e-05, 'epoch': 6.31}\n",
            " 32% 36000/114040 [32:04<1:06:32, 19.54it/s][INFO|trainer.py:1928] 2021-08-18 00:07:24,610 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:07:24,617 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:07:24,744 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:07:24,750 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:07:24,754 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:07:25,081 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35000] due to args.save_total_limit\n",
            "{'loss': 3.1846, 'learning_rate': 3.399684321290775e-05, 'epoch': 6.4}\n",
            " 32% 36500/114040 [32:30<1:03:52, 20.23it/s][INFO|trainer.py:1928] 2021-08-18 00:07:50,798 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:07:50,803 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:07:50,931 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:07:50,936 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:07:50,942 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:07:51,226 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-35500] due to args.save_total_limit\n",
            "{'loss': 3.0825, 'learning_rate': 3.3777621887057176e-05, 'epoch': 6.49}\n",
            " 32% 37000/114040 [32:56<1:07:10, 19.12it/s][INFO|trainer.py:1928] 2021-08-18 00:08:16,679 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:08:16,684 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:08:16,811 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:08:16,816 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:08:16,821 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:08:17,118 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36000] due to args.save_total_limit\n",
            "{'loss': 3.1529, 'learning_rate': 3.355840056120659e-05, 'epoch': 6.58}\n",
            " 33% 37500/114040 [33:21<57:05, 22.35it/s][INFO|trainer.py:1928] 2021-08-18 00:08:42,100 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:08:42,106 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:08:42,244 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:08:42,250 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:08:42,254 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:08:42,551 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-36500] due to args.save_total_limit\n",
            "{'loss': 3.2708, 'learning_rate': 3.3339179235356016e-05, 'epoch': 6.66}\n",
            " 33% 38000/114040 [33:49<1:14:28, 17.02it/s][INFO|trainer.py:1928] 2021-08-18 00:09:09,213 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:09:09,218 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:09:09,335 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:09:09,340 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:09:09,344 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:09:09,658 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37000] due to args.save_total_limit\n",
            "{'loss': 3.2036, 'learning_rate': 3.311995790950544e-05, 'epoch': 6.75}\n",
            " 34% 38500/114040 [34:16<1:20:19, 15.67it/s][INFO|trainer.py:1928] 2021-08-18 00:09:36,915 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:09:36,921 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:09:37,047 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:09:37,052 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:09:37,056 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:09:37,374 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-37500] due to args.save_total_limit\n",
            "{'loss': 3.2454, 'learning_rate': 3.290073658365486e-05, 'epoch': 6.84}\n",
            " 34% 39000/114040 [34:44<1:05:08, 19.20it/s][INFO|trainer.py:1928] 2021-08-18 00:10:04,843 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:10:04,848 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:10:04,968 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:10:04,973 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:10:04,977 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:10:05,280 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38000] due to args.save_total_limit\n",
            "{'loss': 3.1332, 'learning_rate': 3.268151525780428e-05, 'epoch': 6.93}\n",
            " 35% 39500/114040 [35:12<1:10:19, 17.66it/s][INFO|trainer.py:1928] 2021-08-18 00:10:32,722 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:10:32,728 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:10:32,849 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:10:32,855 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:10:32,858 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:10:33,164 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-38500] due to args.save_total_limit\n",
            "{'loss': 3.1206, 'learning_rate': 3.24622939319537e-05, 'epoch': 7.02}\n",
            " 35% 40000/114040 [35:39<1:04:30, 19.13it/s][INFO|trainer.py:1928] 2021-08-18 00:10:59,388 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:10:59,393 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:10:59,519 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:10:59,524 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:10:59,529 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:10:59,818 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39000] due to args.save_total_limit\n",
            "{'loss': 2.9753, 'learning_rate': 3.2243072606103126e-05, 'epoch': 7.1}\n",
            " 36% 40500/114040 [36:05<1:05:45, 18.64it/s][INFO|trainer.py:1928] 2021-08-18 00:11:25,196 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:11:25,206 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:11:25,331 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:11:25,337 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:11:25,341 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:11:25,633 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-39500] due to args.save_total_limit\n",
            "{'loss': 3.203, 'learning_rate': 3.202385128025254e-05, 'epoch': 7.19}\n",
            " 36% 41000/114040 [36:32<59:47, 20.36it/s][INFO|trainer.py:1928] 2021-08-18 00:11:52,415 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:11:52,421 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:11:52,543 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:11:52,548 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:11:52,552 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:11:52,852 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40000] due to args.save_total_limit\n",
            "{'loss': 3.0391, 'learning_rate': 3.1804629954401966e-05, 'epoch': 7.28}\n",
            " 36% 41500/114040 [36:59<1:03:46, 18.96it/s][INFO|trainer.py:1928] 2021-08-18 00:12:20,048 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:12:20,053 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:12:20,176 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:12:20,181 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:12:20,185 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:12:20,495 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-40500] due to args.save_total_limit\n",
            "{'loss': 2.9883, 'learning_rate': 3.158540862855139e-05, 'epoch': 7.37}\n",
            " 37% 42000/114040 [37:26<1:10:37, 17.00it/s][INFO|trainer.py:1928] 2021-08-18 00:12:46,127 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:12:46,132 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:12:46,255 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:12:46,260 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:12:46,263 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:12:46,568 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41000] due to args.save_total_limit\n",
            "{'loss': 2.9758, 'learning_rate': 3.1366187302700806e-05, 'epoch': 7.45}\n",
            " 37% 42500/114040 [37:52<1:06:10, 18.02it/s][INFO|trainer.py:1928] 2021-08-18 00:13:12,712 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:13:12,717 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:13:12,839 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:13:12,844 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:13:12,848 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:13:13,150 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-41500] due to args.save_total_limit\n",
            "{'loss': 3.0145, 'learning_rate': 3.114696597685023e-05, 'epoch': 7.54}\n",
            " 38% 43000/114040 [38:19<1:07:45, 17.47it/s][INFO|trainer.py:1928] 2021-08-18 00:13:40,074 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:13:40,080 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:13:40,199 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:13:40,205 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:13:40,208 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:13:40,508 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42000] due to args.save_total_limit\n",
            "{'loss': 3.0168, 'learning_rate': 3.092774465099965e-05, 'epoch': 7.63}\n",
            " 38% 43500/114040 [38:46<1:13:57, 15.90it/s][INFO|trainer.py:1928] 2021-08-18 00:14:06,472 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:14:06,478 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:14:06,621 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:14:06,626 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:14:06,630 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:14:06,927 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-42500] due to args.save_total_limit\n",
            "{'loss': 2.8899, 'learning_rate': 3.0708523325149076e-05, 'epoch': 7.72}\n",
            " 39% 44000/114040 [39:12<55:42, 20.95it/s][INFO|trainer.py:1928] 2021-08-18 00:14:32,852 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:14:32,858 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:14:32,990 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:14:32,996 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:14:33,019 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:14:33,333 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43000] due to args.save_total_limit\n",
            "{'loss': 3.0083, 'learning_rate': 3.0489301999298492e-05, 'epoch': 7.8}\n",
            " 39% 44500/114040 [39:40<1:11:46, 16.15it/s][INFO|trainer.py:1928] 2021-08-18 00:15:00,414 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:15:00,419 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:15:00,540 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:15:00,547 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:15:00,551 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:15:00,934 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-43500] due to args.save_total_limit\n",
            "{'loss': 2.9509, 'learning_rate': 3.0270080673447916e-05, 'epoch': 7.89}\n",
            " 39% 45000/114040 [40:07<53:02, 21.69it/s][INFO|trainer.py:1928] 2021-08-18 00:15:27,229 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:15:27,235 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:15:27,355 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:15:27,361 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:15:27,366 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:15:28,133 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44000] due to args.save_total_limit\n",
            "{'loss': 2.9503, 'learning_rate': 3.005085934759734e-05, 'epoch': 7.98}\n",
            " 40% 45500/114040 [40:33<1:37:20, 11.73it/s][INFO|trainer.py:1928] 2021-08-18 00:15:53,703 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:15:53,708 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:15:53,834 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:15:53,839 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:15:53,843 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:15:54,158 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-44500] due to args.save_total_limit\n",
            "{'loss': 3.0081, 'learning_rate': 2.9831638021746756e-05, 'epoch': 8.07}\n",
            " 40% 46000/114040 [41:01<1:05:11, 17.39it/s][INFO|trainer.py:1928] 2021-08-18 00:16:21,521 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:16:21,527 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:16:21,653 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:16:21,659 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:16:21,663 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:16:21,976 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45000] due to args.save_total_limit\n",
            "{'loss': 2.8233, 'learning_rate': 2.961241669589618e-05, 'epoch': 8.16}\n",
            " 41% 46500/114040 [41:27<53:07, 21.19it/s][INFO|trainer.py:1928] 2021-08-18 00:16:47,798 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:16:47,818 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:16:47,940 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:16:47,954 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:16:47,959 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:16:48,262 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-45500] due to args.save_total_limit\n",
            "{'loss': 2.9057, 'learning_rate': 2.9393195370045603e-05, 'epoch': 8.24}\n",
            " 41% 47000/114040 [41:54<52:26, 21.30it/s][INFO|trainer.py:1928] 2021-08-18 00:17:14,733 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:17:14,739 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:17:14,865 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:17:14,870 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:17:14,891 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:17:15,194 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46000] due to args.save_total_limit\n",
            "{'loss': 2.9545, 'learning_rate': 2.917397404419502e-05, 'epoch': 8.33}\n",
            " 42% 47500/114040 [42:21<1:05:53, 16.83it/s][INFO|trainer.py:1928] 2021-08-18 00:17:42,057 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:17:42,062 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:17:42,190 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:17:42,195 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:17:42,200 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:17:42,537 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-46500] due to args.save_total_limit\n",
            "{'loss': 2.8827, 'learning_rate': 2.8954752718344443e-05, 'epoch': 8.42}\n",
            " 42% 48000/114040 [42:48<57:21, 19.19it/s][INFO|trainer.py:1928] 2021-08-18 00:18:08,515 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:18:08,521 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:18:08,644 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:18:08,650 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:18:08,654 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:18:08,982 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47000] due to args.save_total_limit\n",
            "{'loss': 2.9846, 'learning_rate': 2.8735531392493863e-05, 'epoch': 8.51}\n",
            " 43% 48500/114040 [43:14<56:30, 19.33it/s][INFO|trainer.py:1928] 2021-08-18 00:18:35,009 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:18:35,015 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:18:35,144 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:18:35,149 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:18:35,153 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:18:35,472 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-47500] due to args.save_total_limit\n",
            "{'loss': 2.8571, 'learning_rate': 2.8516310066643286e-05, 'epoch': 8.59}\n",
            " 43% 49000/114040 [43:42<55:27, 19.55it/s][INFO|trainer.py:1928] 2021-08-18 00:19:03,054 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:19:03,060 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:19:03,194 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:19:03,199 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:19:03,203 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:19:03,505 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48000] due to args.save_total_limit\n",
            "{'loss': 2.8184, 'learning_rate': 2.8297088740792706e-05, 'epoch': 8.68}\n",
            " 43% 49500/114040 [44:09<1:07:19, 15.98it/s][INFO|trainer.py:1928] 2021-08-18 00:19:29,780 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:19:29,801 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:19:29,927 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:19:29,932 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:19:29,936 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:19:30,242 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-48500] due to args.save_total_limit\n",
            "{'loss': 2.8811, 'learning_rate': 2.8077867414942126e-05, 'epoch': 8.77}\n",
            " 44% 50000/114040 [44:36<1:01:50, 17.26it/s][INFO|trainer.py:1928] 2021-08-18 00:19:56,435 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:19:56,441 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:19:56,560 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:19:56,565 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:19:56,585 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:19:56,875 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49000] due to args.save_total_limit\n",
            "{'loss': 2.784, 'learning_rate': 2.785864608909155e-05, 'epoch': 8.86}\n",
            " 44% 50500/114040 [45:02<1:10:48, 14.95it/s][INFO|trainer.py:1928] 2021-08-18 00:20:23,098 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:20:23,104 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:20:23,227 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:20:23,233 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:20:23,237 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:20:23,537 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-49500] due to args.save_total_limit\n",
            "{'loss': 2.8172, 'learning_rate': 2.763942476324097e-05, 'epoch': 8.94}\n",
            " 45% 51000/114040 [45:29<53:27, 19.65it/s][INFO|trainer.py:1928] 2021-08-18 00:20:50,070 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:20:50,075 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:20:50,192 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:20:50,198 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:20:50,202 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:20:50,783 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50000] due to args.save_total_limit\n",
            "{'loss': 2.7245, 'learning_rate': 2.742020343739039e-05, 'epoch': 9.03}\n",
            " 45% 51500/114040 [45:55<48:51, 21.34it/s][INFO|trainer.py:1928] 2021-08-18 00:21:15,941 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:21:15,947 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:21:16,073 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:21:16,078 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:21:16,082 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:21:16,372 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-50500] due to args.save_total_limit\n",
            "{'loss': 2.777, 'learning_rate': 2.7200982111539813e-05, 'epoch': 9.12}\n",
            " 46% 52000/114040 [46:23<44:44, 23.11it/s][INFO|trainer.py:1928] 2021-08-18 00:21:43,330 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:21:43,337 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:21:43,464 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:21:43,469 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:21:43,473 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:21:43,767 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51000] due to args.save_total_limit\n",
            "{'loss': 2.8269, 'learning_rate': 2.698176078568923e-05, 'epoch': 9.21}\n",
            " 46% 52500/114040 [46:50<58:57, 17.40it/s][INFO|trainer.py:1928] 2021-08-18 00:22:11,099 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:22:11,106 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:22:11,228 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:22:11,234 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:22:11,238 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:22:11,530 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-51500] due to args.save_total_limit\n",
            "{'loss': 2.7752, 'learning_rate': 2.6762539459838652e-05, 'epoch': 9.29}\n",
            " 46% 53000/114040 [47:18<47:54, 21.24it/s][INFO|trainer.py:1928] 2021-08-18 00:22:38,345 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:22:38,351 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:22:38,477 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:22:38,481 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:22:38,504 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:22:38,801 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52000] due to args.save_total_limit\n",
            "{'loss': 2.7574, 'learning_rate': 2.6543318133988076e-05, 'epoch': 9.38}\n",
            " 47% 53500/114040 [47:44<59:41, 16.90it/s][INFO|trainer.py:1928] 2021-08-18 00:23:04,954 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:23:04,958 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:23:05,077 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:23:05,082 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:23:05,085 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:23:05,386 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-52500] due to args.save_total_limit\n",
            "{'loss': 2.7054, 'learning_rate': 2.63240968081375e-05, 'epoch': 9.47}\n",
            " 47% 54000/114040 [48:10<51:38, 19.38it/s][INFO|trainer.py:1928] 2021-08-18 00:23:30,617 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:23:30,624 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:23:30,753 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:23:30,758 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:23:30,762 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:23:31,095 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53000] due to args.save_total_limit\n",
            "{'loss': 2.6723, 'learning_rate': 2.6104875482286916e-05, 'epoch': 9.56}\n",
            " 48% 54500/114040 [48:36<54:21, 18.25it/s][INFO|trainer.py:1928] 2021-08-18 00:23:56,578 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:23:56,584 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:23:56,707 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:23:56,712 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:23:56,716 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:23:57,004 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-53500] due to args.save_total_limit\n",
            "{'loss': 2.7858, 'learning_rate': 2.588565415643634e-05, 'epoch': 9.65}\n",
            " 48% 55000/114040 [49:03<1:00:56, 16.15it/s][INFO|trainer.py:1928] 2021-08-18 00:24:23,999 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:24:24,004 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:24:24,126 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:24:24,131 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:24:24,135 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:24:24,424 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54000] due to args.save_total_limit\n",
            "{'loss': 2.7296, 'learning_rate': 2.5666432830585763e-05, 'epoch': 9.73}\n",
            " 49% 55500/114040 [49:31<54:20, 17.95it/s][INFO|trainer.py:1928] 2021-08-18 00:24:51,113 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:24:51,119 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:24:51,244 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:24:51,249 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:24:51,253 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:24:51,549 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-54500] due to args.save_total_limit\n",
            "{'loss': 2.7479, 'learning_rate': 2.544721150473518e-05, 'epoch': 9.82}\n",
            " 49% 56000/114040 [49:58<50:16, 19.24it/s][INFO|trainer.py:1928] 2021-08-18 00:25:18,887 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:25:18,893 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:25:19,012 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:25:19,017 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:25:19,038 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:25:19,340 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55000] due to args.save_total_limit\n",
            "{'loss': 2.7732, 'learning_rate': 2.5227990178884603e-05, 'epoch': 9.91}\n",
            " 50% 56500/114040 [50:25<56:07, 17.09it/s][INFO|trainer.py:1928] 2021-08-18 00:25:45,374 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:25:45,381 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:25:45,507 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:25:45,512 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:25:45,517 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:25:45,832 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-55500] due to args.save_total_limit\n",
            "{'loss': 2.623, 'learning_rate': 2.5008768853034026e-05, 'epoch': 10.0}\n",
            " 50% 57000/114040 [50:51<50:44, 18.74it/s][INFO|trainer.py:1928] 2021-08-18 00:26:11,392 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:26:11,397 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:26:11,515 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:26:11,520 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:26:11,524 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:26:11,825 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56000] due to args.save_total_limit\n",
            "{'loss': 2.6321, 'learning_rate': 2.4789547527183446e-05, 'epoch': 10.08}\n",
            " 50% 57500/114040 [51:17<41:28, 22.72it/s][INFO|trainer.py:1928] 2021-08-18 00:26:37,272 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:26:37,278 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:26:37,400 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:26:37,405 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:26:37,409 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:26:37,696 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-56500] due to args.save_total_limit\n",
            "{'loss': 2.7681, 'learning_rate': 2.4570326201332866e-05, 'epoch': 10.17}\n",
            " 51% 58000/114040 [51:45<48:55, 19.09it/s][INFO|trainer.py:1928] 2021-08-18 00:27:05,601 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:27:05,607 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:27:05,733 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:27:05,738 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:27:05,742 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:27:06,034 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57000] due to args.save_total_limit\n",
            "{'loss': 2.6857, 'learning_rate': 2.435110487548229e-05, 'epoch': 10.26}\n",
            " 51% 58500/114040 [52:11<46:43, 19.81it/s][INFO|trainer.py:1928] 2021-08-18 00:27:31,946 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:27:31,967 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:27:32,091 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:27:32,096 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:27:32,100 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:27:32,395 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-57500] due to args.save_total_limit\n",
            "{'loss': 2.6491, 'learning_rate': 2.413188354963171e-05, 'epoch': 10.35}\n",
            " 52% 59000/114040 [52:39<50:27, 18.18it/s][INFO|trainer.py:1928] 2021-08-18 00:27:59,446 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:27:59,452 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:27:59,580 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:27:59,585 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:27:59,610 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:27:59,911 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58000] due to args.save_total_limit\n",
            "{'loss': 2.5705, 'learning_rate': 2.391266222378113e-05, 'epoch': 10.43}\n",
            " 52% 59500/114040 [53:06<51:58, 17.49it/s][INFO|trainer.py:1928] 2021-08-18 00:28:26,423 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:28:26,429 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:28:26,545 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:28:26,550 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:28:26,557 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:28:26,868 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-58500] due to args.save_total_limit\n",
            "{'loss': 2.6635, 'learning_rate': 2.3693440897930553e-05, 'epoch': 10.52}\n",
            " 53% 60000/114040 [53:32<53:15, 16.91it/s][INFO|trainer.py:1928] 2021-08-18 00:28:52,212 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:28:52,217 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:28:52,337 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:28:52,343 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:28:52,347 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:28:52,664 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59000] due to args.save_total_limit\n",
            "{'loss': 2.6515, 'learning_rate': 2.3474219572079973e-05, 'epoch': 10.61}\n",
            " 53% 60500/114040 [53:59<45:50, 19.46it/s][INFO|trainer.py:1928] 2021-08-18 00:29:19,518 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:29:19,523 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:29:19,648 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:29:19,653 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:29:19,657 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:29:19,952 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-59500] due to args.save_total_limit\n",
            "{'loss': 2.6405, 'learning_rate': 2.3254998246229396e-05, 'epoch': 10.7}\n",
            " 53% 61000/114040 [54:25<49:02, 18.03it/s][INFO|trainer.py:1928] 2021-08-18 00:29:45,715 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:29:45,723 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:29:45,858 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:29:45,863 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:29:45,867 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:29:46,164 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60000] due to args.save_total_limit\n",
            "{'loss': 2.5454, 'learning_rate': 2.3035776920378816e-05, 'epoch': 10.79}\n",
            " 54% 61500/114040 [54:52<50:13, 17.44it/s][INFO|trainer.py:1928] 2021-08-18 00:30:12,813 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:30:12,834 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:30:12,952 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:30:12,957 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:30:12,961 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:30:13,252 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-60500] due to args.save_total_limit\n",
            "{'loss': 2.5751, 'learning_rate': 2.2816555594528236e-05, 'epoch': 10.87}\n",
            " 54% 62000/114040 [55:19<40:04, 21.64it/s][INFO|trainer.py:1928] 2021-08-18 00:30:39,751 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:30:39,757 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:30:39,881 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:30:39,886 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:30:39,890 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:30:40,207 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61000] due to args.save_total_limit\n",
            "{'loss': 2.5011, 'learning_rate': 2.259733426867766e-05, 'epoch': 10.96}\n",
            " 55% 62500/114040 [55:46<43:27, 19.77it/s][INFO|trainer.py:1928] 2021-08-18 00:31:06,214 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:31:06,219 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:31:06,342 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:31:06,348 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:31:06,352 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:31:06,664 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61500] due to args.save_total_limit\n",
            "{'loss': 2.5318, 'learning_rate': 2.237811294282708e-05, 'epoch': 11.05}\n",
            " 55% 63000/114040 [56:12<46:43, 18.21it/s][INFO|trainer.py:1928] 2021-08-18 00:31:32,893 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:31:32,900 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:31:33,028 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:31:33,033 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:31:33,037 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:31:33,355 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62000] due to args.save_total_limit\n",
            "{'loss': 2.6033, 'learning_rate': 2.2158891616976503e-05, 'epoch': 11.14}\n",
            " 56% 63500/114040 [56:39<39:14, 21.47it/s][INFO|trainer.py:1928] 2021-08-18 00:32:00,009 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:32:00,014 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:32:00,124 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:32:00,129 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:32:00,133 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:32:00,432 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62500] due to args.save_total_limit\n",
            "{'loss': 2.5689, 'learning_rate': 2.1939670291125923e-05, 'epoch': 11.22}\n",
            " 56% 64000/114040 [57:06<39:18, 21.21it/s][INFO|trainer.py:1928] 2021-08-18 00:32:26,898 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:32:26,904 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:32:27,026 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:32:27,031 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:32:27,035 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:32:27,337 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000] due to args.save_total_limit\n",
            "{'loss': 2.6452, 'learning_rate': 2.1720448965275343e-05, 'epoch': 11.31}\n",
            " 57% 64500/114040 [57:34<46:37, 17.71it/s][INFO|trainer.py:1928] 2021-08-18 00:32:54,241 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:32:54,260 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:32:54,389 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:32:54,394 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:32:54,397 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:32:54,692 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500] due to args.save_total_limit\n",
            "{'loss': 2.3832, 'learning_rate': 2.1501227639424766e-05, 'epoch': 11.4}\n",
            " 57% 65000/114040 [57:59<38:50, 21.04it/s][INFO|trainer.py:1928] 2021-08-18 00:33:19,611 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:33:19,617 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:33:19,739 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:33:19,744 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:33:19,748 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:33:20,170 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000] due to args.save_total_limit\n",
            "{'loss': 2.505, 'learning_rate': 2.1282006313574186e-05, 'epoch': 11.49}\n",
            " 57% 65500/114040 [58:26<46:19, 17.47it/s][INFO|trainer.py:1928] 2021-08-18 00:33:46,656 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:33:46,661 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:33:46,785 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:33:46,790 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:33:46,794 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:33:47,115 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500] due to args.save_total_limit\n",
            "{'loss': 2.5144, 'learning_rate': 2.106278498772361e-05, 'epoch': 11.57}\n",
            " 58% 66000/114040 [58:53<39:14, 20.40it/s][INFO|trainer.py:1928] 2021-08-18 00:34:13,762 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:34:13,771 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:34:13,890 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:34:13,895 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:34:13,899 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:34:14,209 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000] due to args.save_total_limit\n",
            "{'loss': 2.5203, 'learning_rate': 2.084356366187303e-05, 'epoch': 11.66}\n",
            " 58% 66500/114040 [59:19<38:07, 20.78it/s][INFO|trainer.py:1928] 2021-08-18 00:34:40,008 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:34:40,014 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:34:40,141 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:34:40,146 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:34:40,150 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:34:40,447 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500] due to args.save_total_limit\n",
            "{'loss': 2.5375, 'learning_rate': 2.062434233602245e-05, 'epoch': 11.75}\n",
            " 59% 67000/114040 [59:47<35:53, 21.85it/s][INFO|trainer.py:1928] 2021-08-18 00:35:07,404 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:35:07,410 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:35:07,525 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:35:07,530 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:35:07,534 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:35:07,822 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000] due to args.save_total_limit\n",
            "{'loss': 2.524, 'learning_rate': 2.0405121010171873e-05, 'epoch': 11.84}\n",
            " 59% 67500/114040 [1:00:14<39:00, 19.89it/s][INFO|trainer.py:1928] 2021-08-18 00:35:34,888 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:35:35,173 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:35:35,291 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:35:35,297 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:35:35,301 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:35:35,598 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500] due to args.save_total_limit\n",
            "{'loss': 2.5238, 'learning_rate': 2.0185899684321293e-05, 'epoch': 11.93}\n",
            " 60% 68000/114040 [1:00:42<38:19, 20.02it/s][INFO|trainer.py:1928] 2021-08-18 00:36:02,622 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:36:02,628 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:36:02,761 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:36:02,766 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:36:02,788 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:36:03,096 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000] due to args.save_total_limit\n",
            "{'loss': 2.3924, 'learning_rate': 1.9966678358470713e-05, 'epoch': 12.01}\n",
            " 60% 68500/114040 [1:01:09<52:10, 14.55it/s][INFO|trainer.py:1928] 2021-08-18 00:36:29,163 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:36:29,168 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:36:29,300 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:36:29,306 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:36:29,311 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:36:29,606 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500] due to args.save_total_limit\n",
            "{'loss': 2.474, 'learning_rate': 1.9747457032620133e-05, 'epoch': 12.1}\n",
            " 61% 69000/114040 [1:01:36<37:18, 20.12it/s][INFO|trainer.py:1928] 2021-08-18 00:36:56,529 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:36:56,534 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:36:56,652 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:36:56,657 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:36:56,662 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:36:56,970 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000] due to args.save_total_limit\n",
            "{'loss': 2.5317, 'learning_rate': 1.9528235706769556e-05, 'epoch': 12.19}\n",
            " 61% 69500/114040 [1:02:03<31:06, 23.87it/s][INFO|trainer.py:1928] 2021-08-18 00:37:24,047 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:37:24,053 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:37:24,176 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:37:24,182 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:37:24,187 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:37:24,476 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500] due to args.save_total_limit\n",
            "{'loss': 2.4794, 'learning_rate': 1.9309014380918976e-05, 'epoch': 12.28}\n",
            " 61% 70000/114040 [1:02:30<33:19, 22.03it/s][INFO|trainer.py:1928] 2021-08-18 00:37:50,328 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:37:50,333 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:37:50,461 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:37:50,466 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:37:50,470 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:37:50,762 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000] due to args.save_total_limit\n",
            "{'loss': 2.4388, 'learning_rate': 1.9089793055068396e-05, 'epoch': 12.36}\n",
            " 62% 70500/114040 [1:02:56<32:39, 22.23it/s][INFO|trainer.py:1928] 2021-08-18 00:38:17,096 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:38:17,102 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:38:17,251 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:38:17,256 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:38:17,261 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:38:17,555 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500] due to args.save_total_limit\n",
            "{'loss': 2.4637, 'learning_rate': 1.887057172921782e-05, 'epoch': 12.45}\n",
            " 62% 71000/114040 [1:03:23<30:11, 23.76it/s][INFO|trainer.py:1928] 2021-08-18 00:38:43,250 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:38:43,257 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:38:43,382 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:38:43,387 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:38:43,391 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:38:43,713 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000] due to args.save_total_limit\n",
            "{'loss': 2.4059, 'learning_rate': 1.865135040336724e-05, 'epoch': 12.54}\n",
            " 63% 71500/114040 [1:03:50<47:23, 14.96it/s][INFO|trainer.py:1928] 2021-08-18 00:39:10,453 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:39:10,458 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:39:10,574 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:39:10,579 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:39:10,584 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:39:10,914 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500] due to args.save_total_limit\n",
            "{'loss': 2.4337, 'learning_rate': 1.843212907751666e-05, 'epoch': 12.63}\n",
            " 63% 72000/114040 [1:04:18<42:46, 16.38it/s][INFO|trainer.py:1928] 2021-08-18 00:39:38,298 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:39:38,304 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:39:38,425 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:39:38,432 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:39:38,437 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:39:38,763 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000] due to args.save_total_limit\n",
            "{'loss': 2.4389, 'learning_rate': 1.8212907751666083e-05, 'epoch': 12.71}\n",
            " 64% 72500/114040 [1:04:44<40:39, 17.03it/s][INFO|trainer.py:1928] 2021-08-18 00:40:05,003 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:40:05,008 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:40:05,125 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:40:05,131 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:40:05,136 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:40:05,426 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500] due to args.save_total_limit\n",
            "{'loss': 2.438, 'learning_rate': 1.7993686425815503e-05, 'epoch': 12.8}\n",
            " 64% 73000/114040 [1:05:11<40:48, 16.76it/s][INFO|trainer.py:1928] 2021-08-18 00:40:31,989 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:40:31,994 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:40:32,121 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:40:32,127 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:40:32,132 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:40:32,437 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000] due to args.save_total_limit\n",
            "{'loss': 2.3216, 'learning_rate': 1.7774465099964926e-05, 'epoch': 12.89}\n",
            " 64% 73500/114040 [1:05:36<30:12, 22.37it/s][INFO|trainer.py:1928] 2021-08-18 00:40:57,056 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:40:57,061 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:40:57,189 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:40:57,214 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:40:57,218 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:40:57,504 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500] due to args.save_total_limit\n",
            "{'loss': 2.3812, 'learning_rate': 1.7555243774114346e-05, 'epoch': 12.98}\n",
            " 65% 74000/114040 [1:06:03<34:05, 19.57it/s][INFO|trainer.py:1928] 2021-08-18 00:41:23,806 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:41:23,811 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:41:23,942 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:41:23,948 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:41:23,951 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:41:24,262 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000] due to args.save_total_limit\n",
            "{'loss': 2.3559, 'learning_rate': 1.7336022448263766e-05, 'epoch': 13.07}\n",
            " 65% 74500/114040 [1:06:30<35:17, 18.68it/s][INFO|trainer.py:1928] 2021-08-18 00:41:50,348 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:41:50,354 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:41:50,474 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:41:50,479 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:41:50,483 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:41:50,795 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500] due to args.save_total_limit\n",
            "{'loss': 2.359, 'learning_rate': 1.711680112241319e-05, 'epoch': 13.15}\n",
            " 66% 75000/114040 [1:06:57<44:33, 14.60it/s][INFO|trainer.py:1928] 2021-08-18 00:42:17,128 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:42:17,135 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:42:17,257 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:42:17,263 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:42:17,267 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:42:17,562 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000] due to args.save_total_limit\n",
            "{'loss': 2.3719, 'learning_rate': 1.689757979656261e-05, 'epoch': 13.24}\n",
            " 66% 75500/114040 [1:07:24<33:37, 19.10it/s][INFO|trainer.py:1928] 2021-08-18 00:42:44,689 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:42:44,694 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:42:44,824 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:42:44,829 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:42:44,833 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:42:45,127 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500] due to args.save_total_limit\n",
            "{'loss': 2.3289, 'learning_rate': 1.6678358470712033e-05, 'epoch': 13.33}\n",
            " 67% 76000/114040 [1:07:51<29:11, 21.72it/s][INFO|trainer.py:1928] 2021-08-18 00:43:11,198 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:43:11,205 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:43:11,330 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:43:11,334 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:43:11,338 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:43:11,623 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000] due to args.save_total_limit\n",
            "{'loss': 2.3864, 'learning_rate': 1.6459137144861453e-05, 'epoch': 13.42}\n",
            " 67% 76500/114040 [1:08:17<31:52, 19.63it/s][INFO|trainer.py:1928] 2021-08-18 00:43:37,782 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:43:37,789 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:43:37,913 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:43:37,917 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:43:37,942 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:43:38,224 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500] due to args.save_total_limit\n",
            "{'loss': 2.4554, 'learning_rate': 1.6239915819010873e-05, 'epoch': 13.5}\n",
            " 68% 77000/114040 [1:08:44<38:53, 15.87it/s][INFO|trainer.py:1928] 2021-08-18 00:44:04,564 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:44:04,569 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:44:04,686 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:44:04,691 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:44:04,695 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:44:04,996 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000] due to args.save_total_limit\n",
            "{'loss': 2.2911, 'learning_rate': 1.6020694493160296e-05, 'epoch': 13.59}\n",
            " 68% 77500/114040 [1:09:11<35:05, 17.35it/s][INFO|trainer.py:1928] 2021-08-18 00:44:31,148 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:44:31,154 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:44:31,290 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:44:31,296 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:44:31,300 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:44:31,613 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500] due to args.save_total_limit\n",
            "{'loss': 2.3892, 'learning_rate': 1.5801473167309716e-05, 'epoch': 13.68}\n",
            " 68% 78000/114040 [1:09:38<25:44, 23.34it/s][INFO|trainer.py:1928] 2021-08-18 00:44:58,658 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:44:58,664 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:44:58,776 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:44:58,781 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:44:58,785 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:44:59,074 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000] due to args.save_total_limit\n",
            "{'loss': 2.3832, 'learning_rate': 1.558225184145914e-05, 'epoch': 13.77}\n",
            " 69% 78500/114040 [1:10:05<29:29, 20.08it/s][INFO|trainer.py:1928] 2021-08-18 00:45:25,276 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:45:25,281 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:45:25,400 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:45:25,406 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:45:25,410 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:45:25,709 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500] due to args.save_total_limit\n",
            "{'loss': 2.2312, 'learning_rate': 1.536303051560856e-05, 'epoch': 13.85}\n",
            " 69% 79000/114040 [1:10:30<26:11, 22.30it/s][INFO|trainer.py:1928] 2021-08-18 00:45:50,678 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:45:50,683 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:45:50,811 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:45:50,816 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:45:50,820 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:45:51,105 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000] due to args.save_total_limit\n",
            "{'loss': 2.2182, 'learning_rate': 1.514380918975798e-05, 'epoch': 13.94}\n",
            " 70% 79500/114040 [1:10:55<28:51, 19.95it/s][INFO|trainer.py:1928] 2021-08-18 00:46:15,946 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:46:15,952 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:46:16,068 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:46:16,089 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:46:16,093 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:46:16,390 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500] due to args.save_total_limit\n",
            "{'loss': 2.4395, 'learning_rate': 1.4924587863907403e-05, 'epoch': 14.03}\n",
            " 70% 80000/114040 [1:11:22<29:16, 19.38it/s][INFO|trainer.py:1928] 2021-08-18 00:46:43,093 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:46:43,098 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:46:43,221 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:46:43,226 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:46:43,230 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:46:43,545 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000] due to args.save_total_limit\n",
            "{'loss': 2.24, 'learning_rate': 1.4705366538056823e-05, 'epoch': 14.12}\n",
            " 71% 80500/114040 [1:11:49<34:34, 16.17it/s][INFO|trainer.py:1928] 2021-08-18 00:47:10,007 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:47:10,011 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:47:10,122 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:47:10,126 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:47:10,130 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:47:10,439 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500] due to args.save_total_limit\n",
            "{'loss': 2.3858, 'learning_rate': 1.4486145212206246e-05, 'epoch': 14.21}\n",
            " 71% 81000/114040 [1:12:16<29:08, 18.89it/s][INFO|trainer.py:1928] 2021-08-18 00:47:36,774 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:47:36,780 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:47:36,897 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:47:36,902 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:47:36,906 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:47:37,204 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000] due to args.save_total_limit\n",
            "{'loss': 2.2044, 'learning_rate': 1.4266923886355666e-05, 'epoch': 14.29}\n",
            " 71% 81500/114040 [1:12:43<29:11, 18.58it/s][INFO|trainer.py:1928] 2021-08-18 00:48:03,183 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:48:03,188 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:48:03,324 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:48:03,329 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:48:03,333 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:48:03,635 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500] due to args.save_total_limit\n",
            "{'loss': 2.2559, 'learning_rate': 1.4047702560505086e-05, 'epoch': 14.38}\n",
            " 72% 82000/114040 [1:13:09<28:06, 19.00it/s][INFO|trainer.py:1928] 2021-08-18 00:48:29,743 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:48:29,749 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:48:29,871 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:48:29,876 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:48:29,880 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:48:30,162 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000] due to args.save_total_limit\n",
            "{'loss': 2.3157, 'learning_rate': 1.3828481234654508e-05, 'epoch': 14.47}\n",
            " 72% 82500/114040 [1:13:36<29:34, 17.78it/s][INFO|trainer.py:1928] 2021-08-18 00:48:56,670 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:48:56,675 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:48:56,796 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:48:56,817 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:48:56,824 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:48:57,120 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500] due to args.save_total_limit\n",
            "{'loss': 2.332, 'learning_rate': 1.360925990880393e-05, 'epoch': 14.56}\n",
            " 73% 83000/114040 [1:14:02<23:00, 22.49it/s][INFO|trainer.py:1928] 2021-08-18 00:49:22,785 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:49:22,790 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:49:22,913 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:49:22,919 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:49:22,926 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:49:23,237 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000] due to args.save_total_limit\n",
            "{'loss': 2.1802, 'learning_rate': 1.3390038582953351e-05, 'epoch': 14.64}\n",
            " 73% 83500/114040 [1:14:28<23:47, 21.39it/s][INFO|trainer.py:1928] 2021-08-18 00:49:48,484 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:49:48,489 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:49:48,618 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:49:48,623 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:49:48,628 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:49:48,937 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500] due to args.save_total_limit\n",
            "{'loss': 2.3811, 'learning_rate': 1.3170817257102771e-05, 'epoch': 14.73}\n",
            " 74% 84000/114040 [1:14:56<24:10, 20.70it/s][INFO|trainer.py:1928] 2021-08-18 00:50:16,471 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:50:16,477 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:50:16,612 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:50:16,618 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:50:16,622 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:50:16,924 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000] due to args.save_total_limit\n",
            "{'loss': 2.2887, 'learning_rate': 1.2951595931252191e-05, 'epoch': 14.82}\n",
            " 74% 84500/114040 [1:15:23<23:34, 20.89it/s][INFO|trainer.py:1928] 2021-08-18 00:50:43,233 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:50:43,238 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:50:43,363 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:50:43,368 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:50:43,372 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:50:43,670 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500] due to args.save_total_limit\n",
            "{'loss': 2.2067, 'learning_rate': 1.2732374605401614e-05, 'epoch': 14.91}\n",
            " 75% 85000/114040 [1:15:49<27:58, 17.30it/s][INFO|trainer.py:1928] 2021-08-18 00:51:09,403 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:51:09,410 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:51:09,541 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:51:09,546 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:51:09,550 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:51:09,841 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000] due to args.save_total_limit\n",
            "{'loss': 2.215, 'learning_rate': 1.2513153279551034e-05, 'epoch': 14.99}\n",
            " 75% 85500/114040 [1:16:15<22:52, 20.79it/s][INFO|trainer.py:1928] 2021-08-18 00:51:36,099 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:51:36,105 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:51:36,229 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:51:36,235 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:51:36,257 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:51:36,554 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500] due to args.save_total_limit\n",
            "{'loss': 2.2638, 'learning_rate': 1.2293931953700456e-05, 'epoch': 15.08}\n",
            " 75% 86000/114040 [1:16:41<27:37, 16.92it/s][INFO|trainer.py:1928] 2021-08-18 00:52:01,880 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:52:01,886 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:52:02,012 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:52:02,018 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:52:02,022 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:52:02,348 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000] due to args.save_total_limit\n",
            "{'loss': 2.2853, 'learning_rate': 1.2074710627849878e-05, 'epoch': 15.17}\n",
            " 76% 86500/114040 [1:17:07<22:10, 20.69it/s][INFO|trainer.py:1928] 2021-08-18 00:52:28,097 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:52:28,105 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:52:28,227 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:52:28,234 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:52:28,238 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:52:28,537 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500] due to args.save_total_limit\n",
            "{'loss': 2.1489, 'learning_rate': 1.18554893019993e-05, 'epoch': 15.26}\n",
            " 76% 87000/114040 [1:17:34<22:24, 20.11it/s][INFO|trainer.py:1928] 2021-08-18 00:52:54,579 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:52:54,585 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:52:54,706 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:52:54,711 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:52:54,715 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:52:55,025 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000] due to args.save_total_limit\n",
            "{'loss': 2.2161, 'learning_rate': 1.1636267976148721e-05, 'epoch': 15.35}\n",
            " 77% 87500/114040 [1:18:01<30:22, 14.56it/s][INFO|trainer.py:1928] 2021-08-18 00:53:21,886 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:53:21,891 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:53:22,007 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:53:22,012 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:53:22,016 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:53:22,318 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500] due to args.save_total_limit\n",
            "{'loss': 2.2669, 'learning_rate': 1.1417046650298141e-05, 'epoch': 15.43}\n",
            " 77% 88000/114040 [1:18:29<28:36, 15.17it/s][INFO|trainer.py:1928] 2021-08-18 00:53:49,225 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:53:49,244 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:53:49,372 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:53:49,379 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:53:49,383 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:53:49,671 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000] due to args.save_total_limit\n",
            "{'loss': 2.2806, 'learning_rate': 1.1197825324447563e-05, 'epoch': 15.52}\n",
            " 78% 88500/114040 [1:18:55<31:14, 13.63it/s][INFO|trainer.py:1928] 2021-08-18 00:54:16,058 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:54:16,063 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:54:16,184 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:54:16,189 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:54:16,213 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:54:16,512 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500] due to args.save_total_limit\n",
            "{'loss': 2.1931, 'learning_rate': 1.0978603998596984e-05, 'epoch': 15.61}\n",
            " 78% 89000/114040 [1:19:22<16:52, 24.74it/s][INFO|trainer.py:1928] 2021-08-18 00:54:42,140 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:54:42,147 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:54:42,268 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:54:42,272 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:54:42,276 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:54:42,595 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000] due to args.save_total_limit\n",
            "{'loss': 2.3034, 'learning_rate': 1.0759382672746406e-05, 'epoch': 15.7}\n",
            " 78% 89500/114040 [1:19:49<23:44, 17.23it/s][INFO|trainer.py:1928] 2021-08-18 00:55:09,296 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:55:09,301 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:55:09,419 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:55:09,424 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:55:09,428 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:55:09,745 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500] due to args.save_total_limit\n",
            "{'loss': 2.2291, 'learning_rate': 1.0540161346895828e-05, 'epoch': 15.78}\n",
            " 79% 90000/114040 [1:20:16<20:19, 19.72it/s][INFO|trainer.py:1928] 2021-08-18 00:55:36,354 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:55:36,360 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:55:36,479 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:55:36,484 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:55:36,488 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:55:36,780 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000] due to args.save_total_limit\n",
            "{'loss': 2.1957, 'learning_rate': 1.0320940021045248e-05, 'epoch': 15.87}\n",
            " 79% 90500/114040 [1:20:43<19:07, 20.51it/s][INFO|trainer.py:1928] 2021-08-18 00:56:03,353 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:56:03,359 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:56:03,485 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:56:03,490 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:56:03,494 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:56:03,793 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500] due to args.save_total_limit\n",
            "{'loss': 2.2207, 'learning_rate': 1.010171869519467e-05, 'epoch': 15.96}\n",
            " 80% 91000/114040 [1:21:10<20:40, 18.57it/s][INFO|trainer.py:1928] 2021-08-18 00:56:30,485 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:56:30,494 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:56:30,620 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:56:30,625 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:56:30,629 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:56:30,962 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000] due to args.save_total_limit\n",
            "{'loss': 2.188, 'learning_rate': 9.882497369344091e-06, 'epoch': 16.05}\n",
            " 80% 91500/114040 [1:21:37<20:03, 18.73it/s][INFO|trainer.py:1928] 2021-08-18 00:56:57,645 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:56:57,650 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:56:57,771 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:56:57,776 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:56:57,800 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:56:58,135 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500] due to args.save_total_limit\n",
            "{'loss': 2.2141, 'learning_rate': 9.663276043493513e-06, 'epoch': 16.13}\n",
            " 81% 92000/114040 [1:22:05<21:44, 16.90it/s][INFO|trainer.py:1928] 2021-08-18 00:57:25,147 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:57:25,156 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:57:25,272 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:57:25,277 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:57:25,281 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:57:25,642 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000] due to args.save_total_limit\n",
            "{'loss': 2.0968, 'learning_rate': 9.444054717642933e-06, 'epoch': 16.22}\n",
            " 81% 92500/114040 [1:22:30<17:09, 20.92it/s][INFO|trainer.py:1928] 2021-08-18 00:57:50,703 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:57:50,708 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:57:50,825 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:57:50,831 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:57:50,835 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:57:51,692 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500] due to args.save_total_limit\n",
            "{'loss': 2.2656, 'learning_rate': 9.224833391792354e-06, 'epoch': 16.31}\n",
            " 82% 93000/114040 [1:22:58<18:13, 19.23it/s][INFO|trainer.py:1928] 2021-08-18 00:58:18,771 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:58:18,776 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:58:18,899 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:58:18,909 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:58:18,913 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:58:19,271 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000] due to args.save_total_limit\n",
            "{'loss': 2.1736, 'learning_rate': 9.005612065941774e-06, 'epoch': 16.4}\n",
            " 82% 93500/114040 [1:23:24<14:16, 23.98it/s][INFO|trainer.py:1928] 2021-08-18 00:58:45,069 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:58:45,074 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:58:45,205 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:58:45,211 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:58:45,215 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:58:45,534 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500] due to args.save_total_limit\n",
            "{'loss': 2.0889, 'learning_rate': 8.786390740091196e-06, 'epoch': 16.49}\n",
            " 82% 94000/114040 [1:23:50<17:12, 19.42it/s][INFO|trainer.py:1928] 2021-08-18 00:59:10,842 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:59:10,863 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:59:10,994 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:59:11,000 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:59:11,005 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:59:11,329 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000] due to args.save_total_limit\n",
            "{'loss': 2.141, 'learning_rate': 8.567169414240618e-06, 'epoch': 16.57}\n",
            " 83% 94500/114040 [1:24:17<18:23, 17.71it/s][INFO|trainer.py:1928] 2021-08-18 00:59:37,244 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 00:59:37,250 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 00:59:37,385 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 00:59:37,391 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 00:59:37,415 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 00:59:37,732 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500] due to args.save_total_limit\n",
            "{'loss': 2.1071, 'learning_rate': 8.34794808839004e-06, 'epoch': 16.66}\n",
            " 83% 95000/114040 [1:24:43<14:07, 22.47it/s][INFO|trainer.py:1928] 2021-08-18 01:00:03,309 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:00:03,315 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:00:03,443 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:00:03,449 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:00:03,453 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:00:03,783 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000] due to args.save_total_limit\n",
            "{'loss': 2.2134, 'learning_rate': 8.12872676253946e-06, 'epoch': 16.75}\n",
            " 84% 95500/114040 [1:25:11<16:00, 19.30it/s][INFO|trainer.py:1928] 2021-08-18 01:00:31,203 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:00:31,210 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:00:31,334 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:00:31,339 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:00:31,344 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:00:31,715 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500] due to args.save_total_limit\n",
            "{'loss': 2.2006, 'learning_rate': 7.909505436688881e-06, 'epoch': 16.84}\n",
            " 84% 96000/114040 [1:25:38<24:07, 12.46it/s][INFO|trainer.py:1928] 2021-08-18 01:00:58,409 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:00:58,415 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:00:58,537 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:00:58,543 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:00:58,548 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:00:58,844 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000] due to args.save_total_limit\n",
            "{'loss': 2.162, 'learning_rate': 7.690284110838303e-06, 'epoch': 16.92}\n",
            " 85% 96500/114040 [1:26:05<14:19, 20.41it/s][INFO|trainer.py:1928] 2021-08-18 01:01:25,668 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:01:25,673 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:01:25,793 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:01:25,798 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:01:25,802 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:01:26,107 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500] due to args.save_total_limit\n",
            "{'loss': 2.175, 'learning_rate': 7.471062784987724e-06, 'epoch': 17.01}\n",
            " 85% 97000/114040 [1:26:31<13:54, 20.41it/s][INFO|trainer.py:1928] 2021-08-18 01:01:51,753 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:01:51,772 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:01:51,904 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:01:51,909 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:01:51,913 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:01:52,206 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000] due to args.save_total_limit\n",
            "{'loss': 2.2208, 'learning_rate': 7.251841459137146e-06, 'epoch': 17.1}\n",
            " 85% 97500/114040 [1:26:58<14:36, 18.88it/s][INFO|trainer.py:1928] 2021-08-18 01:02:18,996 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:02:19,002 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:02:19,124 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:02:19,129 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:02:19,133 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:02:19,450 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500] due to args.save_total_limit\n",
            "{'loss': 2.1478, 'learning_rate': 7.032620133286566e-06, 'epoch': 17.19}\n",
            " 86% 98000/114040 [1:27:25<12:31, 21.34it/s][INFO|trainer.py:1928] 2021-08-18 01:02:45,853 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:02:45,860 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:02:45,983 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:02:45,987 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:02:45,991 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:02:46,314 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000] due to args.save_total_limit\n",
            "{'loss': 2.1559, 'learning_rate': 6.813398807435988e-06, 'epoch': 17.27}\n",
            " 86% 98500/114040 [1:27:53<15:37, 16.58it/s][INFO|trainer.py:1928] 2021-08-18 01:03:13,338 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:03:13,342 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:03:13,477 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:03:13,482 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:03:13,486 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:03:13,810 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500] due to args.save_total_limit\n",
            "{'loss': 2.142, 'learning_rate': 6.594177481585409e-06, 'epoch': 17.36}\n",
            " 87% 99000/114040 [1:28:20<11:45, 21.31it/s][INFO|trainer.py:1928] 2021-08-18 01:03:40,848 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:03:40,854 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:03:40,977 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:03:40,986 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:03:40,990 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:03:41,306 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000] due to args.save_total_limit\n",
            "{'loss': 2.1399, 'learning_rate': 6.374956155734831e-06, 'epoch': 17.45}\n",
            " 87% 99500/114040 [1:28:47<11:52, 20.40it/s][INFO|trainer.py:1928] 2021-08-18 01:04:07,755 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:04:07,762 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:04:07,896 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:04:07,900 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:04:07,905 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:04:08,223 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500] due to args.save_total_limit\n",
            "{'loss': 2.1175, 'learning_rate': 6.155734829884252e-06, 'epoch': 17.54}\n",
            " 88% 100000/114040 [1:29:13<12:04, 19.39it/s][INFO|trainer.py:1928] 2021-08-18 01:04:34,058 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:04:34,064 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:04:34,206 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:04:34,211 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:04:34,214 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:04:34,513 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000] due to args.save_total_limit\n",
            "{'loss': 2.1644, 'learning_rate': 5.936513504033673e-06, 'epoch': 17.63}\n",
            " 88% 100500/114040 [1:29:40<09:28, 23.83it/s][INFO|trainer.py:1928] 2021-08-18 01:05:00,665 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:05:00,670 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:05:00,798 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:05:00,804 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:05:00,808 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:05:01,122 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500] due to args.save_total_limit\n",
            "{'loss': 2.1461, 'learning_rate': 5.7172921781830935e-06, 'epoch': 17.71}\n",
            " 89% 101000/114040 [1:30:06<09:16, 23.45it/s][INFO|trainer.py:1928] 2021-08-18 01:05:27,040 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:05:27,046 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:05:27,179 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:05:27,184 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:05:27,188 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:05:27,508 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000] due to args.save_total_limit\n",
            "{'loss': 2.0929, 'learning_rate': 5.498070852332515e-06, 'epoch': 17.8}\n",
            " 89% 101500/114040 [1:30:32<12:21, 16.90it/s][INFO|trainer.py:1928] 2021-08-18 01:05:52,793 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:05:52,798 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:05:52,924 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:05:52,929 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:05:52,933 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:05:53,234 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500] due to args.save_total_limit\n",
            "{'loss': 2.0098, 'learning_rate': 5.278849526481936e-06, 'epoch': 17.89}\n",
            " 89% 102000/114040 [1:30:58<09:25, 21.28it/s][INFO|trainer.py:1928] 2021-08-18 01:06:18,276 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:06:18,281 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:06:18,409 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:06:18,416 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:06:18,419 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:06:18,713 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000] due to args.save_total_limit\n",
            "{'loss': 2.1413, 'learning_rate': 5.059628200631358e-06, 'epoch': 17.98}\n",
            " 90% 102500/114040 [1:31:25<08:55, 21.56it/s][INFO|trainer.py:1928] 2021-08-18 01:06:45,163 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:06:45,169 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:06:45,294 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:06:45,299 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:06:45,304 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:06:45,609 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500] due to args.save_total_limit\n",
            "{'loss': 2.1051, 'learning_rate': 4.840406874780779e-06, 'epoch': 18.06}\n",
            " 90% 103000/114040 [1:31:51<08:02, 22.88it/s][INFO|trainer.py:1928] 2021-08-18 01:07:11,437 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:07:11,442 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:07:11,580 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:07:11,586 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:07:11,590 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:07:11,889 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000] due to args.save_total_limit\n",
            "{'loss': 2.1127, 'learning_rate': 4.6211855489302e-06, 'epoch': 18.15}\n",
            " 91% 103500/114040 [1:32:18<09:21, 18.78it/s][INFO|trainer.py:1928] 2021-08-18 01:07:38,369 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:07:38,376 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:07:38,514 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:07:38,520 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:07:38,524 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:07:38,836 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500] due to args.save_total_limit\n",
            "{'loss': 2.0737, 'learning_rate': 4.401964223079622e-06, 'epoch': 18.24}\n",
            " 91% 104000/114040 [1:32:44<08:59, 18.62it/s][INFO|trainer.py:1928] 2021-08-18 01:08:04,189 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:08:04,194 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:08:04,312 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:08:04,317 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:08:04,321 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:08:04,637 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000] due to args.save_total_limit\n",
            "{'loss': 2.1486, 'learning_rate': 4.182742897229043e-06, 'epoch': 18.33}\n",
            " 92% 104500/114040 [1:33:11<07:02, 22.60it/s][INFO|trainer.py:1928] 2021-08-18 01:08:31,805 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:08:31,810 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:08:31,935 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:08:31,940 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:08:31,944 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:08:32,261 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500] due to args.save_total_limit\n",
            "{'loss': 2.0774, 'learning_rate': 3.9635215713784635e-06, 'epoch': 18.41}\n",
            " 92% 105000/114040 [1:33:38<09:13, 16.32it/s][INFO|trainer.py:1928] 2021-08-18 01:08:58,150 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:08:58,155 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:08:58,273 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:08:58,285 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:08:58,289 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:08:58,594 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000] due to args.save_total_limit\n",
            "{'loss': 2.093, 'learning_rate': 3.7443002455278856e-06, 'epoch': 18.5}\n",
            " 93% 105500/114040 [1:34:04<07:01, 20.27it/s][INFO|trainer.py:1928] 2021-08-18 01:09:24,213 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:09:24,218 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:09:24,348 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:09:24,354 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:09:24,359 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:09:24,642 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500] due to args.save_total_limit\n",
            "{'loss': 2.2798, 'learning_rate': 3.5250789196773064e-06, 'epoch': 18.59}\n",
            " 93% 106000/114040 [1:34:31<06:17, 21.27it/s][INFO|trainer.py:1928] 2021-08-18 01:09:51,707 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:09:51,725 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:09:51,856 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:09:51,861 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:09:51,864 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:09:52,156 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000] due to args.save_total_limit\n",
            "{'loss': 2.1176, 'learning_rate': 3.3058575938267276e-06, 'epoch': 18.68}\n",
            " 93% 106500/114040 [1:34:58<05:47, 21.69it/s][INFO|trainer.py:1928] 2021-08-18 01:10:18,288 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:10:18,293 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:10:18,417 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:10:18,421 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:10:18,425 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:10:18,751 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500] due to args.save_total_limit\n",
            "{'loss': 2.1498, 'learning_rate': 3.086636267976149e-06, 'epoch': 18.77}\n",
            " 94% 107000/114040 [1:35:25<05:24, 21.71it/s][INFO|trainer.py:1928] 2021-08-18 01:10:45,110 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:10:45,116 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:10:45,241 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:10:45,247 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:10:45,258 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:10:45,578 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000] due to args.save_total_limit\n",
            "{'loss': 2.1056, 'learning_rate': 2.86741494212557e-06, 'epoch': 18.85}\n",
            " 94% 107500/114040 [1:35:51<06:21, 17.13it/s][INFO|trainer.py:1928] 2021-08-18 01:11:11,212 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:11:11,217 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:11:11,340 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:11:11,345 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:11:11,348 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:11:11,653 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500] due to args.save_total_limit\n",
            "{'loss': 2.1496, 'learning_rate': 2.6481936162749914e-06, 'epoch': 18.94}\n",
            " 95% 108000/114040 [1:36:18<05:31, 18.25it/s][INFO|trainer.py:1928] 2021-08-18 01:11:38,736 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:11:38,742 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:11:38,882 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:11:38,887 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:11:38,891 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:11:39,177 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000] due to args.save_total_limit\n",
            "{'loss': 2.0521, 'learning_rate': 2.4289722904244127e-06, 'epoch': 19.03}\n",
            " 95% 108500/114040 [1:36:44<03:57, 23.34it/s][INFO|trainer.py:1928] 2021-08-18 01:12:04,866 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:12:04,872 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:12:04,993 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:12:04,998 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:12:05,002 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:12:05,297 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500] due to args.save_total_limit\n",
            "{'loss': 2.1007, 'learning_rate': 2.209750964573834e-06, 'epoch': 19.12}\n",
            " 96% 109000/114040 [1:37:11<04:25, 18.97it/s][INFO|trainer.py:1928] 2021-08-18 01:12:31,287 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:12:31,292 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:12:31,408 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:12:31,413 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:12:31,443 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:12:31,748 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000] due to args.save_total_limit\n",
            "{'loss': 2.0062, 'learning_rate': 1.990529638723255e-06, 'epoch': 19.2}\n",
            " 96% 109500/114040 [1:37:37<03:12, 23.57it/s][INFO|trainer.py:1928] 2021-08-18 01:12:57,297 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:12:57,303 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:12:57,425 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:12:57,430 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:12:57,434 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:12:57,763 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500] due to args.save_total_limit\n",
            "{'loss': 2.1921, 'learning_rate': 1.7713083128726764e-06, 'epoch': 19.29}\n",
            " 96% 110000/114040 [1:38:03<03:50, 17.51it/s][INFO|trainer.py:1928] 2021-08-18 01:13:23,821 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:13:23,826 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:13:23,945 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:13:23,951 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:13:23,964 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:13:24,285 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000] due to args.save_total_limit\n",
            "{'loss': 2.148, 'learning_rate': 1.5520869870220974e-06, 'epoch': 19.38}\n",
            " 97% 110500/114040 [1:38:29<03:03, 19.25it/s][INFO|trainer.py:1928] 2021-08-18 01:13:49,922 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:13:49,927 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:13:50,060 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:13:50,066 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:13:50,070 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:13:50,375 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500] due to args.save_total_limit\n",
            "{'loss': 2.1103, 'learning_rate': 1.332865661171519e-06, 'epoch': 19.47}\n",
            " 97% 111000/114040 [1:38:56<02:37, 19.24it/s][INFO|trainer.py:1928] 2021-08-18 01:14:16,990 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:14:16,996 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:14:17,122 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:14:17,129 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:14:17,133 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:14:17,441 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000] due to args.save_total_limit\n",
            "{'loss': 2.0715, 'learning_rate': 1.1136443353209402e-06, 'epoch': 19.55}\n",
            " 98% 111500/114040 [1:39:23<02:17, 18.52it/s][INFO|trainer.py:1928] 2021-08-18 01:14:43,930 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:14:43,935 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:14:44,061 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:14:44,067 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:14:44,071 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:14:44,378 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500] due to args.save_total_limit\n",
            "{'loss': 2.046, 'learning_rate': 8.944230094703613e-07, 'epoch': 19.64}\n",
            " 98% 112000/114040 [1:39:50<01:44, 19.45it/s][INFO|trainer.py:1928] 2021-08-18 01:15:10,809 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:15:10,814 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:15:10,949 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:15:10,968 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:15:10,974 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:15:11,279 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000] due to args.save_total_limit\n",
            "{'loss': 2.1322, 'learning_rate': 6.752016836197826e-07, 'epoch': 19.73}\n",
            " 99% 112500/114040 [1:40:18<01:22, 18.75it/s][INFO|trainer.py:1928] 2021-08-18 01:15:38,682 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:15:38,687 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:15:38,804 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:15:38,810 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:15:38,814 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:15:39,138 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500] due to args.save_total_limit\n",
            "{'loss': 2.0675, 'learning_rate': 4.559803577692038e-07, 'epoch': 19.82}\n",
            " 99% 113000/114040 [1:40:45<00:49, 20.88it/s][INFO|trainer.py:1928] 2021-08-18 01:16:05,674 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:16:05,681 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:16:05,808 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:16:05,813 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:16:05,817 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:16:06,135 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000] due to args.save_total_limit\n",
            "{'loss': 2.121, 'learning_rate': 2.3675903191862506e-07, 'epoch': 19.91}\n",
            "100% 113500/114040 [1:41:12<00:34, 15.72it/s][INFO|trainer.py:1928] 2021-08-18 01:16:32,327 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:16:32,333 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:16:32,457 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:16:32,462 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:16:32,467 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:16:32,774 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500] due to args.save_total_limit\n",
            "{'loss': 2.072, 'learning_rate': 1.75377060680463e-08, 'epoch': 19.99}\n",
            "100% 114000/114040 [1:41:38<00:01, 21.28it/s][INFO|trainer.py:1928] 2021-08-18 01:16:58,603 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:16:58,609 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:16:58,732 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:16:58,739 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:16:58,743 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/special_tokens_map.json\n",
            "[INFO|trainer.py:2004] 2021-08-18 01:16:59,039 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000] due to args.save_total_limit\n",
            "100% 114038/114040 [1:41:40<00:00, 20.19it/s][INFO|trainer.py:1369] 2021-08-18 01:17:01,121 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6101.3216, 'train_samples_per_second': 149.509, 'train_steps_per_second': 18.691, 'train_loss': 3.063443809356743, 'epoch': 20.0}\n",
            "100% 114040/114040 [1:41:41<00:00, 18.69it/s]\n",
            "[INFO|trainer.py:1928] 2021-08-18 01:17:01,141 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model\n",
            "[INFO|configuration_utils.py:379] 2021-08-18 01:17:01,625 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-08-18 01:17:03,057 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2003] 2021-08-18 01:17:03,221 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2009] 2021-08-18 01:17:03,373 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       20.0\n",
            "  train_loss               =     3.0634\n",
            "  train_runtime            = 1:41:41.32\n",
            "  train_samples            =      45610\n",
            "  train_samples_per_second =    149.509\n",
            "  train_steps_per_second   =     18.691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QGY7F6YUQPe"
      },
      "source": [
        "# language model 만들었으면... 여기서 부터~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj-fQYF-L4gB"
      },
      "source": [
        "#model_name = 'albert-base-v2'\n",
        "#model_name = 'albert-xlarge-v2'\n",
        "#model_name = 'albert-xlarge-v2'\n",
        "#model_name = 'albert-xxlarge-v2'\n",
        "model_name = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhnMlOQ-9YiH",
        "outputId": "40f987f8-adb2-4d8e-ed66-d7335cdf9e30"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "op = tokenizer(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[   5, 2656,  160,  334,  146,  953,  121, 5844, 6340, 4549,  258, 6473,\n",
            "          227,  545,  781, 2677,  702,  160, 3732, 1368, 8997,  124,  156,  491,\n",
            "         1549,  123, 3097,  174,  124, 9387,  953,  121, 5844, 6340, 4549,  258,\n",
            "         6473,  227,  545,  781, 2677,  702,  124, 9387,  248,  160, 3732, 1368,\n",
            "         8997,  124,  156,  702,  327,  800,  549,  234, 2656,  134,  136,  328,\n",
            "          182, 2699, 1635,  123,  493,  305,  217,  123,    6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'a', 'f', '▁in', 'tra', 'net', '.', '▁mil', '▁use', '▁only', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 2656, 160, 334, 146, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 160, 3732, 1368, 8997, 124, 156, 491, 1549, 123, 3097, 174, 124, 9387, 953, 121, 5844, 6340, 4549, 258, 6473, 227, 545, 781, 2677, 702, 124, 9387, 248, 160, 3732, 1368, 8997, 124, 156, 702, 327, 800, 549, 234, 2656, 134, 136, 328, 182, 2699, 1635, 123, 493, 305, 217, 123, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4sl25hxXQPx"
      },
      "source": [
        "# Albert NOTAM Classification Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqdLO2agXWXN"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "\n",
        "class Notam_classification_model:\n",
        "    def __init__(self,pre_trained_model_name='albert-base-v2'):\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained(pre_trained_model_name)\n",
        "        self.model  = AlbertForSequenceClassification.from_pretrained(\n",
        "                                    pre_trained_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 6, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )\n",
        "        # If there's a GPU available...\n",
        "        if torch.cuda.is_available():    \n",
        "\n",
        "            # Tell PyTorch to use the GPU.    \n",
        "            self.device = torch.device(\"cuda\")\n",
        "\n",
        "            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "            print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "        # If not...\n",
        "        else:\n",
        "            print('No GPU available, using the CPU instead.')\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "    # Function to calculate the accuracy of our predictions vs labels\n",
        "    def __flat_accuracy(self,preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "    def __format_time(self,elapsed):\n",
        "        '''\n",
        "        Takes a time in seconds and returns a string hh:mm:ss\n",
        "        '''\n",
        "        # Round to the nearest second.\n",
        "        elapsed_rounded = int(round((elapsed)))\n",
        "        \n",
        "        # Format as hh:mm:ss\n",
        "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "    def save_notam_model(self, output_dir = '/content/drive/MyDrive/NOTAM/notam_model'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    def load_notam_model(self,input_dir = '/content/drive/MyDrive/NOTAM/notam_model/For_E_Section'):\n",
        "        print('Loading Albert notam model...')\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained(input_dir)\n",
        "        self.model = AlbertForSequenceClassification.from_pretrained(input_dir)\n",
        "        #self.tokenizer.to(self.device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self,epochs=1,train_dataloader=None,validation_dataloader=None,output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section'):\n",
        "\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.model.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.model.named_parameters())\n",
        "\n",
        "        print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        "        \n",
        "        print(self.model)\n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        optimizer = AdamW(self.model.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.model.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = self.__format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(self.device)\n",
        "                b_input_mask = batch[1].to(self.device)\n",
        "                b_labels = batch[2].to(self.device)\n",
        "\n",
        "                #print(b_input_ids)\n",
        "                #print(b_input_mask)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.model.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                '''\n",
        "                loss, logits = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                '''\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                loss = outputs[0]\n",
        "                logits = outputs[1]\n",
        "                \n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = self.__format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "            avg_val_loss, avg_val_accuracy, validation_time = self.validate(validation_dataloader)\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "        self.save_notam_model(output_dir)\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(self.__format_time(time.time()-total_t0)))\n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def validate(self,validation_dataloader=None):          \n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tracking variables \n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            \n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "            # the `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(self.device)\n",
        "            b_input_mask = batch[1].to(self.device)\n",
        "            b_labels = batch[2].to(self.device)\n",
        "            for l in batch[2]:\n",
        "                true_labels.append(l.item())        \n",
        "            # Tell pytorch not to bother with constructing the compute graph during\n",
        "            # the forward pass, since this is only needed for backprop (training).\n",
        "            with torch.no_grad():        \n",
        "\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                # values prior to applying an activation function like the softmax.\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]            \n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            for li in logits:\n",
        "                pred_labels.append(np.argmax(li))\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += self.__flat_accuracy(logits, label_ids)\n",
        "            \n",
        "        print(\"  classification_report   \")    \n",
        "        print(classification_report(true_labels,pred_labels))\n",
        "        # Report the final accuracy for this validation run.\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "        \n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = self.__format_time(time.time() - t0)\n",
        "        \n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        return avg_val_loss, avg_val_accuracy, validation_time\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5XcNIyCaq7V",
        "outputId": "dda4ef69-0efb-483c-96a7-46ecb4bcae3f"
      },
      "source": [
        "ancm = Notam_classification_model(pre_trained_model_name=model_name)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aoYodxadTQi"
      },
      "source": [
        "# E Section\n",
        "\n",
        "albert-notam-base : ACC:71%\n",
        "\n",
        "albert-base-v2 : 74%\n",
        "\n",
        "albert-large-v2 : ??%\n",
        " * CUDA out of memory. 그래서 배치 사이즈 줄임 32 -> 16\n",
        " \n",
        "albert-xlarge-v2 : ??%\n",
        " * CUDA out of memory. 그래서 배치 사이즈 줄임 32 -> 16\n",
        "\n",
        "albert-xxlarge-v2 : ??%\n",
        " * CUDA out of memory. 그래서 배치 사이즈 줄임 32 -> 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJU8jabjd1tv"
      },
      "source": [
        "## Trainset, Validationset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVdhAqqrdZzx",
        "outputId": "db8f9c6c-44d8-439a-f4c4-147d43d39e90"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "# For every sentence...\n",
        "for row in df.iterrows():\n",
        "    # E section\n",
        "    sent = clean_text(str(row[1][11]).lower()) \n",
        "    # label for E Section\n",
        "    lb = row[1][3]+0\n",
        "    if lb == 6:\n",
        "        lb = 5\n",
        "    labels.append(lb)\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation = True,\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in input_ids[0].tolist()]))\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Training & Validation Split\n",
        "# Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )   "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2187: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'a', 'f', '▁in', 'tra', 'net', '.', '▁mil', '▁use', '▁only', '.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    5,  2656,   160,   334,   146,   953,   121,  5844,  6340,  4549,\n",
            "          258,  6473,   227,   545,   781,  2677,   702,   160,  3732,  1368,\n",
            "         8997,   124,   156,   491,  1549,   123,  3097,   174,   124,  9387,\n",
            "          953,   121,  5844,  6340,  4549,   258,  6473,   227,   545,   781,\n",
            "         2677,   702,   124,  9387,   248,   160,  3732,  1368,  8997,   124,\n",
            "          156,   702,   327,   800,   549,   234,  2656,   134,   136,   328,\n",
            "          182,  2699,  1635,   123,   493,   305,   217,   123,     6, 12000,\n",
            "        12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000,\n",
            "        12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000,\n",
            "        12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000,\n",
            "        12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000,\n",
            "        12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000,\n",
            "        12000, 12000, 12000, 12000, 12000, 12000, 12000, 12000])\n",
            "9,373 training samples\n",
            "1,042 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ1LFrdDa9xp",
        "outputId": "b854a4e0-b37c-4c00-bb08-756ed14bcbbe"
      },
      "source": [
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(12101, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.94\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        31\n",
            "           1       0.61      0.66      0.64       179\n",
            "           2       0.45      0.43      0.44        84\n",
            "           3       0.65      0.28      0.39       116\n",
            "           4       0.88      0.82      0.85       156\n",
            "           5       0.75      0.91      0.82       476\n",
            "\n",
            "    accuracy                           0.72      1042\n",
            "   macro avg       0.56      0.52      0.52      1042\n",
            "weighted avg       0.69      0.72      0.69      1042\n",
            "\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 0.77\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.74\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        31\n",
            "           1       0.58      0.64      0.61       179\n",
            "           2       0.36      0.57      0.44        84\n",
            "           3       0.67      0.30      0.42       116\n",
            "           4       0.90      0.81      0.86       156\n",
            "           5       0.77      0.84      0.80       476\n",
            "\n",
            "    accuracy                           0.69      1042\n",
            "   macro avg       0.55      0.53      0.52      1042\n",
            "weighted avg       0.69      0.69      0.68      1042\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.75\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        31\n",
            "           1       0.56      0.70      0.62       179\n",
            "           2       0.50      0.62      0.56        84\n",
            "           3       0.56      0.39      0.46       116\n",
            "           4       0.90      0.85      0.88       156\n",
            "           5       0.80      0.82      0.81       476\n",
            "\n",
            "    accuracy                           0.71      1042\n",
            "   macro avg       0.55      0.56      0.55      1042\n",
            "weighted avg       0.70      0.71      0.70      1042\n",
            "\n",
            "  Accuracy: 0.71\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.03      0.06        31\n",
            "           1       0.59      0.73      0.65       179\n",
            "           2       0.49      0.63      0.55        84\n",
            "           3       0.57      0.34      0.42       116\n",
            "           4       0.88      0.86      0.87       156\n",
            "           5       0.80      0.82      0.81       476\n",
            "\n",
            "    accuracy                           0.72      1042\n",
            "   macro avg       0.61      0.57      0.56      1042\n",
            "weighted avg       0.71      0.72      0.70      1042\n",
            "\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.61\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.06      0.11        31\n",
            "           1       0.60      0.69      0.64       179\n",
            "           2       0.51      0.43      0.46        84\n",
            "           3       0.52      0.38      0.44       116\n",
            "           4       0.92      0.83      0.87       156\n",
            "           5       0.76      0.85      0.81       476\n",
            "\n",
            "    accuracy                           0.71      1042\n",
            "   macro avg       0.60      0.54      0.56      1042\n",
            "weighted avg       0.70      0.71      0.70      1042\n",
            "\n",
            "  Accuracy: 0.71\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.06      0.10        31\n",
            "           1       0.56      0.70      0.62       179\n",
            "           2       0.46      0.44      0.45        84\n",
            "           3       0.53      0.34      0.41       116\n",
            "           4       0.89      0.85      0.87       156\n",
            "           5       0.77      0.82      0.80       476\n",
            "\n",
            "    accuracy                           0.70      1042\n",
            "   macro avg       0.58      0.54      0.54      1042\n",
            "weighted avg       0.69      0.70      0.69      1042\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.03      0.05        31\n",
            "           1       0.55      0.68      0.61       179\n",
            "           2       0.45      0.48      0.46        84\n",
            "           3       0.49      0.42      0.46       116\n",
            "           4       0.89      0.85      0.87       156\n",
            "           5       0.78      0.78      0.78       476\n",
            "\n",
            "    accuracy                           0.69      1042\n",
            "   macro avg       0.56      0.54      0.54      1042\n",
            "weighted avg       0.68      0.69      0.68      1042\n",
            "\n",
            "  Accuracy: 0.69\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.06      0.11        31\n",
            "           1       0.58      0.67      0.62       179\n",
            "           2       0.48      0.48      0.48        84\n",
            "           3       0.53      0.30      0.38       116\n",
            "           4       0.87      0.87      0.87       156\n",
            "           5       0.76      0.84      0.80       476\n",
            "\n",
            "    accuracy                           0.70      1042\n",
            "   macro avg       0.59      0.54      0.54      1042\n",
            "weighted avg       0.68      0.70      0.69      1042\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.06      0.11        31\n",
            "           1       0.63      0.64      0.63       179\n",
            "           2       0.42      0.46      0.44        84\n",
            "           3       0.51      0.32      0.39       116\n",
            "           4       0.87      0.87      0.87       156\n",
            "           5       0.76      0.85      0.80       476\n",
            "\n",
            "    accuracy                           0.70      1042\n",
            "   macro avg       0.58      0.53      0.54      1042\n",
            "weighted avg       0.68      0.70      0.69      1042\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.06      0.11        31\n",
            "           1       0.60      0.64      0.62       179\n",
            "           2       0.47      0.49      0.48        84\n",
            "           3       0.48      0.34      0.40       116\n",
            "           4       0.88      0.86      0.87       156\n",
            "           5       0.77      0.83      0.80       476\n",
            "\n",
            "    accuracy                           0.70      1042\n",
            "   macro avg       0.58      0.54      0.54      1042\n",
            "weighted avg       0.68      0.70      0.69      1042\n",
            "\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_E_Section\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:09:22 (h:mm:ss)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 0.9383972057303471,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.7176978114478114,\n",
              "  'Valid. Loss': 0.7709808936624816,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 0.7394315131491769,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6959175084175084,\n",
              "  'Valid. Loss': 0.7524591855930559,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 0.6792167819807554,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.7141203703703703,\n",
              "  'Valid. Loss': 0.7146990380503915,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.6361822410655104,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.7152777777777778,\n",
              "  'Valid. Loss': 0.7179566946896639,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.6112449343090578,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.7122264309764309,\n",
              "  'Valid. Loss': 0.7005332473552588,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.5819331505192832,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6972853535353536,\n",
              "  'Valid. Loss': 0.7097716412760995,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.5579547223479268,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6868686868686869,\n",
              "  'Valid. Loss': 0.7212671836217245,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.5373783015960719,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.7025462962962963,\n",
              "  'Valid. Loss': 0.7164249989119443,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.5241454609323281,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.702756734006734,\n",
              "  'Valid. Loss': 0.7208246986071268,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.5130600677517086,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6991792929292929,\n",
              "  'Valid. Loss': 0.7204886936780178,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb6gLiX5rp8A",
        "outputId": "6b404b55-239d-43be-883a-4acc337bf6cb"
      },
      "source": [
        "ancm.load_notam_model(input_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Albert notam model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFjNixA-SFdz",
        "outputId": "c1043a21-1598-4534-a76b-935d3973f87f"
      },
      "source": [
        "ancm.validate(validation_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        29\n",
            "           1       0.47      0.49      0.48       171\n",
            "           2       0.44      0.34      0.39        91\n",
            "           3       0.68      0.10      0.18       127\n",
            "           4       0.80      0.79      0.79       141\n",
            "           5       0.68      0.90      0.78       483\n",
            "\n",
            "    accuracy                           0.65      1042\n",
            "   macro avg       0.51      0.44      0.44      1042\n",
            "weighted avg       0.62      0.65      0.60      1042\n",
            "\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 0.95\n",
            "  Validation took: 0:00:02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9533687035242716, 0.6493055555555555, '0:00:02')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlTFWsE0Lh60"
      },
      "source": [
        "# AE Section -> ACC: 66%\n",
        "\n",
        "[CLS] A section... [SEP][CLS] E section ...[SEP]\n",
        "로 입력.... 나머지는 똑같음\n",
        "\n",
        "[SEP][CLS] 이거 없이 그냥 박치기 -> 64%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-f95Lzkeb8q"
      },
      "source": [
        "## Trainset, Validationset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0ojNjRgLgyl",
        "outputId": "f09e77d4-59a9-42f6-b7e2-fc8a683297e5"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "# For every sentence...\n",
        "for row in df.iterrows():\n",
        "    # A section\n",
        "    sent_A = clean_text(str(row[1][7]).lower())     \n",
        "    # E section\n",
        "    sent_E = clean_text(str(row[1][11]).lower()) \n",
        "    # label for E Section\n",
        "    lb = row[1][4]+0\n",
        "    if lb == 6:\n",
        "        lb = 5\n",
        "    labels.append(lb)\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent_A + ' [SEP] [CLS] ' + sent_E,                      # Sentence to encode.\n",
        "                        #sent_A + sent_E,   \n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation = True,\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in input_ids[0].tolist()]))\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Training & Validation Split\n",
        "# Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2187: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokens (str)      : ['[CLS]', '▁ybbb', '/', 'ymmm', '[SEP]', '[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'af', '▁intran', 'et', '.', '▁mil', '▁use', '▁only', '.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    5,  1027,   120,  9311,     6,     5,  3011,   158,   325,   150,\n",
            "          948,   121,  6600,  7011,  4561,   282,  5641,   228,   541,   794,\n",
            "         2656,   741,   158,  4850,  1537, 11342,   124,   156,   481,  1539,\n",
            "          123,  3107,   177,   124,  9695,   948,   121,  6600,  7011,  4561,\n",
            "          282,  5641,   228,   541,   794,  2656,   741,   124,  9695,   239,\n",
            "          158,  4850,  1537, 11342,   124,   156,   741,   324,   776,   546,\n",
            "          231,  3011,   133,  3859, 16232,  1786,   123,   491,   296,   218,\n",
            "          123,     6, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988])\n",
            "9,373 training samples\n",
            "1,042 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_g6jaOzcWBf",
        "outputId": "4986399a-3bdd-4b66-fd6f-e9b80d00823f"
      },
      "source": [
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_AE_Section')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(22089, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 1.38\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.47      0.47       173\n",
            "           1       0.43      0.59      0.50       196\n",
            "           2       0.42      0.15      0.22       108\n",
            "           3       0.47      0.14      0.22        98\n",
            "           4       0.63      0.66      0.64       183\n",
            "           5       0.64      0.78      0.70       284\n",
            "\n",
            "    accuracy                           0.55      1042\n",
            "   macro avg       0.51      0.46      0.46      1042\n",
            "weighted avg       0.53      0.55      0.52      1042\n",
            "\n",
            "  Accuracy: 0.55\n",
            "  Validation Loss: 1.21\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 1.16\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.57      0.53       173\n",
            "           1       0.60      0.49      0.54       196\n",
            "           2       0.43      0.40      0.41       108\n",
            "           3       0.49      0.34      0.40        98\n",
            "           4       0.72      0.70      0.71       183\n",
            "           5       0.66      0.78      0.71       284\n",
            "\n",
            "    accuracy                           0.60      1042\n",
            "   macro avg       0.56      0.55      0.55      1042\n",
            "weighted avg       0.59      0.60      0.59      1042\n",
            "\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.54      0.54       173\n",
            "           1       0.57      0.69      0.62       196\n",
            "           2       0.45      0.37      0.41       108\n",
            "           3       0.51      0.32      0.39        98\n",
            "           4       0.73      0.77      0.75       183\n",
            "           5       0.73      0.74      0.74       284\n",
            "\n",
            "    accuracy                           0.62      1042\n",
            "   macro avg       0.59      0.57      0.57      1042\n",
            "weighted avg       0.62      0.62      0.62      1042\n",
            "\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 1.03\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.57      0.55       173\n",
            "           1       0.64      0.58      0.61       196\n",
            "           2       0.47      0.46      0.47       108\n",
            "           3       0.62      0.33      0.43        98\n",
            "           4       0.75      0.75      0.75       183\n",
            "           5       0.70      0.83      0.76       284\n",
            "\n",
            "    accuracy                           0.64      1042\n",
            "   macro avg       0.62      0.59      0.59      1042\n",
            "weighted avg       0.64      0.64      0.63      1042\n",
            "\n",
            "  Accuracy: 0.64\n",
            "  Validation Loss: 1.02\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.95\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.62      0.57       173\n",
            "           1       0.62      0.61      0.61       196\n",
            "           2       0.46      0.44      0.45       108\n",
            "           3       0.53      0.35      0.42        98\n",
            "           4       0.74      0.77      0.75       183\n",
            "           5       0.73      0.75      0.74       284\n",
            "\n",
            "    accuracy                           0.63      1042\n",
            "   macro avg       0.60      0.59      0.59      1042\n",
            "weighted avg       0.63      0.63      0.63      1042\n",
            "\n",
            "  Accuracy: 0.63\n",
            "  Validation Loss: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.91\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.66      0.60       173\n",
            "           1       0.64      0.60      0.62       196\n",
            "           2       0.51      0.45      0.48       108\n",
            "           3       0.63      0.35      0.45        98\n",
            "           4       0.76      0.76      0.76       183\n",
            "           5       0.72      0.81      0.76       284\n",
            "\n",
            "    accuracy                           0.66      1042\n",
            "   macro avg       0.64      0.61      0.61      1042\n",
            "weighted avg       0.66      0.66      0.65      1042\n",
            "\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 0.97\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.87\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.72      0.62       173\n",
            "           1       0.63      0.62      0.62       196\n",
            "           2       0.52      0.45      0.48       108\n",
            "           3       0.53      0.37      0.43        98\n",
            "           4       0.78      0.73      0.76       183\n",
            "           5       0.75      0.75      0.75       284\n",
            "\n",
            "    accuracy                           0.65      1042\n",
            "   macro avg       0.63      0.61      0.61      1042\n",
            "weighted avg       0.65      0.65      0.65      1042\n",
            "\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.84\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.60      0.58       173\n",
            "           1       0.59      0.63      0.61       196\n",
            "           2       0.49      0.45      0.47       108\n",
            "           3       0.55      0.35      0.42        98\n",
            "           4       0.75      0.75      0.75       183\n",
            "           5       0.72      0.78      0.75       284\n",
            "\n",
            "    accuracy                           0.64      1042\n",
            "   macro avg       0.61      0.59      0.60      1042\n",
            "weighted avg       0.64      0.64      0.63      1042\n",
            "\n",
            "  Accuracy: 0.64\n",
            "  Validation Loss: 0.96\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.70      0.62       173\n",
            "           1       0.66      0.54      0.59       196\n",
            "           2       0.49      0.49      0.49       108\n",
            "           3       0.58      0.34      0.43        98\n",
            "           4       0.75      0.75      0.75       183\n",
            "           5       0.71      0.80      0.75       284\n",
            "\n",
            "    accuracy                           0.65      1042\n",
            "   macro avg       0.62      0.60      0.61      1042\n",
            "weighted avg       0.65      0.65      0.64      1042\n",
            "\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 0.96\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.70      0.63       173\n",
            "           1       0.61      0.60      0.61       196\n",
            "           2       0.50      0.47      0.49       108\n",
            "           3       0.57      0.36      0.44        98\n",
            "           4       0.77      0.75      0.76       183\n",
            "           5       0.74      0.77      0.75       284\n",
            "\n",
            "    accuracy                           0.65      1042\n",
            "   macro avg       0.63      0.61      0.61      1042\n",
            "weighted avg       0.65      0.65      0.65      1042\n",
            "\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 0.96\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_AE_Section\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:09:16 (h:mm:ss)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 1.3827737767134918,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.5454545454545454,\n",
              "  'Valid. Loss': 1.211962559006431,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 1.1570194571091454,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.5966961279461279,\n",
              "  'Valid. Loss': 1.0765111897930955,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 1.0535798080952095,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6253156565656566,\n",
              "  'Valid. Loss': 1.0331530462611804,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.9885760507079114,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6406776094276094,\n",
              "  'Valid. Loss': 1.0157358465772686,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.9467147973210331,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6340488215488216,\n",
              "  'Valid. Loss': 0.9881662744464297,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.9052099222412695,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6567760942760943,\n",
              "  'Valid. Loss': 0.9721537160150933,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.8723425069766647,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6510942760942761,\n",
              "  'Valid. Loss': 0.9765027924017473,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.8419133401364597,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6404671717171717,\n",
              "  'Valid. Loss': 0.9649069580164823,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.8214250992182579,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.648253367003367,\n",
              "  'Valid. Loss': 0.9646037031303752,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.8075616482785131,\n",
              "  'Training Time': '0:00:54',\n",
              "  'Valid. Accur.': 0.6529882154882155,\n",
              "  'Valid. Loss': 0.9599067135290666,\n",
              "  'Validation Time': '0:00:02',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jS5LKp4Tj1u",
        "outputId": "cbd6366a-c69f-4c0b-f1a0-436550a5b713"
      },
      "source": [
        "ancm.load_notam_model(input_dir='/content/drive/MyDrive/NOTAM/notam_model/For_AE_Section')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Albert notam model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeLfjzVJTqBG",
        "outputId": "e1c2d9b2-0404-4961-9e47-17d1803168c7"
      },
      "source": [
        "ancm.validate(validation_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.68      0.64       177\n",
            "           1       0.60      0.56      0.58       191\n",
            "           2       0.42      0.39      0.41        97\n",
            "           3       0.64      0.33      0.44       103\n",
            "           4       0.74      0.76      0.75       148\n",
            "           5       0.72      0.82      0.77       326\n",
            "\n",
            "    accuracy                           0.65      1042\n",
            "   macro avg       0.62      0.59      0.60      1042\n",
            "weighted avg       0.65      0.65      0.64      1042\n",
            "\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 0.97\n",
            "  Validation took: 0:00:02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9746354529351899, 0.6542508417508418, '0:00:02')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN-Cw8MJVz4o"
      },
      "source": [
        "## QAE Section -> ACC:57%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQwIrEXUV2Zb",
        "outputId": "9e293ed5-e941-4426-ca01-d5fd208c3fff"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "# For every sentence...\n",
        "for row in df.iterrows():\n",
        "    # Q section\n",
        "    sent_Q = clean_text(str(row[1][6]).lower())      \n",
        "    # A section\n",
        "    sent_A = clean_text(str(row[1][7]).lower())     \n",
        "    # E section\n",
        "    sent_E = clean_text(str(row[1][11]).lower()) \n",
        "    # label for E Section\n",
        "    lb = row[1][5]+0\n",
        "    if lb == 6:\n",
        "        lb = 5\n",
        "    labels.append(lb)\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent_Q + ' [SEP] [CLS] ' + sent_A + ' [SEP] [CLS] ' + sent_E,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation = True,\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in input_ids[0].tolist()]))\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Training & Validation Split\n",
        "# Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2187: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3LyDb6cctGT",
        "outputId": "66ebabfe-2c67-4315-860f-8f0a87e8381f"
      },
      "source": [
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(22089, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 1.20\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.41      0.47       179\n",
            "           1       0.50      0.65      0.57       321\n",
            "           2       0.64      0.23      0.34        79\n",
            "           3       0.40      0.24      0.30        96\n",
            "           4       0.61      0.50      0.55       208\n",
            "           5       0.49      0.69      0.57       159\n",
            "\n",
            "    accuracy                           0.52      1042\n",
            "   macro avg       0.53      0.46      0.47      1042\n",
            "weighted avg       0.53      0.52      0.51      1042\n",
            "\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 1.15\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 1.03\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.69      0.62       179\n",
            "           1       0.63      0.53      0.57       321\n",
            "           2       0.43      0.52      0.47        79\n",
            "           3       0.42      0.30      0.35        96\n",
            "           4       0.58      0.60      0.59       208\n",
            "           5       0.56      0.60      0.58       159\n",
            "\n",
            "    accuracy                           0.56      1042\n",
            "   macro avg       0.53      0.54      0.53      1042\n",
            "weighted avg       0.56      0.56      0.56      1042\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.11\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.66      0.59       179\n",
            "           1       0.62      0.59      0.60       321\n",
            "           2       0.47      0.43      0.45        79\n",
            "           3       0.42      0.44      0.43        96\n",
            "           4       0.60      0.60      0.60       208\n",
            "           5       0.61      0.52      0.56       159\n",
            "\n",
            "    accuracy                           0.57      1042\n",
            "   macro avg       0.54      0.54      0.54      1042\n",
            "weighted avg       0.57      0.57      0.57      1042\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.10\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.91\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.68      0.62       179\n",
            "           1       0.62      0.60      0.61       321\n",
            "           2       0.52      0.48      0.50        79\n",
            "           3       0.47      0.31      0.38        96\n",
            "           4       0.56      0.67      0.61       208\n",
            "           5       0.60      0.52      0.56       159\n",
            "\n",
            "    accuracy                           0.58      1042\n",
            "   macro avg       0.56      0.54      0.55      1042\n",
            "weighted avg       0.58      0.58      0.57      1042\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.86\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.66      0.61       179\n",
            "           1       0.62      0.51      0.56       321\n",
            "           2       0.46      0.51      0.48        79\n",
            "           3       0.48      0.42      0.44        96\n",
            "           4       0.58      0.62      0.60       208\n",
            "           5       0.54      0.58      0.56       159\n",
            "\n",
            "    accuracy                           0.56      1042\n",
            "   macro avg       0.54      0.55      0.54      1042\n",
            "weighted avg       0.56      0.56      0.56      1042\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.12\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.83\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.70      0.63       179\n",
            "           1       0.62      0.57      0.60       321\n",
            "           2       0.52      0.46      0.49        79\n",
            "           3       0.49      0.35      0.41        96\n",
            "           4       0.57      0.62      0.59       208\n",
            "           5       0.58      0.58      0.58       159\n",
            "\n",
            "    accuracy                           0.58      1042\n",
            "   macro avg       0.56      0.55      0.55      1042\n",
            "weighted avg       0.58      0.58      0.57      1042\n",
            "\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.10\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.80\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.68      0.59       179\n",
            "           1       0.61      0.55      0.58       321\n",
            "           2       0.52      0.39      0.45        79\n",
            "           3       0.43      0.36      0.39        96\n",
            "           4       0.56      0.58      0.57       208\n",
            "           5       0.58      0.57      0.57       159\n",
            "\n",
            "    accuracy                           0.55      1042\n",
            "   macro avg       0.53      0.52      0.53      1042\n",
            "weighted avg       0.55      0.55      0.55      1042\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.77\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.61      0.60       179\n",
            "           1       0.59      0.58      0.59       321\n",
            "           2       0.57      0.49      0.53        79\n",
            "           3       0.45      0.39      0.41        96\n",
            "           4       0.57      0.58      0.58       208\n",
            "           5       0.54      0.60      0.57       159\n",
            "\n",
            "    accuracy                           0.56      1042\n",
            "   macro avg       0.55      0.54      0.54      1042\n",
            "weighted avg       0.56      0.56      0.56      1042\n",
            "\n",
            "  Accuracy: 0.56\n",
            "  Validation Loss: 1.16\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.76\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.65      0.60       179\n",
            "           1       0.60      0.57      0.58       321\n",
            "           2       0.57      0.48      0.52        79\n",
            "           3       0.45      0.36      0.40        96\n",
            "           4       0.59      0.61      0.60       208\n",
            "           5       0.57      0.60      0.58       159\n",
            "\n",
            "    accuracy                           0.57      1042\n",
            "   macro avg       0.55      0.55      0.55      1042\n",
            "weighted avg       0.57      0.57      0.57      1042\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 0.74\n",
            "  Training epcoh took: 0:00:54\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.68      0.60       179\n",
            "           1       0.64      0.56      0.60       321\n",
            "           2       0.54      0.48      0.51        79\n",
            "           3       0.39      0.33      0.36        96\n",
            "           4       0.59      0.61      0.60       208\n",
            "           5       0.58      0.62      0.60       159\n",
            "\n",
            "    accuracy                           0.57      1042\n",
            "   macro avg       0.55      0.55      0.54      1042\n",
            "weighted avg       0.57      0.57      0.57      1042\n",
            "\n",
            "  Accuracy: 0.57\n",
            "  Validation Loss: 1.16\n",
            "  Validation took: 0:00:02\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:09:22 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yzP8_WgTwqa",
        "outputId": "d6b16d8f-00b9-4397-d461-e0893f51fc91"
      },
      "source": [
        "ancm.load_notam_model(input_dir='/content/drive/MyDrive/NOTAM/notam_model/For_QAE_Section')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Albert notam model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1BiKiGvTyiT",
        "outputId": "74652703-b85d-4e6c-d3ff-d9558d21d488"
      },
      "source": [
        "ancm.validate(validation_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.57      0.52       175\n",
            "           1       0.59      0.57      0.58       326\n",
            "           2       0.48      0.27      0.34        83\n",
            "           3       0.47      0.39      0.43        95\n",
            "           4       0.54      0.58      0.56       206\n",
            "           5       0.49      0.54      0.51       157\n",
            "\n",
            "    accuracy                           0.53      1042\n",
            "   macro avg       0.51      0.48      0.49      1042\n",
            "weighted avg       0.53      0.53      0.52      1042\n",
            "\n",
            "  Accuracy: 0.53\n",
            "  Validation Loss: 1.21\n",
            "  Validation took: 0:00:02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2070666696086074, 0.5270412457912458, '0:00:02')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjxTaVBdxNev"
      },
      "source": [
        "# BERT에서 multi input을 활용하는 방법들...\n",
        "\n",
        "Combining Categorical and Numerical Features with Text in BERT\n",
        "\n",
        "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
        "\n",
        "Combine multiple sentences together during tokenization\n",
        "\n",
        "https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430\n",
        "\n",
        "\n",
        "How to add a CNN layer on top of BERT?\n",
        "\n",
        "https://datascience.stackexchange.com/questions/54412/how-to-add-a-cnn-layer-on-top-of-bert\n",
        "\n",
        "\n",
        "Multi-label Text Classification with BERT and PyTorch Lightning\n",
        "\n",
        "https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YZNBfQFYafc"
      },
      "source": [
        "from transformers import AlbertModel\n",
        "\n",
        "class Mi_model(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, n_classes):\n",
        "        super().__init__()\n",
        "        self.bert = AlbertModel.from_pretrained(model_name, return_dict=True)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,labels):\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        output = self.classifier(output.pooler_output)\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return [loss, output]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNPckRSncqgt",
        "outputId": "ee879144-4612-41fa-c5ee-e0e5c9dbc74f"
      },
      "source": [
        "mm = Mi_model(model_name='/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model',n_classes=6)\n",
        "print(mm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mi_model(\n",
            "  (bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(22089, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uiQsoUdddRV",
        "outputId": "f8e6b739-e575-493e-f193-4b3f28f6b681"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def __flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def __format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcLKFjenc9SR",
        "outputId": "810aec7f-4812-4f86-9c86-dad5f9e3b2af"
      },
      "source": [
        "\n",
        "epochs = 1\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "mm.cuda()\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(mm.named_parameters())\n",
        "\n",
        "print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(mm.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "#epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "    \n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    mm.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = __format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        #print(b_input_ids)\n",
        "        #print(b_input_mask)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        mm.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "\n",
        "        outputs = mm(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(mm.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = __format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    mm.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        for l in batch[2]:\n",
        "            true_labels.append(l.item())        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = mm(b_input_ids, attention_mask=b_input_mask,labels=b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        for li in logits:\n",
        "            pred_labels.append(np.argmax(li))\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += __flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    print(\"  classification_report   \")    \n",
        "    print(classification_report(true_labels,pred_labels))\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = __format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(__format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:36.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 1.35\n",
            "  Training epcoh took: 0:00:53\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        30\n",
            "           1       0.51      0.63      0.56       181\n",
            "           2       0.57      0.34      0.43        99\n",
            "           3       0.83      0.14      0.24       109\n",
            "           4       0.89      0.81      0.85       175\n",
            "           5       0.70      0.90      0.79       448\n",
            "\n",
            "    accuracy                           0.68      1042\n",
            "   macro avg       0.58      0.47      0.48      1042\n",
            "weighted avg       0.68      0.68      0.64      1042\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 1.33\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:55 (h:mm:ss)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HzDAad-jvVe"
      },
      "source": [
        "class EmbeddingMapping():\n",
        "    \"\"\"\n",
        "    Helper class for handling categorical variables\n",
        "    An instance of this class should be defined for each categorical variable we want to use.\n",
        "    \"\"\"\n",
        "    def __init__(self, series):\n",
        "        # get a list of unique values\n",
        "        values = series.unique().tolist()\n",
        "        \n",
        "        # Set a dictionary mapping from values to integer value\n",
        "        # In our example this will be {'Mercaz': 1, 'Old North': 2, 'Florentine': 3}\n",
        "        self.embedding_dict = {value: int_value+1 for int_value, value in enumerate(values)}\n",
        "        \n",
        "        # The num_values will be used as the input_dim when defining the embedding layer. \n",
        "        # It will also be returned for unseen values \n",
        "        self.num_values = len(values) + 1\n",
        "\n",
        "    def get_mapping(self, value):\n",
        "        # If the value was seen in the training set, return its integer mapping\n",
        "        if value in self.embedding_dict:\n",
        "            return self.embedding_dict[value]\n",
        "        \n",
        "        # Else, return the same integer for unseen values\n",
        "        else:\n",
        "            return self.num_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot6rbRc0kIJX"
      },
      "source": [
        "  # code mapping\n",
        "  AS_mapping = EmbeddingMapping(df['A_LINE'])\n",
        "  df = df.assign(AS_mapping=df['A_LINE'].apply(AS_mapping.get_mapping))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "BU4911lHkZxv",
        "outputId": "e5f3e846-2c57-42a8-ec7b-17d48747d090"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSG_ID</th>\n",
              "      <th>UPDATED_BY</th>\n",
              "      <th>UPDATED_DATE</th>\n",
              "      <th>E_SCORE</th>\n",
              "      <th>AE_SCORE</th>\n",
              "      <th>ALL_SCORE</th>\n",
              "      <th>Q_LINE</th>\n",
              "      <th>A_LINE</th>\n",
              "      <th>B_LINE</th>\n",
              "      <th>C_LINE</th>\n",
              "      <th>D_LINE</th>\n",
              "      <th>E_LINE</th>\n",
              "      <th>AS_mapping</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6824038</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:08</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6824038</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6816851</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:09</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6816851</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:18</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6816333</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:11</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KZOB/QMXLC/IV/BO/A/000/999/4213N08321W005</td>\n",
              "      <td>KDTW</td>\n",
              "      <td>1905211704</td>\n",
              "      <td>1905211800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY K5 CLSD</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10410</th>\n",
              "      <td>15966850</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-08-01 10:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZNY/QMXLC/IV/BO/A/000/999/4038N07347W005</td>\n",
              "      <td>KJFK</td>\n",
              "      <td>1912311421</td>\n",
              "      <td>1912311600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY C BTN TWY C1 AND TWY V CLSD</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10411</th>\n",
              "      <td>15947262</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:06</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>LCCC/QWFLW/IV/M  /W /210/230/3335N03202E133</td>\n",
              "      <td>LCCC</td>\n",
              "      <td>2001040820</td>\n",
              "      <td>2001051150</td>\n",
              "      <td>0820-1150</td>\n",
              "      <td>AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...</td>\n",
              "      <td>468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10412</th>\n",
              "      <td>15962821</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:48</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZAU/QMRLC/IV/NBO/A/000/999/4257N08754W005</td>\n",
              "      <td>KMKE</td>\n",
              "      <td>1912310929</td>\n",
              "      <td>2001012359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 01L/19R CLSD</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10413</th>\n",
              "      <td>15965583</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-06 13:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>2001071500</td>\n",
              "      <td>2001072000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10414</th>\n",
              "      <td>15962570</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-08 14:49</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>1912311430</td>\n",
              "      <td>1912312100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10415 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         MSG_ID  ... AS_mapping\n",
              "0       6824038  ...          1\n",
              "1       6824038  ...          1\n",
              "2       6816851  ...          2\n",
              "3       6816851  ...          2\n",
              "4       6816333  ...          3\n",
              "...         ...  ...        ...\n",
              "10410  15966850  ...         72\n",
              "10411  15947262  ...        468\n",
              "10412  15962821  ...        108\n",
              "10413  15965583  ...         91\n",
              "10414  15962570  ...         91\n",
              "\n",
              "[10415 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI434HmpkgXd",
        "outputId": "e863ccad-446c-460b-da67-282ee53307e1"
      },
      "source": [
        "np.max(df['AS_mapping'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2-6MBDnvurg",
        "outputId": "2c3ee123-4fb2-48f4-b63e-a926469df093"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "x = torch.tensor([1, 0])\n",
        "F.one_hot(x, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNRobYmZv1od",
        "outputId": "b1bac44c-a077-4d09-ebf1-a528ddf6e060"
      },
      "source": [
        "x = torch.tensor([0,1,1,0,0,1,0])\n",
        "F.one_hot(x, num_classes=2).float()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9999, 0.0000],\n",
              "        [0.0000, 0.9999],\n",
              "        [0.0000, 0.9999],\n",
              "        [0.9999, 0.0000],\n",
              "        [0.9999, 0.0000],\n",
              "        [0.0000, 0.9999],\n",
              "        [0.9999, 0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFz1FO-xxAFv",
        "outputId": "dd2300e2-6c27-4243-f4f9-2d3f92044fc3"
      },
      "source": [
        "x = torch.tensor([0,1,1,0,0,1,0])\n",
        "a = F.one_hot(x, num_classes=2).float()\n",
        "print(a)\n",
        "b = torch.tensor([[0],[0],[0],[1],[1],[1],[0]])\n",
        "print(b)\n",
        "torch.cat((a,b),1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [0., 1., 1.],\n",
              "        [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ6tSkhfijYi",
        "outputId": "d8a343e3-7e3c-4d72-db4d-823f35112eba"
      },
      "source": [
        "num_classes = 4\n",
        "embedding_size = 10\n",
        "\n",
        "embedding = nn.Embedding(num_classes, embedding_size)\n",
        "\n",
        "class_vector = torch.tensor([1, 0, 3, 3, 2])\n",
        "\n",
        "embedded_classes = embedding(class_vector)\n",
        "embedded_classes.size() # => torch.Size([5, 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHde8s8ZluI_"
      },
      "source": [
        "# A Section for categorical, E Section for Text -> ACC:56%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t9gssQGwNC6"
      },
      "source": [
        "### embedding version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XNL_wk2l1la"
      },
      "source": [
        "from transformers import AlbertModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Mi_a_model(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, a_mapping_classes, n_classes):\n",
        "        super().__init__()\n",
        "        self.a_mapping_classes = a_mapping_classes\n",
        "        self.bert = AlbertModel.from_pretrained(model_name, return_dict=True)\n",
        "        #self.hidden = nn.Linear(self.a_mapping_classes,8)    \n",
        "        self.embedding = nn.Embedding(self.a_mapping_classes, 256)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size + 256, n_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,a_mapping,labels):\n",
        "        #ta = torch.tensor(np.eye(self.a_mapping_classes, dtype='uint8')[a_mapping])\n",
        "        #ta = F.one_hot(a_mapping, num_classes=self.a_mapping_classes).float()\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        map_output = self.embedding(a_mapping)\n",
        "        #print(output.pooler_output)\n",
        "        #print(map_output)\n",
        "        #print(torch.cat((output.pooler_output,map_output),1))\n",
        "        \n",
        "        output = self.classifier(torch.cat((output.pooler_output,map_output),1))\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return [loss, output]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMoj7pj6wR8Y"
      },
      "source": [
        "### one_hot version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu935W-SwWHT"
      },
      "source": [
        "from transformers import AlbertModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Mi_a_model(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, a_mapping_classes, n_classes):\n",
        "        super().__init__()\n",
        "        self.a_mapping_classes = a_mapping_classes\n",
        "        self.bert = AlbertModel.from_pretrained(model_name, return_dict=True)\n",
        "        self.hidden = nn.Linear(self.a_mapping_classes,128)\n",
        "        #self.ReLU = nn.ReLU()\n",
        "        #self.hidden2 = nn.Linear(self.a_mapping_classes*2,32)\n",
        "        \n",
        "        #self.embedding = nn.Embedding(self.a_mapping_classes, 256)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size + 128, n_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,a_mapping,labels):\n",
        "        #ta = torch.tensor(np.eye(self.a_mapping_classes, dtype='uint8')[a_mapping])\n",
        "        ta = F.one_hot(a_mapping, num_classes=self.a_mapping_classes).float()\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        map_output = self.hidden(ta)\n",
        "        #map_output = self.ReLU(map_output)\n",
        "        #map_output = self.hidden2(map_output)\n",
        "        #print(output.pooler_output)\n",
        "        #print(map_output)\n",
        "        #print(torch.cat((output.pooler_output,map_output),1))\n",
        "        \n",
        "        output = self.classifier(torch.cat((output.pooler_output,map_output),1))\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return [loss, output]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3taqxCPoVyC",
        "outputId": "dabca9ea-baf2-4a42-b643-b17e1732f778"
      },
      "source": [
        "mam = Mi_a_model(model_name='/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model',a_mapping_classes=np.max(df['AS_mapping'])+1,n_classes=6)\n",
        "print(mam)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mi_a_model(\n",
            "  (bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(22089, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (hidden): Linear(in_features=590, out_features=128, bias=True)\n",
            "  (classifier): Linear(in_features=896, out_features=6, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usdn0MYootq2"
      },
      "source": [
        "# AE Transet,Valicationset을 다시 만든다.\n",
        "\n",
        "A Section만 Mapping value로..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SQ__mX2otQ5",
        "outputId": "9e16b09b-98c2-490e-e2f7-79d6e9de88bd"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "a_mappings = []\n",
        "labels = []\n",
        "# For every sentence...\n",
        "for row in df.iterrows():\n",
        "    # A Mapping value\n",
        "    A_mapping = row[1][12]    \n",
        "    # E section\n",
        "    sent_E = clean_text(str(row[1][11]).lower()) \n",
        "    # label for E Section\n",
        "    lb = row[1][4]+0\n",
        "    if lb == 6:\n",
        "        lb = 5\n",
        "    labels.append(lb)\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent_E,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation = True,\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    a_mappings.append(A_mapping)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "a_mappings = torch.tensor(a_mappings)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in input_ids[0].tolist()]))\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Training & Validation Split\n",
        "# Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, a_mappings, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2187: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'af', '▁intran', 'et', '.', '▁mil', '▁use', '▁only', '.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    5,  3011,   158,   325,   150,   948,   121,  6600,  7011,  4561,\n",
            "          282,  5641,   228,   541,   794,  2656,   741,   158,  4850,  1537,\n",
            "        11342,   124,   156,   481,  1539,   123,  3107,   177,   124,  9695,\n",
            "          948,   121,  6600,  7011,  4561,   282,  5641,   228,   541,   794,\n",
            "         2656,   741,   124,  9695,   239,   158,  4850,  1537, 11342,   124,\n",
            "          156,   741,   324,   776,   546,   231,  3011,   133,  3859, 16232,\n",
            "         1786,   123,   491,   296,   218,   123,     6, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988])\n",
            "9,373 training samples\n",
            "1,042 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qjAOu3Dpydj"
      },
      "source": [
        "# A mapping, E Text에 대한 학습..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94t82bLip4TO",
        "outputId": "79e3d867-84ee-413b-b741-20dd2d28d4ab"
      },
      "source": [
        "\n",
        "epochs = 2\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "mam.cuda()\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(mam.named_parameters())\n",
        "\n",
        "print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(mam.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "#epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "    \n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    mam.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = __format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_a_mapping = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        #print(b_input_ids)\n",
        "        #print(b_input_mask)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        mam.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "\n",
        "        outputs = mam(b_input_ids,attention_mask=b_input_mask,a_mapping=b_a_mapping,labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(mam.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = __format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    mam.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_a_mapping = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        for l in batch[3]:\n",
        "            true_labels.append(l.item())        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = mam(b_input_ids, attention_mask=b_input_mask,a_mapping=b_a_mapping,labels=b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        for li in logits:\n",
        "            pred_labels.append(np.argmax(li))\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += __flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    print(\"  classification_report   \")    \n",
        "    print(classification_report(true_labels,pred_labels))\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = __format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(__format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 29 different named parameters.\n",
            "\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:45.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epcoh took: 0:00:55\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.36      0.38       201\n",
            "           1       0.44      0.43      0.43       200\n",
            "           2       0.20      0.09      0.13        99\n",
            "           3       0.45      0.06      0.10        87\n",
            "           4       0.48      0.58      0.53       154\n",
            "           5       0.55      0.77      0.64       301\n",
            "\n",
            "    accuracy                           0.48      1042\n",
            "   macro avg       0.42      0.38      0.37      1042\n",
            "weighted avg       0.45      0.48      0.44      1042\n",
            "\n",
            "  Accuracy: 0.48\n",
            "  Validation Loss: 1.52\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:45.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 1.48\n",
            "  Training epcoh took: 0:00:55\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.49      0.46       201\n",
            "           1       0.45      0.46      0.45       200\n",
            "           2       0.24      0.10      0.14        99\n",
            "           3       0.40      0.05      0.08        87\n",
            "           4       0.48      0.60      0.53       154\n",
            "           5       0.61      0.75      0.68       301\n",
            "\n",
            "    accuracy                           0.50      1042\n",
            "   macro avg       0.44      0.41      0.39      1042\n",
            "weighted avg       0.47      0.50      0.47      1042\n",
            "\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:54 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}