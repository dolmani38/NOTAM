{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNtfWiuzR3zGnCkAdhxI8l+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/NOTAM/blob/main/albert_notam_model_1024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tlIhtCjJia"
      },
      "source": [
        "# Albert NOTAM 언어 모델 학습!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc1weyK_jPwQ",
        "outputId": "2ac747e0-58b4-4c3e-b8c5-7c8cbc0d1776"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5DRavVNRey3"
      },
      "source": [
        "## 참조\n",
        "\n",
        "https://colab.research.google.com/github/parmarsuraj99/suraj-parmar/blob/master/_notebooks/2020-05-02-SanskritALBERT.ipynb#scrollTo=VNAOMXjpMHZD\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038IYm33Muol"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import torch\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVCIQF4UMmyF",
        "outputId": "a9fd3734-101b-4229-8f19-4dced228019d"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install transformers/.\n",
        "!pip install sentencepiece==0.1.95\n",
        "!pip install datasets==1.8.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "Processing ./transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.49.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.0.19)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.12.0.dev0-py3-none-any.whl size=2994301 sha256=67540090520f3b138da9baaa44eff846208bbd0837c08486ab165deb15458016\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xcxad049/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.12.0.dev0\n",
            "    Uninstalling transformers-4.12.0.dev0:\n",
            "      Successfully uninstalled transformers-4.12.0.dev0\n",
            "Successfully installed transformers-4.12.0.dev0\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Requirement already satisfied: datasets==1.8.0 in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (21.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (4.49.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.19.5)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.1.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2021.10.1)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.0.19)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.3.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.8.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.8.0) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.8.0) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.8.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_K0rsDlckzd"
      },
      "source": [
        "# NOTAM Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWN53GjXcqSa",
        "outputId": "3a9cd49c-bd4e-467a-f081-50a4f3a3a298"
      },
      "source": [
        "%%time\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/NOTAM/data/TRAIN_20210928.csv')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 70.3 ms, sys: 16.1 ms, total: 86.4 ms\n",
            "Wall time: 90.6 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "MPBJvnYscutM",
        "outputId": "39c2311e-853e-450b-cab7-3956280a0617"
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSG_ID</th>\n",
              "      <th>UPDATED_BY</th>\n",
              "      <th>UPDATED_DATE</th>\n",
              "      <th>E_SCORE</th>\n",
              "      <th>AE_SCORE</th>\n",
              "      <th>ALL_SCORE</th>\n",
              "      <th>Q_LINE</th>\n",
              "      <th>A_LINE</th>\n",
              "      <th>B_LINE</th>\n",
              "      <th>C_LINE</th>\n",
              "      <th>D_LINE</th>\n",
              "      <th>E_LINE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6231155</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-16 11:02</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>PAZA/QMNXX////000/999/6449N14751W005</td>\n",
              "      <td>PAFA</td>\n",
              "      <td>1902140120</td>\n",
              "      <td>1902150120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SOUTH TERMINAL RAMP \\nICE AND 1/8IN DRY SN OVE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6824038</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6824038</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:08</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6816851</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:18</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6816851</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:09</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23714</th>\n",
              "      <td>15947262</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:06</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>LCCC/QWFLW/IV/M  /W /210/230/3335N03202E133</td>\n",
              "      <td>LCCC</td>\n",
              "      <td>2001040820</td>\n",
              "      <td>2001051150</td>\n",
              "      <td>0820-1150</td>\n",
              "      <td>AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23715</th>\n",
              "      <td>15962821</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:48</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZAU/QMRLC/IV/NBO/A/000/999/4257N08754W005</td>\n",
              "      <td>KMKE</td>\n",
              "      <td>1912310929</td>\n",
              "      <td>2001012359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 01L/19R CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23716</th>\n",
              "      <td>15964683</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-17 17:00</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>KZOB/QMXXX////000/999/4213N08321W005</td>\n",
              "      <td>KDTW</td>\n",
              "      <td>1912311140</td>\n",
              "      <td>2001011140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY ALL 1/4IN DRY SN OBS AT 1912311140.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23717</th>\n",
              "      <td>15965583</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-06 13:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>2001071500</td>\n",
              "      <td>2001072000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23718</th>\n",
              "      <td>15962570</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-08 14:49</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>1912311430</td>\n",
              "      <td>1912312100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23719 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         MSG_ID  ...                                             E_LINE\n",
              "0       6231155  ...  SOUTH TERMINAL RAMP \\nICE AND 1/8IN DRY SN OVE...\n",
              "1       6824038  ...  AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...\n",
              "2       6824038  ...  AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...\n",
              "3       6816851  ...  LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...\n",
              "4       6816851  ...  LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...\n",
              "...         ...  ...                                                ...\n",
              "23714  15947262  ...  AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...\n",
              "23715  15962821  ...                                   RWY 01L/19R CLSD\n",
              "23716  15964683  ...            TWY ALL 1/4IN DRY SN OBS AT 1912311140.\n",
              "23717  15965583  ...  TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...\n",
              "23718  15962570  ...  TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...\n",
              "\n",
              "[23719 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYbBdeVJhhYN",
        "outputId": "29be197f-a0be-4b6b-f83d-31d16278248f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "# if nugu =='shbl1021' and date1 >= '2021-07-09 00:00':\n",
        "\n",
        "df1 = df[df['UPDATED_BY'] == 'shbl1021'][df['UPDATED_DATE'] >= '2021-07-09 00:00'][df['AE_SCORE'] > 0]\n",
        "df1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSG_ID</th>\n",
              "      <th>UPDATED_BY</th>\n",
              "      <th>UPDATED_DATE</th>\n",
              "      <th>E_SCORE</th>\n",
              "      <th>AE_SCORE</th>\n",
              "      <th>ALL_SCORE</th>\n",
              "      <th>Q_LINE</th>\n",
              "      <th>A_LINE</th>\n",
              "      <th>B_LINE</th>\n",
              "      <th>C_LINE</th>\n",
              "      <th>D_LINE</th>\n",
              "      <th>E_LINE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6231155</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-16 11:02</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>PAZA/QMNXX////000/999/6449N14751W005</td>\n",
              "      <td>PAFA</td>\n",
              "      <td>1902140120</td>\n",
              "      <td>1902150120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SOUTH TERMINAL RAMP \\nICE AND 1/8IN DRY SN OVE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6816735</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-18 15:31</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3606N00702W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905311900</td>\n",
              "      <td>27-28 31 0600-1900, 29 0600-0830 1200-1600, 30...</td>\n",
              "      <td>LED122 ACTIVATED. UPPER VERTICAL LIMIT MODIFIED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5502755</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-18 9:22</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KZAU/QMRXX////000/999/4158N08754W005</td>\n",
              "      <td>KORD</td>\n",
              "      <td>1901021632</td>\n",
              "      <td>1901031632</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 27L 5/5/5 100 PCT WET DEICED LIQUID \\nOBS ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5537714</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-18 11:24</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>PAZA/QMXXX////000/999/6449N14751W005</td>\n",
              "      <td>PAFA</td>\n",
              "      <td>1901041643</td>\n",
              "      <td>1901051643</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY C, D COMPACTED SN 30IN BERMS OBS AT 190104...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5554972</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-16 11:33</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>LKAA/QFDAS/I/M/A/000/999/5006N01416E005</td>\n",
              "      <td>LKPR</td>\n",
              "      <td>1901051740</td>\n",
              "      <td>1901071100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>VISUAL GUIDANCE DOCKING SYSTEM APIS ON STAND 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23714</th>\n",
              "      <td>15947262</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:06</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>LCCC/QWFLW/IV/M  /W /210/230/3335N03202E133</td>\n",
              "      <td>LCCC</td>\n",
              "      <td>2001040820</td>\n",
              "      <td>2001051150</td>\n",
              "      <td>0820-1150</td>\n",
              "      <td>AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23715</th>\n",
              "      <td>15962821</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:48</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZAU/QMRLC/IV/NBO/A/000/999/4257N08754W005</td>\n",
              "      <td>KMKE</td>\n",
              "      <td>1912310929</td>\n",
              "      <td>2001012359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 01L/19R CLSD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23716</th>\n",
              "      <td>15964683</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-09-17 17:00</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>KZOB/QMXXX////000/999/4213N08321W005</td>\n",
              "      <td>KDTW</td>\n",
              "      <td>1912311140</td>\n",
              "      <td>2001011140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY ALL 1/4IN DRY SN OBS AT 1912311140.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23717</th>\n",
              "      <td>15965583</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-06 13:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>2001071500</td>\n",
              "      <td>2001072000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23718</th>\n",
              "      <td>15962570</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-08 14:49</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>1912311430</td>\n",
              "      <td>1912312100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14996 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         MSG_ID  ...                                             E_LINE\n",
              "0       6231155  ...  SOUTH TERMINAL RAMP \\nICE AND 1/8IN DRY SN OVE...\n",
              "11      6816735  ...    LED122 ACTIVATED. UPPER VERTICAL LIMIT MODIFIED\n",
              "16      5502755  ...  RWY 27L 5/5/5 100 PCT WET DEICED LIQUID \\nOBS ...\n",
              "26      5537714  ...  TWY C, D COMPACTED SN 30IN BERMS OBS AT 190104...\n",
              "27      5554972  ...  VISUAL GUIDANCE DOCKING SYSTEM APIS ON STAND 2...\n",
              "...         ...  ...                                                ...\n",
              "23714  15947262  ...  AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...\n",
              "23715  15962821  ...                                   RWY 01L/19R CLSD\n",
              "23716  15964683  ...            TWY ALL 1/4IN DRY SN OBS AT 1912311140.\n",
              "23717  15965583  ...  TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...\n",
              "23718  15962570  ...  TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...\n",
              "\n",
              "[14996 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8lFNQCAiNBU",
        "outputId": "76e89ce9-102a-4196-8c8c-c305d02db84f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "df1['AE_SCORE'].hist()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fad2cbf1850>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "knGmtFXginvW",
        "outputId": "dc4f53bd-2e2b-4d62-8fd3-0148fad7f476"
      },
      "source": [
        "df2 = pd.read_csv('/content/drive/MyDrive/NOTAM/data/TRAIN_SET_2019.csv')\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSG_ID</th>\n",
              "      <th>Q_ICAO_CODE</th>\n",
              "      <th>Q_CODE</th>\n",
              "      <th>Q_LINE</th>\n",
              "      <th>A_LINE</th>\n",
              "      <th>B_LINE</th>\n",
              "      <th>C_LINE</th>\n",
              "      <th>D_LINE</th>\n",
              "      <th>E_LINE</th>\n",
              "      <th>F_LINE</th>\n",
              "      <th>G_LINE</th>\n",
              "      <th>CREATED_DATE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11970570</td>\n",
              "      <td>EVRR</td>\n",
              "      <td>QWBLW</td>\n",
              "      <td>EVRR/QWBLW/IV/M  /W /015/080/5648N02431E002</td>\n",
              "      <td>EVRR</td>\n",
              "      <td>1.90531e+09</td>\n",
              "      <td>1905311800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AEROBATIC FLIGHT TRAINING WILL TAKE PLACE\\nIN ...</td>\n",
              "      <td>1500FT AMSL</td>\n",
              "      <td>FL080</td>\n",
              "      <td>2019-05-30 19:56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9666169</td>\n",
              "      <td>EDGG</td>\n",
              "      <td>QSTCF</td>\n",
              "      <td>EDGG/QSTCF/IV/BO /A /000/999/4841N00913E005</td>\n",
              "      <td>EDDS</td>\n",
              "      <td>1.90502e+09</td>\n",
              "      <td>1905030500EST</td>\n",
              "      <td>NaN</td>\n",
              "      <td>STUTTGART TWR FREQ 119.050MHZ OUT OF SERVICE. ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-05-03 1:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9500003</td>\n",
              "      <td>URRV</td>\n",
              "      <td>QRRCA</td>\n",
              "      <td>URRV/QRRCA/IV/BO/W/000/490/4847N04348E010</td>\n",
              "      <td>URRV</td>\n",
              "      <td>1.90501e+09</td>\n",
              "      <td>1905312359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RESTRICTED AREA ACT: URR523.</td>\n",
              "      <td>GND</td>\n",
              "      <td>FL490</td>\n",
              "      <td>2019-04-25 11:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14017186</td>\n",
              "      <td>EVRR</td>\n",
              "      <td>QRRCA</td>\n",
              "      <td>EVRR/QRRCA/IV/BO /W /000/025/5634N02333E004</td>\n",
              "      <td>EVRR</td>\n",
              "      <td>1.90918e+09</td>\n",
              "      <td>1909182200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RESTRICTED AREA EVR15 ZALENIEKI ACT FOR UNMANN...</td>\n",
              "      <td>GND</td>\n",
              "      <td>2500FT AMSL</td>\n",
              "      <td>2019-09-17 19:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9827869</td>\n",
              "      <td>KZME</td>\n",
              "      <td>QMRXX</td>\n",
              "      <td>KZME/QMRXX////000/999/3438N08647W005</td>\n",
              "      <td>KHSV</td>\n",
              "      <td>1.90511e+09</td>\n",
              "      <td>1905122232</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 18L 5/5/5 100 PCT WET OBS AT 1905112232. C...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-05-12 7:39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610385</th>\n",
              "      <td>13808174</td>\n",
              "      <td>KZDV</td>\n",
              "      <td>QMXXX</td>\n",
              "      <td>KZDV/QMXXX////000/999/3952N10440W005</td>\n",
              "      <td>KDEN</td>\n",
              "      <td>1909060129</td>\n",
              "      <td>1909070129</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY ALL WET OBS AT 1909060129.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-09-06 10:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610386</th>\n",
              "      <td>12658425</td>\n",
              "      <td>KZFW</td>\n",
              "      <td>QMRXX</td>\n",
              "      <td>KZFW/QMRXX////000/999/3254N09702W005</td>\n",
              "      <td>KDFW</td>\n",
              "      <td>1907070142</td>\n",
              "      <td>1907080142</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 17C 5/5/5 100 PCT WET OBS AT 1907070141.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-07-07 10:49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610387</th>\n",
              "      <td>10064108</td>\n",
              "      <td>ENOR</td>\n",
              "      <td>QRDCA</td>\n",
              "      <td>ENOR/QRDCA/IV/BO /W /000/040/5806N00638E002</td>\n",
              "      <td>ENOR</td>\n",
              "      <td>1906170900</td>\n",
              "      <td>1906231900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DANGER AREA END257 AWES LISTA ACTIVATED. FOR C...</td>\n",
              "      <td>GND</td>\n",
              "      <td>4000FT AMSL</td>\n",
              "      <td>2019-05-31 15:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610388</th>\n",
              "      <td>14251481</td>\n",
              "      <td>WAAF</td>\n",
              "      <td>QWULW</td>\n",
              "      <td>WAAF/QWULW/IV/BO/W/000/007/0738S11257E002</td>\n",
              "      <td>WAAF</td>\n",
              "      <td>1910010100</td>\n",
              "      <td>1910061000</td>\n",
              "      <td>DLY 0100-1000</td>\n",
              "      <td>UNMANNED AERIAL VEHICLE (UAV) WILL TAKE PLACE ...</td>\n",
              "      <td>GND</td>\n",
              "      <td>700FT AGL</td>\n",
              "      <td>2019-09-30 11:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610389</th>\n",
              "      <td>12348741</td>\n",
              "      <td>KZFW</td>\n",
              "      <td>QMRLC</td>\n",
              "      <td>KZFW/QMRLC/IV/NBO/A/000/999/3254N09702W005</td>\n",
              "      <td>KDFW</td>\n",
              "      <td>1906210400</td>\n",
              "      <td>1906211100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 18R/36L CLSD</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019-06-21 2:20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>610390 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          MSG_ID Q_ICAO_CODE Q_CODE  ...       F_LINE       G_LINE      CREATED_DATE\n",
              "0       11970570        EVRR  QWBLW  ...  1500FT AMSL        FL080  2019-05-30 19:56\n",
              "1        9666169        EDGG  QSTCF  ...          NaN          NaN   2019-05-03 1:46\n",
              "2        9500003        URRV  QRRCA  ...          GND        FL490  2019-04-25 11:37\n",
              "3       14017186        EVRR  QRRCA  ...          GND  2500FT AMSL  2019-09-17 19:34\n",
              "4        9827869        KZME  QMRXX  ...          NaN          NaN   2019-05-12 7:39\n",
              "...          ...         ...    ...  ...          ...          ...               ...\n",
              "610385  13808174        KZDV  QMXXX  ...          NaN          NaN  2019-09-06 10:36\n",
              "610386  12658425        KZFW  QMRXX  ...          NaN          NaN  2019-07-07 10:49\n",
              "610387  10064108        ENOR  QRDCA  ...          GND  4000FT AMSL  2019-05-31 15:57\n",
              "610388  14251481        WAAF  QWULW  ...          GND    700FT AGL  2019-09-30 11:42\n",
              "610389  12348741        KZFW  QMRLC  ...          NaN          NaN   2019-06-21 2:20\n",
              "\n",
              "[610390 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAyAjbU1eBWU"
      },
      "source": [
        "\n",
        "# 간단한 전처리\n",
        "def clean_text(txt):\n",
        "    txt = txt.replace('\\n',' ')\n",
        "    txt = txt.replace('\\r',' ')    \n",
        "    #txt = txt.replace('=','')\n",
        "    txt = txt.replace('\\\"','')   \n",
        "    txt = txt.replace('\\'','')\n",
        "    #txt = txt.replace(',','')\n",
        "    #txt = txt.replace('..','')\n",
        "    #txt = txt.replace('...','')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    #txt = txt.replace('.','. ')\n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')           \n",
        "    txt = txt.replace('  ',' ')\n",
        "    txt = txt.replace('  ',' ')    \n",
        "    txt = txt.replace('  ',' ')   \n",
        "    txt = txt.replace('  ',' ')             \n",
        "    return txt.strip()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmFGrzRQdi_l"
      },
      "source": [
        "notam_contents = []\n",
        "\n",
        "for row in df.iterrows():\n",
        "    doc_cont = str(row[1][11])\n",
        "    # Q Section\n",
        "    notam_contents.append(clean_text(str(row[1][6]).lower()))\n",
        "    # A Section\n",
        "    notam_contents.append(clean_text(str(row[1][7]).lower()))\n",
        "    # E Section\n",
        "    notam_contents.append(clean_text(doc_cont.lower()))\n",
        "\n",
        "for row in df2.iterrows():\n",
        "    doc_cont = str(row[1][8])\n",
        "    # Q Section\n",
        "    notam_contents.append(clean_text(str(row[1][3]).lower()))\n",
        "    # A Section\n",
        "    notam_contents.append(clean_text(str(row[1][4]).lower()))\n",
        "    # E Section\n",
        "    notam_contents.append(clean_text(doc_cont.lower()))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfnVFX03d1Dn",
        "outputId": "5f239c2f-7f79-43c0-cfb4-adf49d1fbde1"
      },
      "source": [
        "len(notam_contents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1862415"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "f16NAw2Id3ei",
        "outputId": "a7bc48b1-2035-4df2-f213-334d2d43e999"
      },
      "source": [
        "notam_contents[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vr5SWcZpZ00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "52715d54-f1ba-4901-e4c7-f1fe59fb7609"
      },
      "source": [
        "f = open('/content/drive/MyDrive/NOTAM/train_tokenizer3.txt', 'r')\n",
        "while True:\n",
        "    line = f.readline()\n",
        "    if not line: break\n",
        "    notam_contents.append(clean_text(line.lower()))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ea2807136ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/NOTAM/train_tokenizer3.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnotam_contents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NOTAM/train_tokenizer3.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm0Z7smipl3J",
        "outputId": "bde7024e-d116-4e05-f267-561eb05e056f"
      },
      "source": [
        "len(notam_contents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1862415"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzHDt-FVdW6P",
        "outputId": "12b76d3b-fd04-4a95-d7b4-83084f207d4d"
      },
      "source": [
        "%%time\n",
        "# subword 학습을 위해 문장만 따로 저장\n",
        "with open('/content/drive/MyDrive/NOTAM/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in notam_contents:\n",
        "        f.write(line+'\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 695 ms, sys: 105 ms, total: 800 ms\n",
            "Wall time: 1.35 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8bNszLMewp4"
      },
      "source": [
        "# Tokenizer 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGBDbPNwy1X-",
        "outputId": "70cee2b2-e2ce-4739-b3ad-cf69c4fd6611"
      },
      "source": [
        "%%time\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "# spm_train --input=data/train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
        "\n",
        "input_file = '/content/drive/MyDrive/NOTAM/train_tokenizer.txt'\n",
        "vocab_size = 32000\n",
        "\n",
        "sp_model_root='/content/drive/MyDrive/NOTAM/albert_tokenizer_model'\n",
        "if not os.path.isdir(sp_model_root):\n",
        "    os.mkdir(sp_model_root)\n",
        "sp_model_name = 'spiece'\n",
        "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
        "model_type = 'unigram'  # 학습할 모델 선택, unigram이 더 성능이 좋음'bpe'\n",
        "character_coverage  = 1.0  # 전체를 cover 하기 위해, default=0.9995\n",
        "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
        "\n",
        "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
        "cmd = input_argument%(input_file, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "print('train done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train done\n",
            "CPU times: user 2min 29s, sys: 1.65 s, total: 2min 30s\n",
            "Wall time: 1min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23ogFfIztOR",
        "outputId": "d0329f94-5e1c-43eb-93c9-956941b6e03a"
      },
      "source": [
        "## check\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(sp_model_path))\n",
        "\n",
        "tokens = sp.encode_as_pieces(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "ids = sp.encode_as_ids(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "\n",
        "print(ids)\n",
        "print(tokens)\n",
        "\n",
        "tokens = sp.decode_pieces(tokens)\n",
        "ids = sp.decode_ids(ids)\n",
        "\n",
        "print(ids)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1826, 156, 149, 154, 516, 121, 129, 8596, 2815, 139, 531, 372, 1914, 5957, 597, 156, 397, 128, 173, 122, 1783, 407, 2897, 597, 154, 1193, 127, 9230, 7439, 516, 121, 129, 8596, 2815, 139, 531, 372, 1914, 5957, 597, 7439, 228, 156, 397, 128, 173, 122, 1783, 407, 2897, 597, 377, 764, 487, 381, 1826, 148, 122, 407, 648, 143, 122, 121, 123, 177, 127, 226, 343, 328, 127]\n",
            "['▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁', 'digital', '▁aeronautica', 'l', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'au', 's', 'd', 'a', 'fi', 'f', '▁ed', '▁8', ')', '▁withdrawn', '.', '▁replacement', '▁version', '▁australia', 'n', '▁', 'digital', '▁aeronautica', 'l', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁version', '▁2', '▁(', 'au', 's', 'd', 'a', 'fi', 'f', '▁ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'a', 'f', '▁int', 'r', 'a', 'n', 'e', 't', '.', '▁mil', '▁use', '▁only', '.']\n",
            "ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\n",
            "ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXWYH5FRyu1"
      },
      "source": [
        "# 여기서부터 다시 -> NOTAM Language model 만들기..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55T0J0MDFtrm",
        "outputId": "87113ea8-db71-414e-c6ff-9082d4357ae1"
      },
      "source": [
        "## 1) define special tokens\n",
        "user_defined_symbols = ['[BOS]','[EOS]','[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]']\n",
        "unused_token_num = 200\n",
        "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
        "user_defined_symbols = user_defined_symbols + unused_list\n",
        "\n",
        "print(user_defined_symbols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]', '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]', '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]', '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]', '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]', '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]', '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]', '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]', '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]', '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF_Q_0VTFzPv",
        "outputId": "a0ff2031-66a7-496c-923e-d3156bbc9f62"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "#'/content/drive/MyDrive/Tokenizer_train/albert_tokenizer_model'\n",
        "\n",
        "albet_tokenizer_model = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model'\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained(albet_tokenizer_model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file /content/drive/MyDrive/NOTAM/albert_tokenizer_model/config.json not found\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "file /content/drive/MyDrive/NOTAM/albert_tokenizer_model/config.json not found\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "QB0SlhA4GMQQ",
        "outputId": "3963b106-e5c6-45f5-8795-29bbed0be50e"
      },
      "source": [
        "op = tokenizer.encode(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\")\n",
        "print(op)\n",
        "tokenizer.decode(op)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 1826, 156, 149, 154, 516, 121, 129, 8596, 2815, 139, 531, 372, 1914, 5957, 597, 156, 397, 128, 173, 122, 1783, 407, 2897, 597, 154, 1193, 127, 9230, 7439, 516, 121, 129, 8596, 2815, 139, 531, 372, 1914, 5957, 597, 7439, 228, 156, 397, 128, 173, 122, 1783, 407, 2897, 597, 377, 764, 487, 381, 1826, 148, 122, 407, 648, 143, 122, 121, 123, 177, 127, 226, 343, 328, 127, 6]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.[SEP]'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMDfH8GZF_mK",
        "outputId": "8684fad1-e58b-4786-c86c-b3361e2cf417"
      },
      "source": [
        "# tokenizer에 special token 추가\n",
        "special_tokens_dict = {'additional_special_tokens': user_defined_symbols}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# check tokenizer vocab with special tokens\n",
        "print('check special tokens : %s'%tokenizer.all_special_tokens[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check special tokens : ['[CLS]', '[SEP]', '<unk>', '<pad>', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inr3cDwQG6Fs",
        "outputId": "90d7c413-d784-479f-a285-dd735088c1af"
      },
      "source": [
        "# save tokenizer model with special tokens\n",
        "tokenizer.save_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/spiece.model',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/added_tokens.json',\n",
              " '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwhlWrXXHr6L"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(albet_tokenizer_model+'_special')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHhYsZbH3zr",
        "outputId": "919ebc01-0884-433c-b0ab-2c2cbd168546"
      },
      "source": [
        "op = tokenizer(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   5, 1826,  156,  149,  154,  516,  121,  129, 8596, 2815,  139,  531,\n",
            "          372, 1914, 5957,  597,  156,  397,  128,  173,  122, 1783,  407, 2897,\n",
            "          597,  154, 1193,  127, 9230, 7439,  516,  121,  129, 8596, 2815,  139,\n",
            "          531,  372, 1914, 5957,  597, 7439,  228,  156,  397,  128,  173,  122,\n",
            "         1783,  407, 2897,  597,  377,  764,  487,  381, 1826,  148,  122,  407,\n",
            "          648,  143,  122,  121,  123,  177,  127,  226,  343,  328,  127,    6]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁', 'digital', '▁aeronautica', 'l', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'au', 's', 'd', 'a', 'fi', 'f', '▁ed', '▁8', ')', '▁withdrawn', '.', '▁replacement', '▁version', '▁australia', 'n', '▁', 'digital', '▁aeronautica', 'l', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁version', '▁2', '▁(', 'au', 's', 'd', 'a', 'fi', 'f', '▁ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'a', 'f', '▁int', 'r', 'a', 'n', 'e', 't', '.', '▁mil', '▁use', '▁only', '.', '[SEP]']\n",
            "Tokens (int)      : [5, 1826, 156, 149, 154, 516, 121, 129, 8596, 2815, 139, 531, 372, 1914, 5957, 597, 156, 397, 128, 173, 122, 1783, 407, 2897, 597, 154, 1193, 127, 9230, 7439, 516, 121, 129, 8596, 2815, 139, 531, 372, 1914, 5957, 597, 7439, 228, 156, 397, 128, 173, 122, 1783, 407, 2897, 597, 377, 764, 487, 381, 1826, 148, 122, 407, 648, 143, 122, 121, 123, 177, 127, 226, 343, 328, 127, 6]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paElX_hhJAoh",
        "outputId": "ab6f02e5-b173-4281-a568-4c6650d76bc4"
      },
      "source": [
        "#Checking vocabulary size\n",
        "vocab_size=tokenizer.vocab_size ; vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32000"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9RPI1UerOSCB",
        "outputId": "5323caf3-1759-47a7-d6c1-5e38974be9a0"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "        \"AlbertModel\"\n",
        "    ],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 6,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": vocab_size\n",
        "}\n",
        "with open(albet_tokenizer_model + \"_special/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "\n",
        "#Configuration for tokenizer.\n",
        "#Note: I set do_lower_case: False, and keep_accents:True\n",
        "# Opening JSON file\n",
        "f = open(albet_tokenizer_model+ \"_special/tokenizer_config.json\")\n",
        "   \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "tokenizer_config = json.load(f)\n",
        "\n",
        "tokenizer_config['max_len'] = 512\n",
        "tokenizer_config['model_type'] = 'albert'\n",
        "tokenizer_config['do_lower_case'] = False\n",
        "tokenizer_config['keep_accents'] = True\n",
        "\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as outfile:\n",
        "    json.dump(tokenizer_config, outfile)\n",
        "'''\n",
        "tokenizer_config = {\n",
        "\t\"max_len\": 512,\n",
        "\t\"model_type\": \"albert\",\n",
        "\t\"do_lower_case\":False, \n",
        "\t\"keep_accents\":True\n",
        "}\n",
        "with open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntokenizer_config = {\\n\\t\"max_len\": 512,\\n\\t\"model_type\": \"albert\",\\n\\t\"do_lower_case\":False, \\n\\t\"keep_accents\":True\\n}\\nwith open(albet_tokenizer_model+ \"_special/tokenizer_config.json\", \\'w\\') as fp:\\n    json.dump(tokenizer_config, fp)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdjOOoG6QX2p",
        "outputId": "881e3898-46ae-4101-bf21-c457b46f2dc3"
      },
      "source": [
        "\n",
        "#To train from scratch\n",
        "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special \\\n",
        "        --tokenizer_name /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special \\\n",
        "        --train_file /content/drive/MyDrive/NOTAM/train_tokenizer.txt \\\n",
        "        --output_dir /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model \\\n",
        "        --use_fast_tokenizer \\\n",
        "        --do_train \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 100000 \\\n",
        "        --logging_steps 100000 \\\n",
        "        --save_total_limit 2 \\\n",
        "        --num_train_epochs 10 \\\n",
        "        --seed 108 \\\n",
        "        --overwrite_output_dir \\\n",
        "        --logging_dir /content/drive/MyDrive/Tokenizer_train/logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:33:50,264 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-61500] due to args.save_total_limit\n",
            "{'loss': 2.7488, 'learning_rate': 4.928785234353099e-05, 'epoch': 0.28}\n",
            "  1% 63000/4423240 [1:39:02<86:35:07, 13.99it/s][INFO|trainer.py:1937] 2021-09-13 11:34:36,778 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:34:36,785 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:34:36,983 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:34:36,991 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:34:36,995 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:34:37,610 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62000] due to args.save_total_limit\n",
            "{'loss': 2.7116, 'learning_rate': 4.9282200378003456e-05, 'epoch': 0.29}\n",
            "  1% 63500/4423240 [1:39:50<130:41:31,  9.27it/s][INFO|trainer.py:1937] 2021-09-13 11:35:24,049 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:35:24,056 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:35:24,239 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:35:24,245 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:35:24,267 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:35:24,849 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-62500] due to args.save_total_limit\n",
            "{'loss': 2.6968, 'learning_rate': 4.927654841247593e-05, 'epoch': 0.29}\n",
            "  1% 64000/4423240 [1:40:36<123:35:02,  9.80it/s][INFO|trainer.py:1937] 2021-09-13 11:36:10,104 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:36:10,112 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:36:10,296 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:36:10,301 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:36:10,306 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:36:10,934 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63000] due to args.save_total_limit\n",
            "{'loss': 2.6967, 'learning_rate': 4.927089644694839e-05, 'epoch': 0.29}\n",
            "  1% 64500/4423240 [1:41:22<83:12:26, 14.55it/s][INFO|trainer.py:1937] 2021-09-13 11:36:56,194 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:36:56,205 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:36:56,387 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:36:56,393 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:36:56,398 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:36:57,404 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-63500] due to args.save_total_limit\n",
            "{'loss': 2.6061, 'learning_rate': 4.926524448142086e-05, 'epoch': 0.29}\n",
            "  1% 65000/4423240 [1:42:10<118:06:53, 10.25it/s][INFO|trainer.py:1937] 2021-09-13 11:37:44,539 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:37:44,547 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:37:44,743 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:37:44,749 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:37:44,754 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:37:45,344 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64000] due to args.save_total_limit\n",
            "{'loss': 2.6345, 'learning_rate': 4.925959251589333e-05, 'epoch': 0.3}\n",
            "  1% 65500/4423240 [1:42:59<91:58:40, 13.16it/s][INFO|trainer.py:1937] 2021-09-13 11:38:33,058 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:38:33,064 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:38:33,275 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:38:33,281 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:38:33,286 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:38:33,900 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-64500] due to args.save_total_limit\n",
            "{'loss': 2.6513, 'learning_rate': 4.92539405503658e-05, 'epoch': 0.3}\n",
            "  1% 66000/4423240 [1:43:45<155:47:32,  7.77it/s][INFO|trainer.py:1937] 2021-09-13 11:39:19,564 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:39:19,584 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:39:19,779 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:39:19,785 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:39:19,790 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:39:20,401 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65000] due to args.save_total_limit\n",
            "{'loss': 2.7604, 'learning_rate': 4.9248288584838264e-05, 'epoch': 0.3}\n",
            "  2% 66500/4423240 [1:44:34<96:06:05, 12.59it/s][INFO|trainer.py:1937] 2021-09-13 11:40:07,813 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:40:07,821 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:40:08,012 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:40:08,018 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:40:08,026 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:40:08,635 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-65500] due to args.save_total_limit\n",
            "{'loss': 2.7043, 'learning_rate': 4.9242636619310735e-05, 'epoch': 0.3}\n",
            "  2% 67000/4423240 [1:45:21<101:33:17, 11.92it/s][INFO|trainer.py:1937] 2021-09-13 11:40:54,875 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:40:54,882 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:40:55,061 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:40:55,067 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:40:55,091 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:40:55,670 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66000] due to args.save_total_limit\n",
            "{'loss': 2.7034, 'learning_rate': 4.92369846537832e-05, 'epoch': 0.31}\n",
            "  2% 67500/4423240 [1:46:09<116:36:47, 10.38it/s][INFO|trainer.py:1937] 2021-09-13 11:41:43,045 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:41:43,055 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:41:43,222 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:41:43,229 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:41:43,233 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:41:44,236 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-66500] due to args.save_total_limit\n",
            "{'loss': 2.6566, 'learning_rate': 4.923133268825567e-05, 'epoch': 0.31}\n",
            "  2% 68000/4423240 [1:46:56<90:39:57, 13.34it/s][INFO|trainer.py:1937] 2021-09-13 11:42:30,800 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:42:30,807 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:42:30,989 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:42:30,996 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:42:31,001 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:42:31,598 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67000] due to args.save_total_limit\n",
            "{'loss': 2.6535, 'learning_rate': 4.922568072272814e-05, 'epoch': 0.31}\n",
            "  2% 68500/4423240 [1:47:45<95:57:50, 12.61it/s][INFO|trainer.py:1937] 2021-09-13 11:43:19,320 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:43:19,328 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:43:19,528 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:43:19,538 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:43:19,543 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:43:20,143 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-67500] due to args.save_total_limit\n",
            "{'loss': 2.5777, 'learning_rate': 4.922002875720061e-05, 'epoch': 0.31}\n",
            "  2% 69000/4423240 [1:48:30<151:44:18,  7.97it/s][INFO|trainer.py:1937] 2021-09-13 11:44:04,795 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:44:04,802 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:44:04,971 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:44:04,993 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:44:04,998 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:44:05,615 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68000] due to args.save_total_limit\n",
            "{'loss': 2.5745, 'learning_rate': 4.921437679167307e-05, 'epoch': 0.31}\n",
            "  2% 69500/4423240 [1:49:17<140:35:32,  8.60it/s][INFO|trainer.py:1937] 2021-09-13 11:44:51,444 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:44:51,451 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:44:51,629 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:44:51,636 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:44:51,641 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:44:52,224 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-68500] due to args.save_total_limit\n",
            "{'loss': 2.61, 'learning_rate': 4.920872482614554e-05, 'epoch': 0.32}\n",
            "  2% 70000/4423240 [1:50:04<144:22:37,  8.38it/s][INFO|trainer.py:1937] 2021-09-13 11:45:37,877 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:45:37,884 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:45:38,082 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:45:38,089 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:45:38,095 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:45:39,096 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69000] due to args.save_total_limit\n",
            "{'loss': 2.5738, 'learning_rate': 4.920307286061801e-05, 'epoch': 0.32}\n",
            "  2% 70500/4423240 [1:50:51<143:43:22,  8.41it/s][INFO|trainer.py:1937] 2021-09-13 11:46:25,308 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:46:25,315 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:46:25,491 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:46:25,496 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:46:25,502 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:46:26,068 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-69500] due to args.save_total_limit\n",
            "{'loss': 2.5167, 'learning_rate': 4.919742089509048e-05, 'epoch': 0.32}\n",
            "  2% 71000/4423240 [1:51:37<87:43:30, 13.78it/s][INFO|trainer.py:1937] 2021-09-13 11:47:11,168 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:47:11,174 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:47:11,351 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:47:11,361 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:47:11,366 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:47:11,933 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70000] due to args.save_total_limit\n",
            "{'loss': 2.6745, 'learning_rate': 4.919176892956295e-05, 'epoch': 0.32}\n",
            "  2% 71500/4423240 [1:52:25<107:55:39, 11.20it/s][INFO|trainer.py:1937] 2021-09-13 11:47:59,708 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:47:59,715 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:47:59,941 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:47:59,947 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:47:59,952 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:48:00,537 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-70500] due to args.save_total_limit\n",
            "{'loss': 2.5289, 'learning_rate': 4.9186116964035414e-05, 'epoch': 0.33}\n",
            "  2% 72000/4423240 [1:53:13<113:24:56, 10.66it/s][INFO|trainer.py:1937] 2021-09-13 11:48:46,917 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:48:46,925 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:48:47,112 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:48:47,132 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:48:47,144 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:48:47,747 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71000] due to args.save_total_limit\n",
            "{'loss': 2.5166, 'learning_rate': 4.9180464998507886e-05, 'epoch': 0.33}\n",
            "  2% 72500/4423240 [1:54:00<122:23:43,  9.87it/s][INFO|trainer.py:1937] 2021-09-13 11:49:34,539 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:49:34,546 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:49:34,729 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:49:34,738 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:49:34,743 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:49:35,326 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-71500] due to args.save_total_limit\n",
            "{'loss': 2.6682, 'learning_rate': 4.917481303298035e-05, 'epoch': 0.33}\n",
            "  2% 73000/4423240 [1:54:49<106:48:34, 11.31it/s][INFO|trainer.py:1937] 2021-09-13 11:50:23,018 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:50:23,026 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:50:23,228 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:50:23,236 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:50:23,242 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:50:24,298 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72000] due to args.save_total_limit\n",
            "{'loss': 2.5333, 'learning_rate': 4.9169161067452815e-05, 'epoch': 0.33}\n",
            "  2% 73500/4423240 [1:55:36<136:02:25,  8.88it/s][INFO|trainer.py:1937] 2021-09-13 11:51:10,497 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:51:10,504 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:51:10,678 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:51:10,684 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:51:10,689 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:51:11,269 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-72500] due to args.save_total_limit\n",
            "{'loss': 2.5167, 'learning_rate': 4.9163509101925286e-05, 'epoch': 0.33}\n",
            "  2% 74000/4423240 [1:56:25<109:17:19, 11.05it/s][INFO|trainer.py:1937] 2021-09-13 11:51:58,925 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:51:58,932 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:51:59,118 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:51:59,126 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:51:59,131 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:51:59,677 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73000] due to args.save_total_limit\n",
            "{'loss': 2.56, 'learning_rate': 4.915785713639776e-05, 'epoch': 0.34}\n",
            "  2% 74500/4423240 [1:57:12<108:08:05, 11.17it/s][INFO|trainer.py:1937] 2021-09-13 11:52:46,395 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:52:46,416 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:52:46,595 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:52:46,604 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:52:46,609 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:52:47,228 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-73500] due to args.save_total_limit\n",
            "{'loss': 2.5485, 'learning_rate': 4.915220517087022e-05, 'epoch': 0.34}\n",
            "  2% 75000/4423240 [1:57:59<79:41:46, 15.16it/s][INFO|trainer.py:1937] 2021-09-13 11:53:33,276 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:53:33,283 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:53:33,458 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:53:33,465 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:53:33,471 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:53:34,079 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74000] due to args.save_total_limit\n",
            "{'loss': 2.6572, 'learning_rate': 4.9146553205342694e-05, 'epoch': 0.34}\n",
            "  2% 75500/4423240 [1:58:49<107:52:21, 11.20it/s][INFO|trainer.py:1937] 2021-09-13 11:54:23,228 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:54:23,234 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:54:23,410 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:54:23,416 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:54:23,421 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:54:24,031 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-74500] due to args.save_total_limit\n",
            "{'loss': 2.5137, 'learning_rate': 4.9140901239815165e-05, 'epoch': 0.34}\n",
            "  2% 76000/4423240 [1:59:38<114:54:24, 10.51it/s][INFO|trainer.py:1937] 2021-09-13 11:55:12,478 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:55:12,485 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:55:12,688 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:55:12,695 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:55:12,701 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:55:13,711 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75000] due to args.save_total_limit\n",
            "{'loss': 2.5127, 'learning_rate': 4.913524927428763e-05, 'epoch': 0.35}\n",
            "  2% 76500/4423240 [2:00:24<110:51:25, 10.89it/s][INFO|trainer.py:1937] 2021-09-13 11:55:58,747 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:55:58,754 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:55:58,924 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:55:58,930 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:55:58,936 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:55:59,542 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-75500] due to args.save_total_limit\n",
            "{'loss': 2.5905, 'learning_rate': 4.9129597308760094e-05, 'epoch': 0.35}\n",
            "  2% 77000/4423240 [2:01:12<111:28:22, 10.83it/s][INFO|trainer.py:1937] 2021-09-13 11:56:45,871 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:56:45,879 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:56:46,056 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:56:46,063 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:56:46,069 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:56:46,696 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76000] due to args.save_total_limit\n",
            "{'loss': 2.4577, 'learning_rate': 4.9123945343232565e-05, 'epoch': 0.35}\n",
            "  2% 77500/4423240 [2:01:56<103:34:22, 11.66it/s][INFO|trainer.py:1937] 2021-09-13 11:57:30,082 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:57:30,113 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:57:30,299 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:57:30,306 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:57:30,312 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:57:30,936 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-76500] due to args.save_total_limit\n",
            "{'loss': 2.653, 'learning_rate': 4.911829337770503e-05, 'epoch': 0.35}\n",
            "  2% 78000/4423240 [2:02:43<115:29:38, 10.45it/s][INFO|trainer.py:1937] 2021-09-13 11:58:17,510 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:58:17,517 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:58:17,716 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:58:17,724 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:58:17,730 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:58:18,372 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77000] due to args.save_total_limit\n",
            "{'loss': 2.5524, 'learning_rate': 4.91126414121775e-05, 'epoch': 0.35}\n",
            "  2% 78500/4423240 [2:03:32<93:00:18, 12.98it/s][INFO|trainer.py:1937] 2021-09-13 11:59:06,360 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:59:06,368 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:59:06,564 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:59:06,570 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:59:06,575 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:59:07,229 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-77500] due to args.save_total_limit\n",
            "{'loss': 2.4604, 'learning_rate': 4.910698944664997e-05, 'epoch': 0.36}\n",
            "  2% 79000/4423240 [2:04:19<108:42:42, 11.10it/s][INFO|trainer.py:1937] 2021-09-13 11:59:53,012 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 11:59:53,020 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 11:59:53,224 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 11:59:53,230 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 11:59:53,236 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 11:59:54,130 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78000] due to args.save_total_limit\n",
            "{'loss': 2.5845, 'learning_rate': 4.910133748112244e-05, 'epoch': 0.36}\n",
            "  2% 79500/4423240 [2:05:07<174:51:08,  6.90it/s][INFO|trainer.py:1937] 2021-09-13 12:00:41,571 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:00:41,579 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:00:41,786 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:00:41,793 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:00:41,798 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:00:42,377 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-78500] due to args.save_total_limit\n",
            "{'loss': 2.4335, 'learning_rate': 4.909568551559491e-05, 'epoch': 0.36}\n",
            "  2% 80000/4423240 [2:05:52<100:13:10, 12.04it/s][INFO|trainer.py:1937] 2021-09-13 12:01:26,792 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:01:26,798 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:01:26,978 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:01:26,986 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:01:26,991 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:01:27,566 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79000] due to args.save_total_limit\n",
            "{'loss': 2.4403, 'learning_rate': 4.909003355006737e-05, 'epoch': 0.36}\n",
            "  2% 80500/4423240 [2:06:39<112:27:40, 10.73it/s][INFO|trainer.py:1937] 2021-09-13 12:02:13,719 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:02:13,727 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:02:13,933 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:02:14,090 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:02:14,096 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:02:14,655 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-79500] due to args.save_total_limit\n",
            "{'loss': 2.5706, 'learning_rate': 4.908438158453984e-05, 'epoch': 0.37}\n",
            "  2% 81000/4423240 [2:07:28<92:41:52, 13.01it/s][INFO|trainer.py:1937] 2021-09-13 12:03:02,689 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:03:02,704 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:03:02,886 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:03:02,892 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:03:02,897 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:03:03,511 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80000] due to args.save_total_limit\n",
            "{'loss': 2.3934, 'learning_rate': 4.907872961901231e-05, 'epoch': 0.37}\n",
            "  2% 81500/4423240 [2:08:14<154:37:04,  7.80it/s][INFO|trainer.py:1937] 2021-09-13 12:03:48,136 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:03:48,143 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:03:48,332 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:03:48,340 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:03:48,347 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:03:49,326 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-80500] due to args.save_total_limit\n",
            "{'loss': 2.4594, 'learning_rate': 4.907307765348478e-05, 'epoch': 0.37}\n",
            "  2% 82000/4423240 [2:09:01<128:14:03,  9.40it/s][INFO|trainer.py:1937] 2021-09-13 12:04:35,202 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:04:35,209 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:04:35,407 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:04:35,417 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:04:35,423 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:04:35,998 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81000] due to args.save_total_limit\n",
            "{'loss': 2.4735, 'learning_rate': 4.9067425687957245e-05, 'epoch': 0.37}\n",
            "  2% 82500/4423240 [2:09:50<189:03:01,  6.38it/s][INFO|trainer.py:1937] 2021-09-13 12:05:23,902 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:05:23,909 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:05:24,105 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:05:24,112 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:05:24,117 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:05:24,722 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-81500] due to args.save_total_limit\n",
            "{'loss': 2.3837, 'learning_rate': 4.9061773722429716e-05, 'epoch': 0.38}\n",
            "  2% 83000/4423240 [2:10:35<86:16:32, 13.97it/s][INFO|trainer.py:1937] 2021-09-13 12:06:09,787 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:06:09,794 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:06:09,986 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:06:09,994 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:06:10,000 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:06:10,600 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82000] due to args.save_total_limit\n",
            "{'loss': 2.5344, 'learning_rate': 4.905612175690219e-05, 'epoch': 0.38}\n",
            "  2% 83500/4423240 [2:11:24<97:30:41, 12.36it/s][INFO|trainer.py:1937] 2021-09-13 12:06:57,890 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:06:57,897 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:06:58,077 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:06:58,101 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:06:58,107 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:06:58,705 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-82500] due to args.save_total_limit\n",
            "{'loss': 2.4323, 'learning_rate': 4.9050469791374645e-05, 'epoch': 0.38}\n",
            "  2% 84000/4423240 [2:12:13<100:28:06, 12.00it/s][INFO|trainer.py:1937] 2021-09-13 12:07:46,950 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:07:46,957 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:07:47,142 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:07:47,150 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:07:47,155 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:07:47,732 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83000] due to args.save_total_limit\n",
            "{'loss': 2.5054, 'learning_rate': 4.904481782584712e-05, 'epoch': 0.38}\n",
            "  2% 84500/4423240 [2:13:00<130:36:57,  9.23it/s][INFO|trainer.py:1937] 2021-09-13 12:08:34,455 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:08:34,463 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:08:34,647 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:08:34,653 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:08:34,658 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:08:35,651 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-83500] due to args.save_total_limit\n",
            "{'loss': 2.4594, 'learning_rate': 4.903916586031959e-05, 'epoch': 0.38}\n",
            "  2% 85000/4423240 [2:13:47<110:49:03, 10.87it/s][INFO|trainer.py:1937] 2021-09-13 12:09:21,368 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:09:21,374 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:09:21,560 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:09:21,566 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:09:21,571 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:09:22,160 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84000] due to args.save_total_limit\n",
            "{'loss': 2.3772, 'learning_rate': 4.903351389479205e-05, 'epoch': 0.39}\n",
            "  2% 85500/4423240 [2:14:32<94:37:23, 12.73it/s][INFO|trainer.py:1937] 2021-09-13 12:10:06,504 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:10:06,513 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:10:06,694 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:10:06,703 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:10:06,724 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:10:07,333 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-84500] due to args.save_total_limit\n",
            "{'loss': 2.4731, 'learning_rate': 4.9027861929264524e-05, 'epoch': 0.39}\n",
            "  2% 86000/4423240 [2:15:19<122:25:58,  9.84it/s][INFO|trainer.py:1937] 2021-09-13 12:10:53,410 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:10:53,434 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:10:53,636 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:10:53,644 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:10:53,649 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:10:54,237 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85000] due to args.save_total_limit\n",
            "{'loss': 2.4384, 'learning_rate': 4.9022209963736995e-05, 'epoch': 0.39}\n",
            "  2% 86500/4423240 [2:16:08<110:49:51, 10.87it/s][INFO|trainer.py:1937] 2021-09-13 12:11:41,821 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:11:41,837 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:11:42,004 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:11:42,010 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:11:42,031 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:11:42,637 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-85500] due to args.save_total_limit\n",
            "{'loss': 2.4875, 'learning_rate': 4.901655799820946e-05, 'epoch': 0.39}\n",
            "  2% 87000/4423240 [2:16:56<117:38:22, 10.24it/s][INFO|trainer.py:1937] 2021-09-13 12:12:29,851 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:12:29,858 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:12:30,028 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:12:30,034 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:12:30,039 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:12:30,670 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86000] due to args.save_total_limit\n",
            "{'loss': 2.3791, 'learning_rate': 4.901090603268193e-05, 'epoch': 0.4}\n",
            "  2% 87500/4423240 [2:17:42<116:49:21, 10.31it/s][INFO|trainer.py:1937] 2021-09-13 12:13:16,430 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:13:16,438 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:13:16,617 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:13:16,622 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:13:16,628 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:13:17,363 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-86500] due to args.save_total_limit\n",
            "{'loss': 2.5069, 'learning_rate': 4.9005254067154396e-05, 'epoch': 0.4}\n",
            "  2% 88000/4423240 [2:18:30<106:42:11, 11.29it/s][INFO|trainer.py:1937] 2021-09-13 12:14:04,364 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:14:04,371 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:14:04,552 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:14:04,558 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:14:04,581 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:14:05,201 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87000] due to args.save_total_limit\n",
            "{'loss': 2.4046, 'learning_rate': 4.899960210162686e-05, 'epoch': 0.4}\n",
            "  2% 88500/4423240 [2:19:17<99:06:05, 12.15it/s][INFO|trainer.py:1937] 2021-09-13 12:14:51,073 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:14:51,080 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:14:51,251 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:14:51,257 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:14:51,264 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:14:51,885 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-87500] due to args.save_total_limit\n",
            "{'loss': 2.432, 'learning_rate': 4.899395013609933e-05, 'epoch': 0.4}\n",
            "  2% 89000/4423240 [2:20:06<111:09:52, 10.83it/s][INFO|trainer.py:1937] 2021-09-13 12:15:40,738 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:15:40,745 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:15:40,951 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:15:40,964 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:15:40,970 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:15:41,541 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88000] due to args.save_total_limit\n",
            "{'loss': 2.522, 'learning_rate': 4.89882981705718e-05, 'epoch': 0.4}\n",
            "  2% 89500/4423240 [2:20:55<89:50:19, 13.40it/s][INFO|trainer.py:1937] 2021-09-13 12:16:28,974 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:16:28,980 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:16:29,179 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:16:29,185 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:16:29,190 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:16:29,810 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-88500] due to args.save_total_limit\n",
            "{'loss': 2.4758, 'learning_rate': 4.898264620504427e-05, 'epoch': 0.41}\n",
            "  2% 90000/4423240 [2:21:42<95:36:57, 12.59it/s][INFO|trainer.py:1937] 2021-09-13 12:17:16,408 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:17:16,415 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:17:16,603 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:17:16,609 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:17:16,614 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:17:17,670 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89000] due to args.save_total_limit\n",
            "{'loss': 2.4811, 'learning_rate': 4.897699423951674e-05, 'epoch': 0.41}\n",
            "  2% 90500/4423240 [2:22:32<140:10:27,  8.59it/s][INFO|trainer.py:1937] 2021-09-13 12:18:06,487 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:18:06,494 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:18:06,672 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:18:06,679 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:18:06,684 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:18:07,321 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-89500] due to args.save_total_limit\n",
            "{'loss': 2.4766, 'learning_rate': 4.897134227398921e-05, 'epoch': 0.41}\n",
            "  2% 91000/4423240 [2:23:18<102:18:40, 11.76it/s][INFO|trainer.py:1937] 2021-09-13 12:18:52,171 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:18:52,178 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:18:52,357 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:18:52,363 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:18:52,368 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:18:52,972 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90000] due to args.save_total_limit\n",
            "{'loss': 2.3659, 'learning_rate': 4.896569030846167e-05, 'epoch': 0.41}\n",
            "  2% 91500/4423240 [2:24:07<110:00:09, 10.94it/s][INFO|trainer.py:1937] 2021-09-13 12:19:40,907 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:19:40,915 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:19:41,094 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:19:41,103 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:19:41,111 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:19:41,729 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-90500] due to args.save_total_limit\n",
            "{'loss': 2.3961, 'learning_rate': 4.896003834293414e-05, 'epoch': 0.42}\n",
            "  2% 92000/4423240 [2:24:53<105:02:15, 11.45it/s][INFO|trainer.py:1937] 2021-09-13 12:20:27,071 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:20:27,078 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:20:27,263 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:20:27,269 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:20:27,292 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:20:27,855 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91000] due to args.save_total_limit\n",
            "{'loss': 2.395, 'learning_rate': 4.895438637740661e-05, 'epoch': 0.42}\n",
            "  2% 92500/4423240 [2:25:40<93:11:53, 12.91it/s][INFO|trainer.py:1937] 2021-09-13 12:21:14,345 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:21:14,352 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:21:14,543 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:21:14,547 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:21:14,552 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:21:15,167 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-91500] due to args.save_total_limit\n",
            "{'loss': 2.3829, 'learning_rate': 4.8948734411879075e-05, 'epoch': 0.42}\n",
            "  2% 93000/4423240 [2:26:27<129:25:02,  9.29it/s][INFO|trainer.py:1937] 2021-09-13 12:22:01,617 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:22:01,625 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:22:01,795 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:22:01,801 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:22:01,805 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:22:02,363 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92000] due to args.save_total_limit\n",
            "{'loss': 2.4003, 'learning_rate': 4.8943082446351547e-05, 'epoch': 0.42}\n",
            "  2% 93500/4423240 [2:27:14<132:52:45,  9.05it/s][INFO|trainer.py:1937] 2021-09-13 12:22:48,087 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:22:48,094 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:22:48,275 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:22:48,286 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:22:48,291 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:22:48,914 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-92500] due to args.save_total_limit\n",
            "{'loss': 2.3351, 'learning_rate': 4.893743048082402e-05, 'epoch': 0.43}\n",
            "  2% 94000/4423240 [2:28:01<126:02:28,  9.54it/s][INFO|trainer.py:1937] 2021-09-13 12:23:35,318 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:23:35,326 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:23:35,523 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:23:35,529 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:23:35,537 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:23:36,149 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93000] due to args.save_total_limit\n",
            "{'loss': 2.4385, 'learning_rate': 4.893177851529648e-05, 'epoch': 0.43}\n",
            "  2% 94500/4423240 [2:28:50<142:10:39,  8.46it/s][INFO|trainer.py:1937] 2021-09-13 12:24:24,375 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:24:24,395 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:24:24,572 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:24:24,597 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:24:24,602 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:24:25,215 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-93500] due to args.save_total_limit\n",
            "{'loss': 2.2886, 'learning_rate': 4.8926126549768954e-05, 'epoch': 0.43}\n",
            "  2% 95000/4423240 [2:29:35<88:51:10, 13.53it/s][INFO|trainer.py:1937] 2021-09-13 12:25:09,609 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:25:09,615 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:25:09,810 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:25:09,817 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:25:09,823 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:25:10,452 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94000] due to args.save_total_limit\n",
            "{'loss': 2.3313, 'learning_rate': 4.892047458424142e-05, 'epoch': 0.43}\n",
            "  2% 95500/4423240 [2:30:23<102:40:28, 11.71it/s][INFO|trainer.py:1937] 2021-09-13 12:25:56,994 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:25:57,001 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:25:57,196 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:25:57,207 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:25:57,217 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:25:57,859 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-94500] due to args.save_total_limit\n",
            "{'loss': 2.2934, 'learning_rate': 4.891482261871388e-05, 'epoch': 0.43}\n",
            "  2% 96000/4423240 [2:31:10<143:42:35,  8.36it/s][INFO|trainer.py:1937] 2021-09-13 12:26:44,412 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:26:44,420 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:26:44,619 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:26:44,630 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:26:44,644 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:26:45,605 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95000] due to args.save_total_limit\n",
            "{'loss': 2.3674, 'learning_rate': 4.8909170653186354e-05, 'epoch': 0.44}\n",
            "  2% 96500/4423240 [2:31:59<99:29:06, 12.08it/s][INFO|trainer.py:1937] 2021-09-13 12:27:33,547 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:27:33,555 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:27:33,735 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:27:33,744 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:27:33,752 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:27:34,371 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-95500] due to args.save_total_limit\n",
            "{'loss': 2.4196, 'learning_rate': 4.8903518687658826e-05, 'epoch': 0.44}\n",
            "  2% 97000/4423240 [2:32:47<87:01:01, 13.81it/s][INFO|trainer.py:1937] 2021-09-13 12:28:21,548 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:28:21,556 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:28:21,763 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:28:21,770 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:28:21,775 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:28:22,382 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96000] due to args.save_total_limit\n",
            "{'loss': 2.3299, 'learning_rate': 4.889786672213129e-05, 'epoch': 0.44}\n",
            "  2% 97500/4423240 [2:33:34<90:25:03, 13.29it/s][INFO|trainer.py:1937] 2021-09-13 12:29:08,278 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:29:08,286 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:29:08,488 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:29:08,494 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:29:08,499 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:29:09,120 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-96500] due to args.save_total_limit\n",
            "{'loss': 2.3584, 'learning_rate': 4.889221475660376e-05, 'epoch': 0.44}\n",
            "  2% 98000/4423240 [2:34:19<94:23:06, 12.73it/s][INFO|trainer.py:1937] 2021-09-13 12:29:53,573 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:29:53,581 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:29:53,803 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:29:53,809 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:29:53,814 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:29:54,385 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97000] due to args.save_total_limit\n",
            "{'loss': 2.3013, 'learning_rate': 4.8886562791076226e-05, 'epoch': 0.45}\n",
            "  2% 98500/4423240 [2:35:08<117:40:58, 10.21it/s][INFO|trainer.py:1937] 2021-09-13 12:30:41,853 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:30:41,861 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:30:42,029 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:30:42,035 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:30:42,040 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:30:42,641 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-97500] due to args.save_total_limit\n",
            "{'loss': 2.3981, 'learning_rate': 4.888091082554869e-05, 'epoch': 0.45}\n",
            "  2% 99000/4423240 [2:35:54<86:54:45, 13.82it/s][INFO|trainer.py:1937] 2021-09-13 12:31:28,675 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:31:28,683 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:31:28,866 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:31:28,888 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:31:28,893 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:31:29,887 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98000] due to args.save_total_limit\n",
            "{'loss': 2.3107, 'learning_rate': 4.887525886002116e-05, 'epoch': 0.45}\n",
            "  2% 99500/4423240 [2:36:41<123:09:50,  9.75it/s][INFO|trainer.py:1937] 2021-09-13 12:32:15,471 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:32:15,478 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:32:15,676 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:32:15,683 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:32:15,690 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:32:16,256 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-98500] due to args.save_total_limit\n",
            "{'loss': 2.3323, 'learning_rate': 4.886960689449363e-05, 'epoch': 0.45}\n",
            "  2% 100000/4423240 [2:37:29<140:00:26,  8.58it/s][INFO|trainer.py:1937] 2021-09-13 12:33:03,258 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:33:03,265 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:33:03,452 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:33:03,457 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:33:03,462 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:33:04,044 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99000] due to args.save_total_limit\n",
            "{'loss': 2.3235, 'learning_rate': 4.88639549289661e-05, 'epoch': 0.45}\n",
            "  2% 100500/4423240 [2:38:17<111:24:13, 10.78it/s][INFO|trainer.py:1937] 2021-09-13 12:33:51,306 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:33:51,313 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:33:51,496 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:33:51,517 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:33:51,526 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:33:52,128 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-99500] due to args.save_total_limit\n",
            "{'loss': 2.3167, 'learning_rate': 4.885830296343857e-05, 'epoch': 0.46}\n",
            "  2% 101000/4423240 [2:39:03<96:34:25, 12.43it/s][INFO|trainer.py:1937] 2021-09-13 12:34:36,982 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:34:36,988 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:34:37,163 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:34:37,169 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:34:37,174 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:34:37,812 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100000] due to args.save_total_limit\n",
            "{'loss': 2.4386, 'learning_rate': 4.8852650997911034e-05, 'epoch': 0.46}\n",
            "  2% 101500/4423240 [2:39:53<103:58:44, 11.55it/s][INFO|trainer.py:1937] 2021-09-13 12:35:27,084 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:35:27,092 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:35:27,266 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:35:27,272 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:35:27,276 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:35:28,286 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-100500] due to args.save_total_limit\n",
            "{'loss': 2.3409, 'learning_rate': 4.8846999032383505e-05, 'epoch': 0.46}\n",
            "  2% 102000/4423240 [2:40:39<103:06:55, 11.64it/s][INFO|trainer.py:1937] 2021-09-13 12:36:13,454 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:36:13,463 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:36:13,642 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:36:13,648 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:36:13,654 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:36:14,273 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101000] due to args.save_total_limit\n",
            "{'loss': 2.4574, 'learning_rate': 4.8841347066855976e-05, 'epoch': 0.46}\n",
            "  2% 102500/4423240 [2:41:30<104:59:21, 11.43it/s][INFO|trainer.py:1937] 2021-09-13 12:37:04,059 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:37:04,068 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:37:04,238 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:37:04,255 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:37:04,260 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:37:04,880 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-101500] due to args.save_total_limit\n",
            "{'loss': 2.2786, 'learning_rate': 4.883569510132844e-05, 'epoch': 0.47}\n",
            "  2% 103000/4423240 [2:42:17<88:53:10, 13.50it/s][INFO|trainer.py:1937] 2021-09-13 12:37:50,840 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:37:50,850 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:37:51,036 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:37:51,042 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:37:51,047 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:37:51,632 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102000] due to args.save_total_limit\n",
            "{'loss': 2.2955, 'learning_rate': 4.8830043135800906e-05, 'epoch': 0.47}\n",
            "  2% 103500/4423240 [2:43:03<198:16:09,  6.05it/s][INFO|trainer.py:1937] 2021-09-13 12:38:37,441 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:38:37,448 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:38:37,624 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:38:37,630 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:38:37,635 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:38:38,245 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-102500] due to args.save_total_limit\n",
            "{'loss': 2.325, 'learning_rate': 4.882439117027338e-05, 'epoch': 0.47}\n",
            "  2% 104000/4423240 [2:43:51<108:52:05, 11.02it/s][INFO|trainer.py:1937] 2021-09-13 12:39:25,256 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:39:25,263 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:39:25,450 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:39:25,456 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:39:25,468 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:39:26,096 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103000] due to args.save_total_limit\n",
            "{'loss': 2.3976, 'learning_rate': 4.881873920474584e-05, 'epoch': 0.47}\n",
            "  2% 104500/4423240 [2:44:38<138:46:35,  8.64it/s][INFO|trainer.py:1937] 2021-09-13 12:40:12,746 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:40:12,772 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:40:12,960 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:40:12,966 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:40:12,972 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:40:13,592 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-103500] due to args.save_total_limit\n",
            "{'loss': 2.2224, 'learning_rate': 4.881308723921831e-05, 'epoch': 0.47}\n",
            "  2% 105000/4423240 [2:45:24<113:28:07, 10.57it/s][INFO|trainer.py:1937] 2021-09-13 12:40:58,315 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:40:58,323 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:40:58,494 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:40:58,501 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:40:58,505 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:40:59,125 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104000] due to args.save_total_limit\n",
            "{'loss': 2.3757, 'learning_rate': 4.8807435273690784e-05, 'epoch': 0.48}\n",
            "  2% 105500/4423240 [2:46:15<201:42:57,  5.95it/s][INFO|trainer.py:1937] 2021-09-13 12:41:49,475 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:41:49,481 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:41:49,668 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:41:49,678 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:41:49,683 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:41:50,291 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-104500] due to args.save_total_limit\n",
            "{'loss': 2.2962, 'learning_rate': 4.880178330816325e-05, 'epoch': 0.48}\n",
            "  2% 106000/4423240 [2:47:02<84:07:02, 14.26it/s][INFO|trainer.py:1937] 2021-09-13 12:42:36,186 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:42:36,207 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:42:36,395 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:42:36,401 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:42:36,406 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:42:36,973 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105000] due to args.save_total_limit\n",
            "{'loss': 2.3318, 'learning_rate': 4.879613134263571e-05, 'epoch': 0.48}\n",
            "  2% 106500/4423240 [2:47:47<86:42:42, 13.83it/s][INFO|trainer.py:1937] 2021-09-13 12:43:21,416 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:43:21,423 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:43:21,593 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:43:21,599 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:43:21,604 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:43:22,229 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-105500] due to args.save_total_limit\n",
            "{'loss': 2.2688, 'learning_rate': 4.8790479377108185e-05, 'epoch': 0.48}\n",
            "  2% 107000/4423240 [2:48:34<108:16:01, 11.07it/s][INFO|trainer.py:1937] 2021-09-13 12:44:07,899 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:44:07,905 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:44:08,080 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:44:08,086 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:44:08,091 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:44:08,748 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106000] due to args.save_total_limit\n",
            "{'loss': 2.2892, 'learning_rate': 4.878482741158065e-05, 'epoch': 0.49}\n",
            "  2% 107500/4423240 [2:49:22<100:52:17, 11.88it/s][INFO|trainer.py:1937] 2021-09-13 12:44:56,756 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:44:56,763 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:44:56,974 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:44:56,982 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:44:56,987 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:44:57,987 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-106500] due to args.save_total_limit\n",
            "{'loss': 2.3283, 'learning_rate': 4.877917544605312e-05, 'epoch': 0.49}\n",
            "  2% 108000/4423240 [2:50:08<97:58:05, 12.24it/s][INFO|trainer.py:1937] 2021-09-13 12:45:42,182 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:45:42,188 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:45:42,365 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:45:42,371 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:45:42,377 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:45:42,972 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107000] due to args.save_total_limit\n",
            "{'loss': 2.2053, 'learning_rate': 4.877352348052559e-05, 'epoch': 0.49}\n",
            "  2% 108500/4423240 [2:50:56<129:10:12,  9.28it/s][INFO|trainer.py:1937] 2021-09-13 12:46:30,406 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:46:30,413 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:46:30,598 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:46:30,604 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:46:30,609 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:46:31,219 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-107500] due to args.save_total_limit\n",
            "{'loss': 2.2698, 'learning_rate': 4.8767871514998056e-05, 'epoch': 0.49}\n",
            "  2% 109000/4423240 [2:51:44<95:47:12, 12.51it/s][INFO|trainer.py:1937] 2021-09-13 12:47:18,558 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:47:18,566 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:47:18,754 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:47:18,794 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:47:18,801 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:47:19,407 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108000] due to args.save_total_limit\n",
            "{'loss': 2.3137, 'learning_rate': 4.876221954947053e-05, 'epoch': 0.5}\n",
            "  2% 109500/4423240 [2:52:30<110:04:40, 10.89it/s][INFO|trainer.py:1937] 2021-09-13 12:48:04,712 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:48:04,720 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:48:04,906 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:48:04,912 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:48:04,917 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:48:05,564 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-108500] due to args.save_total_limit\n",
            "{'loss': 2.3565, 'learning_rate': 4.8756567583943e-05, 'epoch': 0.5}\n",
            "  2% 110000/4423240 [2:53:18<117:06:47, 10.23it/s][INFO|trainer.py:1937] 2021-09-13 12:48:52,169 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:48:52,175 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:48:52,374 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:48:52,380 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:48:52,386 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:48:53,354 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109000] due to args.save_total_limit\n",
            "{'loss': 2.362, 'learning_rate': 4.8750915618415464e-05, 'epoch': 0.5}\n",
            "  2% 110500/4423240 [2:54:07<119:19:20, 10.04it/s][INFO|trainer.py:1937] 2021-09-13 12:49:40,948 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:49:40,958 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:49:41,146 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:49:41,152 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:49:41,157 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:49:42,176 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-109500] due to args.save_total_limit\n",
            "{'loss': 2.1187, 'learning_rate': 4.874526365288793e-05, 'epoch': 0.5}\n",
            "  3% 111000/4423240 [2:54:52<101:07:02, 11.85it/s][INFO|trainer.py:1937] 2021-09-13 12:50:26,245 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:50:26,252 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:50:26,446 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:50:26,452 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:50:26,458 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:50:27,036 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110000] due to args.save_total_limit\n",
            "{'loss': 2.2174, 'learning_rate': 4.87396116873604e-05, 'epoch': 0.5}\n",
            "  3% 111500/4423240 [2:55:38<91:07:45, 13.14it/s][INFO|trainer.py:1937] 2021-09-13 12:51:12,250 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:51:12,257 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:51:12,449 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:51:12,455 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:51:12,460 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:51:13,050 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-110500] due to args.save_total_limit\n",
            "{'loss': 2.3195, 'learning_rate': 4.8733959721832864e-05, 'epoch': 0.51}\n",
            "  3% 112000/4423240 [2:56:26<106:59:49, 11.19it/s][INFO|trainer.py:1937] 2021-09-13 12:52:00,579 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:52:00,586 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:52:00,765 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:52:00,788 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:52:00,793 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:52:01,805 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111000] due to args.save_total_limit\n",
            "{'loss': 2.219, 'learning_rate': 4.8728307756305335e-05, 'epoch': 0.51}\n",
            "  3% 112500/4423240 [2:57:13<152:22:06,  7.86it/s][INFO|trainer.py:1937] 2021-09-13 12:52:46,907 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:52:46,914 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:52:47,115 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:52:47,122 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:52:47,128 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:52:47,757 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-111500] due to args.save_total_limit\n",
            "{'loss': 2.3075, 'learning_rate': 4.872265579077781e-05, 'epoch': 0.51}\n",
            "  3% 113000/4423240 [2:58:01<95:48:39, 12.50it/s][INFO|trainer.py:1937] 2021-09-13 12:53:35,389 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:53:35,399 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:53:35,597 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:53:35,603 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:53:35,609 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:53:36,589 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112000] due to args.save_total_limit\n",
            "{'loss': 2.1758, 'learning_rate': 4.871700382525027e-05, 'epoch': 0.51}\n",
            "  3% 113500/4423240 [2:58:48<98:14:01, 12.19it/s][INFO|trainer.py:1937] 2021-09-13 12:54:22,114 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:54:22,122 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:54:22,321 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:54:22,327 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:54:22,332 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:54:22,914 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-112500] due to args.save_total_limit\n",
            "{'loss': 2.2581, 'learning_rate': 4.8711351859722736e-05, 'epoch': 0.52}\n",
            "  3% 114000/4423240 [2:59:36<114:56:09, 10.41it/s][INFO|trainer.py:1937] 2021-09-13 12:55:10,268 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:55:10,290 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:55:10,492 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:55:10,500 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:55:10,506 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:55:11,144 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113000] due to args.save_total_limit\n",
            "{'loss': 2.2751, 'learning_rate': 4.870569989419521e-05, 'epoch': 0.52}\n",
            "  3% 114500/4423240 [3:00:22<98:32:52, 12.15it/s][INFO|trainer.py:1937] 2021-09-13 12:55:55,819 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:55:55,826 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:55:56,034 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:55:56,039 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:55:56,044 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:55:56,658 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-113500] due to args.save_total_limit\n",
            "{'loss': 2.2662, 'learning_rate': 4.870004792866767e-05, 'epoch': 0.52}\n",
            "  3% 115000/4423240 [3:01:11<97:42:55, 12.25it/s][INFO|trainer.py:1937] 2021-09-13 12:56:45,760 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:56:45,767 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:56:45,963 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:56:45,969 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:56:45,974 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:56:46,601 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114000] due to args.save_total_limit\n",
            "{'loss': 2.2716, 'learning_rate': 4.869439596314014e-05, 'epoch': 0.52}\n",
            "  3% 115500/4423240 [3:01:59<130:23:44,  9.18it/s][INFO|trainer.py:1937] 2021-09-13 12:57:32,836 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:57:32,844 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:57:33,036 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:57:33,042 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:57:33,047 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:57:33,720 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-114500] due to args.save_total_limit\n",
            "{'loss': 2.2328, 'learning_rate': 4.8688743997612615e-05, 'epoch': 0.52}\n",
            "  3% 116000/4423240 [3:02:44<118:26:29, 10.10it/s][INFO|trainer.py:1937] 2021-09-13 12:58:18,768 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:58:18,775 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:58:18,962 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:58:18,968 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:58:18,972 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:58:19,965 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115000] due to args.save_total_limit\n",
            "{'loss': 2.2375, 'learning_rate': 4.868309203208508e-05, 'epoch': 0.53}\n",
            "  3% 116500/4423240 [3:03:33<87:25:46, 13.68it/s][INFO|trainer.py:1937] 2021-09-13 12:59:06,846 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:59:06,852 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:59:07,024 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:59:07,030 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:59:07,035 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:59:07,635 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-115500] due to args.save_total_limit\n",
            "{'loss': 2.226, 'learning_rate': 4.867744006655755e-05, 'epoch': 0.53}\n",
            "  3% 117000/4423240 [3:04:18<134:40:25,  8.88it/s][INFO|trainer.py:1937] 2021-09-13 12:59:52,172 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 12:59:52,178 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 12:59:52,361 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 12:59:52,367 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 12:59:52,372 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 12:59:52,940 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116000] due to args.save_total_limit\n",
            "{'loss': 2.174, 'learning_rate': 4.867178810103002e-05, 'epoch': 0.53}\n",
            "  3% 117500/4423240 [3:05:04<105:08:15, 11.38it/s][INFO|trainer.py:1937] 2021-09-13 13:00:38,643 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:00:38,664 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:00:38,843 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:00:38,857 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:00:38,871 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:00:39,437 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-116500] due to args.save_total_limit\n",
            "{'loss': 2.3487, 'learning_rate': 4.866613613550248e-05, 'epoch': 0.53}\n",
            "  3% 118000/4423240 [3:05:52<91:17:20, 13.10it/s][INFO|trainer.py:1937] 2021-09-13 13:01:26,331 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:01:26,342 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:01:26,554 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:01:26,561 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:01:26,583 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:01:27,202 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117000] due to args.save_total_limit\n",
            "{'loss': 2.1003, 'learning_rate': 4.866048416997495e-05, 'epoch': 0.54}\n",
            "  3% 118500/4423240 [3:06:39<82:01:16, 14.58it/s][INFO|trainer.py:1937] 2021-09-13 13:02:13,685 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:02:13,693 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:02:13,873 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:02:13,879 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:02:13,884 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:02:14,513 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-117500] due to args.save_total_limit\n",
            "{'loss': 2.1383, 'learning_rate': 4.865483220444742e-05, 'epoch': 0.54}\n",
            "  3% 119000/4423240 [3:07:25<101:05:39, 11.83it/s][INFO|trainer.py:1937] 2021-09-13 13:02:59,414 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:02:59,421 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:02:59,601 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:02:59,607 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:02:59,612 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:03:00,602 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118000] due to args.save_total_limit\n",
            "{'loss': 2.2609, 'learning_rate': 4.864918023891989e-05, 'epoch': 0.54}\n",
            "  3% 119500/4423240 [3:08:13<105:58:35, 11.28it/s][INFO|trainer.py:1937] 2021-09-13 13:03:47,350 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:03:47,358 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:03:47,530 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:03:47,537 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:03:47,543 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:03:48,135 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-118500] due to args.save_total_limit\n",
            "{'loss': 2.2197, 'learning_rate': 4.864352827339236e-05, 'epoch': 0.54}\n",
            "  3% 120000/4423240 [3:09:03<106:43:04, 11.20it/s][INFO|trainer.py:1937] 2021-09-13 13:04:37,227 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:04:37,234 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:04:37,442 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:04:37,451 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:04:37,457 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:04:38,087 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119000] due to args.save_total_limit\n",
            "{'loss': 2.1259, 'learning_rate': 4.863787630786483e-05, 'epoch': 0.54}\n",
            "  3% 120500/4423240 [3:09:48<93:35:21, 12.77it/s][INFO|trainer.py:1937] 2021-09-13 13:05:22,394 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:05:22,415 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:05:22,588 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:05:22,595 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:05:22,601 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:05:23,219 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-119500] due to args.save_total_limit\n",
            "{'loss': 2.1929, 'learning_rate': 4.8632224342337294e-05, 'epoch': 0.55}\n",
            "  3% 121000/4423240 [3:10:35<97:14:06, 12.29it/s][INFO|trainer.py:1937] 2021-09-13 13:06:09,494 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:06:09,502 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:06:09,696 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:06:09,704 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:06:09,710 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:06:10,335 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120000] due to args.save_total_limit\n",
            "{'loss': 2.2143, 'learning_rate': 4.862657237680976e-05, 'epoch': 0.55}\n",
            "  3% 121500/4423240 [3:11:24<92:28:43, 12.92it/s][INFO|trainer.py:1937] 2021-09-13 13:06:58,421 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:06:58,428 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:06:58,615 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:06:58,621 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:06:58,626 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:06:59,276 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-120500] due to args.save_total_limit\n",
            "{'loss': 2.2035, 'learning_rate': 4.862092041128223e-05, 'epoch': 0.55}\n",
            "  3% 122000/4423240 [3:12:12<140:54:46,  8.48it/s][INFO|trainer.py:1937] 2021-09-13 13:07:46,340 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:07:46,349 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:07:46,525 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:07:46,532 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:07:46,537 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:07:47,607 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121000] due to args.save_total_limit\n",
            "{'loss': 2.1713, 'learning_rate': 4.8615268445754694e-05, 'epoch': 0.55}\n",
            "  3% 122500/4423240 [3:12:58<98:20:35, 12.15it/s][INFO|trainer.py:1937] 2021-09-13 13:08:32,180 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:08:32,187 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:08:32,355 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:08:32,361 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:08:32,366 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:08:32,997 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-121500] due to args.save_total_limit\n",
            "{'loss': 2.2067, 'learning_rate': 4.8609616480227166e-05, 'epoch': 0.56}\n",
            "  3% 123000/4423240 [3:13:46<103:40:49, 11.52it/s][INFO|trainer.py:1937] 2021-09-13 13:09:20,534 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:09:20,545 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:09:20,717 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:09:20,723 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:09:20,728 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:09:21,320 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122000] due to args.save_total_limit\n",
            "{'loss': 2.2604, 'learning_rate': 4.860396451469964e-05, 'epoch': 0.56}\n",
            "  3% 123500/4423240 [3:14:34<123:44:32,  9.65it/s][INFO|trainer.py:1937] 2021-09-13 13:10:08,731 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:10:08,739 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:10:08,948 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:10:08,954 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:10:08,959 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:10:09,597 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-122500] due to args.save_total_limit\n",
            "{'loss': 2.1758, 'learning_rate': 4.85983125491721e-05, 'epoch': 0.56}\n",
            "  3% 124000/4423240 [3:15:23<95:21:48, 12.52it/s][INFO|trainer.py:1937] 2021-09-13 13:10:57,352 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:10:57,361 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:10:57,538 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:10:57,544 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:10:57,549 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:10:58,157 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123000] due to args.save_total_limit\n",
            "{'loss': 2.0832, 'learning_rate': 4.859266058364457e-05, 'epoch': 0.56}\n",
            "  3% 124500/4423240 [3:16:09<102:35:35, 11.64it/s][INFO|trainer.py:1937] 2021-09-13 13:11:42,809 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:11:42,819 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:11:43,027 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:11:43,034 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:11:43,059 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:11:43,692 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-123500] due to args.save_total_limit\n",
            "{'loss': 2.1884, 'learning_rate': 4.8587008618117044e-05, 'epoch': 0.57}\n",
            "  3% 125000/4423240 [3:16:58<147:42:44,  8.08it/s][INFO|trainer.py:1937] 2021-09-13 13:12:32,041 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:12:32,048 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:12:32,236 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:12:32,242 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:12:32,247 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:12:32,869 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124000] due to args.save_total_limit\n",
            "{'loss': 2.2204, 'learning_rate': 4.85813566525895e-05, 'epoch': 0.57}\n",
            "  3% 125500/4423240 [3:17:46<105:53:56, 11.27it/s][INFO|trainer.py:1937] 2021-09-13 13:13:20,590 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:13:20,597 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:13:20,783 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:13:20,809 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:13:20,814 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:13:21,414 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-124500] due to args.save_total_limit\n",
            "{'loss': 2.1988, 'learning_rate': 4.8575704687061974e-05, 'epoch': 0.57}\n",
            "  3% 126000/4423240 [3:18:34<110:57:24, 10.76it/s][INFO|trainer.py:1937] 2021-09-13 13:14:08,267 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:14:08,273 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:14:08,479 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:14:08,487 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:14:08,492 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:14:09,093 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125000] due to args.save_total_limit\n",
            "{'loss': 2.1938, 'learning_rate': 4.8570052721534445e-05, 'epoch': 0.57}\n",
            "  3% 126500/4423240 [3:19:21<122:19:16,  9.76it/s][INFO|trainer.py:1937] 2021-09-13 13:14:55,234 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:14:55,240 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:14:55,427 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:14:55,433 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:14:55,457 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:14:56,045 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-125500] due to args.save_total_limit\n",
            "{'loss': 2.0421, 'learning_rate': 4.856440075600691e-05, 'epoch': 0.57}\n",
            "  3% 127000/4423240 [3:20:07<95:12:14, 12.54it/s][INFO|trainer.py:1937] 2021-09-13 13:15:40,899 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:15:40,908 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:15:41,096 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:15:41,102 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:15:41,108 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:15:41,732 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126000] due to args.save_total_limit\n",
            "{'loss': 2.0996, 'learning_rate': 4.855874879047938e-05, 'epoch': 0.58}\n",
            "  3% 127500/4423240 [3:20:53<96:07:15, 12.41it/s][INFO|trainer.py:1937] 2021-09-13 13:16:27,155 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:16:27,170 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:16:27,343 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:16:27,350 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:16:27,355 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:16:28,362 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-126500] due to args.save_total_limit\n",
            "{'loss': 2.1936, 'learning_rate': 4.855309682495185e-05, 'epoch': 0.58}\n",
            "  3% 128000/4423240 [3:21:39<105:40:19, 11.29it/s][INFO|trainer.py:1937] 2021-09-13 13:17:13,275 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:17:13,282 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:17:13,477 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:17:13,484 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:17:13,490 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:17:14,108 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127000] due to args.save_total_limit\n",
            "{'loss': 2.0944, 'learning_rate': 4.854744485942432e-05, 'epoch': 0.58}\n",
            "  3% 128500/4423240 [3:22:26<109:44:36, 10.87it/s][INFO|trainer.py:1937] 2021-09-13 13:18:00,757 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:18:00,763 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:18:00,935 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:18:00,941 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:18:00,946 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:18:01,487 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-127500] due to args.save_total_limit\n",
            "{'loss': 2.1355, 'learning_rate': 4.854179289389678e-05, 'epoch': 0.58}\n",
            "  3% 129000/4423240 [3:23:13<168:54:52,  7.06it/s][INFO|trainer.py:1937] 2021-09-13 13:18:46,878 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:18:46,899 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:18:47,087 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:18:47,097 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:18:47,101 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:18:47,677 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128000] due to args.save_total_limit\n",
            "{'loss': 2.1973, 'learning_rate': 4.853614092836925e-05, 'epoch': 0.59}\n",
            "  3% 129500/4423240 [3:24:00<107:03:52, 11.14it/s][INFO|trainer.py:1937] 2021-09-13 13:19:34,618 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:19:34,624 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:19:34,815 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:19:34,821 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:19:34,826 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:19:35,366 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-128500] due to args.save_total_limit\n",
            "{'loss': 2.1684, 'learning_rate': 4.853048896284172e-05, 'epoch': 0.59}\n",
            "  3% 130000/4423240 [3:24:46<121:54:39,  9.78it/s][INFO|trainer.py:1937] 2021-09-13 13:20:20,709 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:20:20,716 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:20:20,878 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:20:20,884 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:20:20,888 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:20:21,507 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129000] due to args.save_total_limit\n",
            "{'loss': 2.1744, 'learning_rate': 4.852483699731419e-05, 'epoch': 0.59}\n",
            "  3% 130500/4423240 [3:25:33<86:21:17, 13.81it/s][INFO|trainer.py:1937] 2021-09-13 13:21:07,785 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:21:07,791 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:21:07,986 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:21:07,992 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:21:07,996 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:21:08,963 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-129500] due to args.save_total_limit\n",
            "{'loss': 2.1611, 'learning_rate': 4.851918503178666e-05, 'epoch': 0.59}\n",
            "  3% 131000/4423240 [3:26:22<154:37:28,  7.71it/s][INFO|trainer.py:1937] 2021-09-13 13:21:56,336 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:21:56,345 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:21:56,538 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:21:56,545 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:21:56,550 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:21:57,114 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130000] due to args.save_total_limit\n",
            "{'loss': 2.1596, 'learning_rate': 4.8513533066259124e-05, 'epoch': 0.59}\n",
            "  3% 131500/4423240 [3:27:11<111:38:14, 10.68it/s][INFO|trainer.py:1937] 2021-09-13 13:22:45,209 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:22:45,220 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:22:45,404 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:22:45,410 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:22:45,416 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:22:46,010 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-130500] due to args.save_total_limit\n",
            "{'loss': 2.0604, 'learning_rate': 4.8507881100731596e-05, 'epoch': 0.6}\n",
            "  3% 132000/4423240 [3:28:00<95:09:39, 12.53it/s][INFO|trainer.py:1937] 2021-09-13 13:23:34,175 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:23:34,182 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:23:34,430 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:23:34,440 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:23:34,448 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:23:35,062 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131000] due to args.save_total_limit\n",
            "{'loss': 2.1049, 'learning_rate': 4.850222913520406e-05, 'epoch': 0.6}\n",
            "  3% 132500/4423240 [3:28:48<101:08:19, 11.78it/s][INFO|trainer.py:1937] 2021-09-13 13:24:22,681 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:24:22,688 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:24:22,876 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:24:22,883 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:24:22,888 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:24:23,530 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-131500] due to args.save_total_limit\n",
            "{'loss': 2.1433, 'learning_rate': 4.8496577169676525e-05, 'epoch': 0.6}\n",
            "  3% 133000/4423240 [3:29:37<91:53:07, 12.97it/s][INFO|trainer.py:1937] 2021-09-13 13:25:11,625 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:25:11,633 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:25:11,822 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:25:11,829 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:25:11,835 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:25:12,469 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132000] due to args.save_total_limit\n",
            "{'loss': 2.1756, 'learning_rate': 4.8490925204148996e-05, 'epoch': 0.6}\n",
            "  3% 133500/4423240 [3:30:24<85:30:56, 13.93it/s][INFO|trainer.py:1937] 2021-09-13 13:25:58,427 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:25:58,435 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:25:58,619 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:25:58,624 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:25:58,630 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:25:59,255 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-132500] due to args.save_total_limit\n",
            "{'loss': 2.1682, 'learning_rate': 4.848527323862147e-05, 'epoch': 0.61}\n",
            "  3% 134000/4423240 [3:31:12<116:02:27, 10.27it/s][INFO|trainer.py:1937] 2021-09-13 13:26:46,543 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:26:46,550 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:26:46,744 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:26:46,751 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:26:46,756 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:26:47,355 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133000] due to args.save_total_limit\n",
            "{'loss': 2.199, 'learning_rate': 4.847962127309393e-05, 'epoch': 0.61}\n",
            "  3% 134500/4423240 [3:32:00<94:48:46, 12.56it/s][INFO|trainer.py:1937] 2021-09-13 13:27:34,130 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:27:34,137 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:27:34,336 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:27:34,342 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:27:34,347 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:27:34,934 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-133500] due to args.save_total_limit\n",
            "{'loss': 2.1876, 'learning_rate': 4.8473969307566403e-05, 'epoch': 0.61}\n",
            "  3% 135000/4423240 [3:32:48<83:45:02, 14.22it/s][INFO|trainer.py:1937] 2021-09-13 13:28:22,411 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:28:22,428 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:28:22,603 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:28:22,609 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:28:22,634 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:28:23,209 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134000] due to args.save_total_limit\n",
            "{'loss': 2.1926, 'learning_rate': 4.8468317342038875e-05, 'epoch': 0.61}\n",
            "  3% 135500/4423240 [3:33:35<122:07:20,  9.75it/s][INFO|trainer.py:1937] 2021-09-13 13:29:09,379 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:29:09,387 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:29:09,570 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:29:09,580 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:29:09,588 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:29:10,256 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-134500] due to args.save_total_limit\n",
            "{'loss': 2.1157, 'learning_rate': 4.846266537651134e-05, 'epoch': 0.61}\n",
            "  3% 136000/4423240 [3:34:22<94:01:24, 12.67it/s][INFO|trainer.py:1937] 2021-09-13 13:29:56,651 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:29:56,660 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:29:56,842 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:29:56,848 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:29:56,853 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:29:57,844 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135000] due to args.save_total_limit\n",
            "{'loss': 2.0527, 'learning_rate': 4.8457013410983804e-05, 'epoch': 0.62}\n",
            "  3% 136500/4423240 [3:35:10<116:49:24, 10.19it/s][INFO|trainer.py:1937] 2021-09-13 13:30:44,798 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:30:44,805 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:30:44,982 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:30:45,002 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:30:45,013 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:30:45,630 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-135500] due to args.save_total_limit\n",
            "{'loss': 2.1567, 'learning_rate': 4.8451361445456275e-05, 'epoch': 0.62}\n",
            "  3% 137000/4423240 [3:35:57<98:02:26, 12.14it/s][INFO|trainer.py:1937] 2021-09-13 13:31:31,251 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:31:31,258 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:31:31,426 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:31:31,432 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:31:31,436 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:31:31,995 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136000] due to args.save_total_limit\n",
            "{'loss': 2.1084, 'learning_rate': 4.844570947992874e-05, 'epoch': 0.62}\n",
            "  3% 137500/4423240 [3:36:44<117:36:06, 10.12it/s][INFO|trainer.py:1937] 2021-09-13 13:32:18,083 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:32:18,104 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:32:18,293 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:32:18,300 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:32:18,327 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:32:19,008 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-136500] due to args.save_total_limit\n",
            "{'loss': 2.1529, 'learning_rate': 4.844005751440121e-05, 'epoch': 0.62}\n",
            "  3% 138000/4423240 [3:37:30<102:17:32, 11.64it/s][INFO|trainer.py:1937] 2021-09-13 13:33:04,729 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:33:04,738 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:33:04,920 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:33:04,926 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:33:04,931 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:33:05,576 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137000] due to args.save_total_limit\n",
            "{'loss': 2.1704, 'learning_rate': 4.843440554887368e-05, 'epoch': 0.63}\n",
            "  3% 138500/4423240 [3:38:21<134:19:43,  8.86it/s][INFO|trainer.py:1937] 2021-09-13 13:33:55,066 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:33:55,076 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:33:55,251 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:33:55,258 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:33:55,273 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:33:55,928 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-137500] due to args.save_total_limit\n",
            "{'loss': 2.0307, 'learning_rate': 4.842875358334615e-05, 'epoch': 0.63}\n",
            "  3% 139000/4423240 [3:39:08<113:58:33, 10.44it/s][INFO|trainer.py:1937] 2021-09-13 13:34:42,322 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:34:42,329 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:34:42,509 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:34:42,516 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:34:42,521 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:34:43,517 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138000] due to args.save_total_limit\n",
            "{'loss': 2.111, 'learning_rate': 4.842310161781862e-05, 'epoch': 0.63}\n",
            "  3% 139500/4423240 [3:39:55<101:32:48, 11.72it/s][INFO|trainer.py:1937] 2021-09-13 13:35:28,803 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:35:28,810 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:35:29,007 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:35:29,014 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:35:29,019 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:35:29,633 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-138500] due to args.save_total_limit\n",
            "{'loss': 1.9712, 'learning_rate': 4.841744965229108e-05, 'epoch': 0.63}\n",
            "  3% 140000/4423240 [3:40:41<92:26:03, 12.87it/s][INFO|trainer.py:1937] 2021-09-13 13:36:15,251 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:36:15,259 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:36:15,433 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:36:15,439 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:36:15,444 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:36:16,252 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139000] due to args.save_total_limit\n",
            "{'loss': 2.1346, 'learning_rate': 4.841179768676355e-05, 'epoch': 0.64}\n",
            "  3% 140500/4423240 [3:41:29<131:24:21,  9.05it/s][INFO|trainer.py:1937] 2021-09-13 13:37:02,920 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:37:02,927 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:37:03,110 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:37:03,134 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:37:03,139 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:37:03,744 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-139500] due to args.save_total_limit\n",
            "{'loss': 2.1308, 'learning_rate': 4.840614572123602e-05, 'epoch': 0.64}\n",
            "  3% 141000/4423240 [3:42:16<87:50:24, 13.54it/s][INFO|trainer.py:1937] 2021-09-13 13:37:50,549 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:37:50,557 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:37:50,737 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:37:50,743 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:37:50,747 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:37:51,383 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140000] due to args.save_total_limit\n",
            "{'loss': 2.1076, 'learning_rate': 4.840049375570849e-05, 'epoch': 0.64}\n",
            "  3% 141500/4423240 [3:43:03<89:24:55, 13.30it/s][INFO|trainer.py:1937] 2021-09-13 13:38:37,247 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:38:37,254 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:38:37,448 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:38:37,454 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:38:37,459 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:38:38,481 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-140500] due to args.save_total_limit\n",
            "{'loss': 2.0406, 'learning_rate': 4.8394841790180955e-05, 'epoch': 0.64}\n",
            "  3% 142000/4423240 [3:43:50<116:25:17, 10.21it/s][INFO|trainer.py:1937] 2021-09-13 13:39:24,313 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:39:24,320 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:39:24,505 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:39:24,511 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:39:24,517 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:39:25,151 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141000] due to args.save_total_limit\n",
            "{'loss': 2.106, 'learning_rate': 4.8389189824653426e-05, 'epoch': 0.64}\n",
            "  3% 142500/4423240 [3:44:37<100:38:56, 11.81it/s][INFO|trainer.py:1937] 2021-09-13 13:40:10,827 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:40:10,833 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:40:11,000 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:40:11,006 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:40:11,028 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:40:11,739 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-141500] due to args.save_total_limit\n",
            "{'loss': 2.1717, 'learning_rate': 4.838353785912589e-05, 'epoch': 0.65}\n",
            "  3% 143000/4423240 [3:45:23<105:02:28, 11.32it/s][INFO|trainer.py:1937] 2021-09-13 13:40:57,688 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:40:57,710 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:40:57,889 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:40:57,896 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:40:57,901 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:40:58,490 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142000] due to args.save_total_limit\n",
            "{'loss': 2.107, 'learning_rate': 4.837788589359836e-05, 'epoch': 0.65}\n",
            "  3% 143500/4423240 [3:46:12<89:49:11, 13.24it/s][INFO|trainer.py:1937] 2021-09-13 13:41:46,359 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:41:46,365 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:41:46,564 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:41:46,569 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:41:46,590 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:41:47,200 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-142500] due to args.save_total_limit\n",
            "{'loss': 2.1969, 'learning_rate': 4.8372233928070827e-05, 'epoch': 0.65}\n",
            "  3% 144000/4423240 [3:47:00<119:43:19,  9.93it/s][INFO|trainer.py:1937] 2021-09-13 13:42:34,241 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:42:34,250 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:42:34,427 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:42:34,433 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:42:34,437 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:42:35,058 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143000] due to args.save_total_limit\n",
            "{'loss': 2.1824, 'learning_rate': 4.83665819625433e-05, 'epoch': 0.65}\n",
            "  3% 144500/4423240 [3:47:46<100:27:17, 11.83it/s][INFO|trainer.py:1937] 2021-09-13 13:43:20,770 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:43:20,777 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:43:20,948 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:43:20,960 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:43:20,965 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:43:21,973 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-143500] due to args.save_total_limit\n",
            "{'loss': 2.0453, 'learning_rate': 4.836092999701576e-05, 'epoch': 0.66}\n",
            "  3% 145000/4423240 [3:48:34<115:19:50, 10.30it/s][INFO|trainer.py:1937] 2021-09-13 13:44:07,894 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:44:07,901 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:44:08,088 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:44:08,095 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:44:08,100 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:44:08,662 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144000] due to args.save_total_limit\n",
            "{'loss': 2.0368, 'learning_rate': 4.8355278031488234e-05, 'epoch': 0.66}\n",
            "  3% 145500/4423240 [3:49:21<99:58:25, 11.89it/s][INFO|trainer.py:1937] 2021-09-13 13:44:55,652 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:44:55,677 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:44:55,874 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:44:55,880 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:44:55,885 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:44:56,468 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-144500] due to args.save_total_limit\n",
            "{'loss': 2.0462, 'learning_rate': 4.83496260659607e-05, 'epoch': 0.66}\n",
            "  3% 146000/4423240 [3:50:08<302:34:13,  3.93it/s][INFO|trainer.py:1937] 2021-09-13 13:45:42,017 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:45:42,023 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:45:42,220 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:45:42,226 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:45:42,231 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:45:42,806 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145000] due to args.save_total_limit\n",
            "{'loss': 2.0723, 'learning_rate': 4.834397410043317e-05, 'epoch': 0.66}\n",
            "  3% 146500/4423240 [3:50:55<148:05:20,  8.02it/s][INFO|trainer.py:1937] 2021-09-13 13:46:29,747 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:46:29,755 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:46:29,946 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:46:29,957 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:46:29,966 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:46:30,587 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-145500] due to args.save_total_limit\n",
            "{'loss': 2.184, 'learning_rate': 4.833832213490564e-05, 'epoch': 0.66}\n",
            "  3% 147000/4423240 [3:51:44<104:38:57, 11.35it/s][INFO|trainer.py:1937] 2021-09-13 13:47:18,078 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:47:18,085 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:47:18,252 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:47:18,257 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:47:18,262 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:47:19,269 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146000] due to args.save_total_limit\n",
            "{'loss': 2.011, 'learning_rate': 4.8332670169378106e-05, 'epoch': 0.67}\n",
            "  3% 147500/4423240 [3:52:31<99:02:56, 11.99it/s][INFO|trainer.py:1937] 2021-09-13 13:48:05,389 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:48:05,396 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:48:05,574 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:48:05,581 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:48:05,586 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:48:06,156 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-146500] due to args.save_total_limit\n",
            "{'loss': 2.0506, 'learning_rate': 4.832701820385057e-05, 'epoch': 0.67}\n",
            "  3% 148000/4423240 [3:53:21<111:00:15, 10.70it/s][INFO|trainer.py:1937] 2021-09-13 13:48:54,803 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:48:54,809 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:48:54,976 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:48:54,981 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:48:54,986 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:48:55,558 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147000] due to args.save_total_limit\n",
            "{'loss': 2.0516, 'learning_rate': 4.832136623832304e-05, 'epoch': 0.67}\n",
            "  3% 148500/4423240 [3:54:07<127:40:52,  9.30it/s][INFO|trainer.py:1937] 2021-09-13 13:49:41,401 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:49:41,408 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:49:41,597 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:49:41,604 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:49:41,609 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:49:42,181 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-147500] due to args.save_total_limit\n",
            "{'loss': 2.1251, 'learning_rate': 4.8315714272795506e-05, 'epoch': 0.67}\n",
            "  3% 149000/4423240 [3:54:54<87:51:22, 13.51it/s][INFO|trainer.py:1937] 2021-09-13 13:50:28,216 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:50:28,223 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:50:28,399 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:50:28,405 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:50:28,429 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:50:29,038 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148000] due to args.save_total_limit\n",
            "{'loss': 2.0855, 'learning_rate': 4.831006230726798e-05, 'epoch': 0.68}\n",
            "  3% 149500/4423240 [3:55:42<113:17:15, 10.48it/s][INFO|trainer.py:1937] 2021-09-13 13:51:16,010 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:51:16,019 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:51:16,200 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:51:16,211 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:51:16,220 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:51:16,827 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-148500] due to args.save_total_limit\n",
            "{'loss': 2.0213, 'learning_rate': 4.830441034174045e-05, 'epoch': 0.68}\n",
            "  3% 150000/4423240 [3:56:28<113:06:40, 10.49it/s][INFO|trainer.py:1937] 2021-09-13 13:52:02,004 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:52:02,011 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:52:02,175 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:52:02,181 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:52:02,186 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:52:03,194 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149000] due to args.save_total_limit\n",
            "{'loss': 1.9935, 'learning_rate': 4.829875837621291e-05, 'epoch': 0.68}\n",
            "  3% 150500/4423240 [3:57:16<108:25:11, 10.95it/s][INFO|trainer.py:1937] 2021-09-13 13:52:50,736 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:52:50,742 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:52:50,918 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:52:50,924 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:52:50,930 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:52:51,916 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-149500] due to args.save_total_limit\n",
            "{'loss': 1.9182, 'learning_rate': 4.8293106410685385e-05, 'epoch': 0.68}\n",
            "  3% 151000/4423240 [3:58:04<107:39:44, 11.02it/s][INFO|trainer.py:1937] 2021-09-13 13:53:37,894 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:53:37,902 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:53:38,064 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:53:38,070 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:53:38,075 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:53:38,681 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150000] due to args.save_total_limit\n",
            "{'loss': 2.1506, 'learning_rate': 4.828745444515785e-05, 'epoch': 0.69}\n",
            "  3% 151500/4423240 [3:58:53<100:28:06, 11.81it/s][INFO|trainer.py:1937] 2021-09-13 13:54:27,314 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:54:27,324 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:54:27,512 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:54:27,519 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:54:27,524 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:54:28,161 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-150500] due to args.save_total_limit\n",
            "{'loss': 2.0364, 'learning_rate': 4.8281802479630314e-05, 'epoch': 0.69}\n",
            "  3% 152000/4423240 [3:59:42<133:07:10,  8.91it/s][INFO|trainer.py:1937] 2021-09-13 13:55:16,112 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:55:16,119 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:55:16,279 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:55:16,285 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:55:16,305 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:55:16,878 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151000] due to args.save_total_limit\n",
            "{'loss': 2.1704, 'learning_rate': 4.8276150514102785e-05, 'epoch': 0.69}\n",
            "  3% 152500/4423240 [4:00:29<116:47:14, 10.16it/s][INFO|trainer.py:1937] 2021-09-13 13:56:03,794 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:56:03,801 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:56:03,986 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:56:03,992 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:56:03,997 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:56:04,626 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-151500] due to args.save_total_limit\n",
            "{'loss': 2.1101, 'learning_rate': 4.8270498548575256e-05, 'epoch': 0.69}\n",
            "  3% 153000/4423240 [4:01:16<108:49:52, 10.90it/s][INFO|trainer.py:1937] 2021-09-13 13:56:50,396 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:56:50,403 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:56:50,589 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:56:50,595 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:56:50,600 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:56:51,601 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152000] due to args.save_total_limit\n",
            "{'loss': 2.0544, 'learning_rate': 4.826484658304772e-05, 'epoch': 0.69}\n",
            "  3% 153500/4423240 [4:02:03<93:31:13, 12.68it/s][INFO|trainer.py:1937] 2021-09-13 13:57:37,763 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:57:37,770 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:57:37,973 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:57:37,979 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:57:37,984 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:57:38,579 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-152500] due to args.save_total_limit\n",
            "{'loss': 2.1292, 'learning_rate': 4.825919461752019e-05, 'epoch': 0.7}\n",
            "  3% 154000/4423240 [4:02:49<117:56:53, 10.05it/s][INFO|trainer.py:1937] 2021-09-13 13:58:23,426 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:58:23,457 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:58:23,645 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:58:23,652 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:58:23,657 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:58:24,557 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153000] due to args.save_total_limit\n",
            "{'loss': 2.1029, 'learning_rate': 4.8253542651992664e-05, 'epoch': 0.7}\n",
            "  3% 154500/4423240 [4:03:37<111:47:36, 10.61it/s][INFO|trainer.py:1937] 2021-09-13 13:59:10,825 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:59:10,831 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:59:11,032 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:59:11,056 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:59:11,061 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 13:59:11,681 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-153500] due to args.save_total_limit\n",
            "{'loss': 2.1338, 'learning_rate': 4.824789068646512e-05, 'epoch': 0.7}\n",
            "  4% 155000/4423240 [4:04:25<126:05:22,  9.40it/s][INFO|trainer.py:1937] 2021-09-13 13:59:59,689 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 13:59:59,696 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 13:59:59,878 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 13:59:59,885 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 13:59:59,907 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:00:00,540 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154000] due to args.save_total_limit\n",
            "{'loss': 1.9342, 'learning_rate': 4.824223872093759e-05, 'epoch': 0.7}\n",
            "  4% 155500/4423240 [4:05:12<109:52:31, 10.79it/s][INFO|trainer.py:1937] 2021-09-13 14:00:46,481 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:00:46,488 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:00:46,681 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:00:46,687 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:00:46,693 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:00:47,654 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-154500] due to args.save_total_limit\n",
            "{'loss': 2.0629, 'learning_rate': 4.8236586755410064e-05, 'epoch': 0.71}\n",
            "  4% 156000/4423240 [4:06:00<99:47:55, 11.88it/s][INFO|trainer.py:1937] 2021-09-13 14:01:33,896 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:01:33,904 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:01:34,084 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:01:34,093 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:01:34,098 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:01:34,787 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155000] due to args.save_total_limit\n",
            "{'loss': 2.1161, 'learning_rate': 4.823093478988253e-05, 'epoch': 0.71}\n",
            "  4% 156500/4423240 [4:06:50<95:33:20, 12.40it/s][INFO|trainer.py:1937] 2021-09-13 14:02:24,027 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:02:24,034 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:02:24,224 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:02:24,230 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:02:24,237 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:02:24,827 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-155500] due to args.save_total_limit\n",
            "{'loss': 2.0343, 'learning_rate': 4.8225282824355e-05, 'epoch': 0.71}\n",
            "  4% 157000/4423240 [4:07:37<92:10:08, 12.86it/s][INFO|trainer.py:1937] 2021-09-13 14:03:11,151 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:03:11,170 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:03:11,356 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:03:11,361 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:03:11,366 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:03:11,910 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156000] due to args.save_total_limit\n",
            "{'loss': 2.0124, 'learning_rate': 4.821963085882747e-05, 'epoch': 0.71}\n",
            "  4% 157500/4423240 [4:08:25<102:02:43, 11.61it/s][INFO|trainer.py:1937] 2021-09-13 14:03:59,743 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:03:59,750 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:03:59,941 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:03:59,946 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:03:59,951 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:04:00,557 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-156500] due to args.save_total_limit\n",
            "{'loss': 2.0739, 'learning_rate': 4.8213978893299936e-05, 'epoch': 0.71}\n",
            "  4% 158000/4423240 [4:09:12<98:36:25, 12.02it/s][INFO|trainer.py:1937] 2021-09-13 14:04:45,840 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:04:45,848 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:04:46,051 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:04:46,058 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:04:46,063 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:04:46,680 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157000] due to args.save_total_limit\n",
            "{'loss': 2.0514, 'learning_rate': 4.820832692777241e-05, 'epoch': 0.72}\n",
            "  4% 158500/4423240 [4:10:00<98:04:12, 12.08it/s][INFO|trainer.py:1937] 2021-09-13 14:05:34,601 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:05:34,608 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:05:34,795 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:05:34,803 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:05:34,815 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:05:35,831 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-157500] due to args.save_total_limit\n",
            "{'loss': 2.0155, 'learning_rate': 4.820267496224487e-05, 'epoch': 0.72}\n",
            "  4% 159000/4423240 [4:10:48<97:48:15, 12.11it/s][INFO|trainer.py:1937] 2021-09-13 14:06:22,598 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:06:22,605 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:06:22,799 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:06:22,809 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:06:22,817 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:06:23,407 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158000] due to args.save_total_limit\n",
            "{'loss': 2.0068, 'learning_rate': 4.8197022996717336e-05, 'epoch': 0.72}\n",
            "  4% 159500/4423240 [4:11:37<93:30:49, 12.67it/s][INFO|trainer.py:1937] 2021-09-13 14:07:10,956 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:07:10,963 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:07:11,147 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:07:11,157 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:07:11,166 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:07:11,786 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-158500] due to args.save_total_limit\n",
            "{'loss': 1.9959, 'learning_rate': 4.819137103118981e-05, 'epoch': 0.72}\n",
            "  4% 160000/4423240 [4:12:25<99:20:58, 11.92it/s][INFO|trainer.py:1937] 2021-09-13 14:07:59,311 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:07:59,317 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:07:59,507 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:07:59,531 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:07:59,540 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:08:00,144 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159000] due to args.save_total_limit\n",
            "{'loss': 2.1318, 'learning_rate': 4.818571906566228e-05, 'epoch': 0.73}\n",
            "  4% 160500/4423240 [4:13:14<129:13:17,  9.16it/s][INFO|trainer.py:1937] 2021-09-13 14:08:47,927 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:08:47,936 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:08:48,104 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:08:48,110 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:08:48,115 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:08:48,746 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-159500] due to args.save_total_limit\n",
            "{'loss': 1.9942, 'learning_rate': 4.8180067100134744e-05, 'epoch': 0.73}\n",
            "  4% 161000/4423240 [4:14:02<115:55:47, 10.21it/s][INFO|trainer.py:1937] 2021-09-13 14:09:36,203 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:09:36,231 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:09:36,422 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:09:36,428 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:09:36,433 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:09:37,416 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160000] due to args.save_total_limit\n",
            "{'loss': 2.0804, 'learning_rate': 4.8174415134607215e-05, 'epoch': 0.73}\n",
            "  4% 161500/4423240 [4:14:53<95:26:29, 12.40it/s][INFO|trainer.py:1937] 2021-09-13 14:10:27,504 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:10:27,511 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:10:27,709 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:10:27,716 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:10:27,721 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:10:28,303 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-160500] due to args.save_total_limit\n",
            "{'loss': 2.0709, 'learning_rate': 4.8168763169079686e-05, 'epoch': 0.73}\n",
            "  4% 162000/4423240 [4:15:43<99:32:32, 11.89it/s][INFO|trainer.py:1937] 2021-09-13 14:11:17,074 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:11:17,080 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:11:17,250 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:11:17,256 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:11:17,261 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:11:17,861 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161000] due to args.save_total_limit\n",
            "{'loss': 2.0495, 'learning_rate': 4.8163111203552144e-05, 'epoch': 0.73}\n",
            "  4% 162500/4423240 [4:16:29<87:15:32, 13.56it/s][INFO|trainer.py:1937] 2021-09-13 14:12:03,074 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:12:03,084 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:12:03,282 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:12:03,289 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:12:03,295 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:12:03,871 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-161500] due to args.save_total_limit\n",
            "{'loss': 1.9166, 'learning_rate': 4.8157459238024615e-05, 'epoch': 0.74}\n",
            "  4% 163000/4423240 [4:17:15<99:20:25, 11.91it/s][INFO|trainer.py:1937] 2021-09-13 14:12:49,047 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:12:49,054 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:12:49,236 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:12:49,242 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:12:49,247 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:12:49,901 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162000] due to args.save_total_limit\n",
            "{'loss': 2.1299, 'learning_rate': 4.815180727249709e-05, 'epoch': 0.74}\n",
            "  4% 163500/4423240 [4:18:02<110:01:32, 10.75it/s][INFO|trainer.py:1937] 2021-09-13 14:13:36,274 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:13:36,281 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:13:36,464 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:13:36,471 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:13:36,477 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:13:37,070 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-162500] due to args.save_total_limit\n",
            "{'loss': 2.1727, 'learning_rate': 4.814615530696955e-05, 'epoch': 0.74}\n",
            "  4% 164000/4423240 [4:18:53<131:50:35,  8.97it/s][INFO|trainer.py:1937] 2021-09-13 14:14:27,687 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:14:27,700 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:14:27,905 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:14:27,911 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:14:27,915 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:14:28,503 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163000] due to args.save_total_limit\n",
            "{'loss': 2.0386, 'learning_rate': 4.814050334144202e-05, 'epoch': 0.74}\n",
            "  4% 164500/4423240 [4:19:43<91:29:02, 12.93it/s][INFO|trainer.py:1937] 2021-09-13 14:15:17,326 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:15:17,335 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:15:17,532 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:15:17,542 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:15:17,547 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:15:18,171 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-163500] due to args.save_total_limit\n",
            "{'loss': 2.0464, 'learning_rate': 4.8134851375914494e-05, 'epoch': 0.75}\n",
            "  4% 165000/4423240 [4:20:31<127:41:35,  9.26it/s][INFO|trainer.py:1937] 2021-09-13 14:16:05,065 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:16:05,072 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:16:05,250 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:16:05,257 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:16:05,262 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:16:05,858 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164000] due to args.save_total_limit\n",
            "{'loss': 2.0035, 'learning_rate': 4.812919941038696e-05, 'epoch': 0.75}\n",
            "  4% 165500/4423240 [4:21:19<97:25:14, 12.14it/s][INFO|trainer.py:1937] 2021-09-13 14:16:53,154 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:16:53,162 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:16:53,343 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:16:53,367 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:16:53,372 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:16:54,011 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-164500] due to args.save_total_limit\n",
            "{'loss': 1.9521, 'learning_rate': 4.812354744485943e-05, 'epoch': 0.75}\n",
            "  4% 166000/4423240 [4:22:06<84:42:10, 13.96it/s][INFO|trainer.py:1937] 2021-09-13 14:17:39,950 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:17:39,957 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:17:40,119 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:17:40,125 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:17:40,130 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:17:40,728 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165000] due to args.save_total_limit\n",
            "{'loss': 2.0305, 'learning_rate': 4.8117895479331895e-05, 'epoch': 0.75}\n",
            "  4% 166500/4423240 [4:22:51<94:35:42, 12.50it/s][INFO|trainer.py:1937] 2021-09-13 14:18:25,246 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:18:25,254 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:18:25,434 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:18:25,439 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:18:25,444 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:18:26,466 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-165500] due to args.save_total_limit\n",
            "{'loss': 2.0178, 'learning_rate': 4.811224351380436e-05, 'epoch': 0.76}\n",
            "  4% 167000/4423240 [4:23:38<121:00:33,  9.77it/s][INFO|trainer.py:1937] 2021-09-13 14:19:12,378 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:19:12,387 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:19:12,559 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:19:12,565 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:19:12,581 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:19:13,186 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166000] due to args.save_total_limit\n",
            "{'loss': 2.1101, 'learning_rate': 4.810659154827683e-05, 'epoch': 0.76}\n",
            "  4% 167500/4423240 [4:24:27<97:56:05, 12.07it/s][INFO|trainer.py:1937] 2021-09-13 14:20:00,977 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:20:00,984 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:20:01,175 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:20:01,180 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:20:01,188 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:20:02,140 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-166500] due to args.save_total_limit\n",
            "{'loss': 1.9554, 'learning_rate': 4.81009395827493e-05, 'epoch': 0.76}\n",
            "  4% 168000/4423240 [4:25:15<108:16:34, 10.92it/s][INFO|trainer.py:1937] 2021-09-13 14:20:48,827 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:20:48,834 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:20:49,007 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:20:49,013 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:20:49,019 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:20:49,660 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167000] due to args.save_total_limit\n",
            "{'loss': 2.0236, 'learning_rate': 4.8095287617221766e-05, 'epoch': 0.76}\n",
            "  4% 168500/4423240 [4:26:04<118:05:11, 10.01it/s][INFO|trainer.py:1937] 2021-09-13 14:21:38,247 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:21:38,254 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:21:38,427 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:21:38,433 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:21:38,455 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:21:39,042 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-167500] due to args.save_total_limit\n",
            "{'loss': 1.991, 'learning_rate': 4.808963565169424e-05, 'epoch': 0.76}\n",
            "  4% 169000/4423240 [4:26:51<107:21:06, 11.01it/s][INFO|trainer.py:1937] 2021-09-13 14:22:25,785 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:22:25,791 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:22:25,968 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:22:25,973 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:22:25,977 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:22:26,636 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168000] due to args.save_total_limit\n",
            "{'loss': 2.0965, 'learning_rate': 4.808398368616671e-05, 'epoch': 0.77}\n",
            "  4% 169500/4423240 [4:27:41<84:17:27, 14.02it/s][INFO|trainer.py:1937] 2021-09-13 14:23:15,447 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:23:15,454 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:23:15,632 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:23:15,639 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:23:15,652 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:23:16,681 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-168500] due to args.save_total_limit\n",
            "{'loss': 2.089, 'learning_rate': 4.807833172063917e-05, 'epoch': 0.77}\n",
            "  4% 170000/4423240 [4:28:30<142:35:39,  8.29it/s][INFO|trainer.py:1937] 2021-09-13 14:24:04,679 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:24:04,686 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:24:04,880 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:24:04,887 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:24:04,892 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:24:05,520 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169000] due to args.save_total_limit\n",
            "{'loss': 1.9945, 'learning_rate': 4.807267975511164e-05, 'epoch': 0.77}\n",
            "  4% 170500/4423240 [4:29:18<145:28:18,  8.12it/s][INFO|trainer.py:1937] 2021-09-13 14:24:52,325 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:24:52,333 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:24:52,546 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:24:52,552 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:24:52,558 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:24:53,276 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-169500] due to args.save_total_limit\n",
            "{'loss': 2.1266, 'learning_rate': 4.806702778958411e-05, 'epoch': 0.77}\n",
            "  4% 171000/4423240 [4:30:06<129:44:26,  9.10it/s][INFO|trainer.py:1937] 2021-09-13 14:25:40,540 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:25:40,563 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:25:40,755 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:25:40,762 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:25:40,767 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:25:41,412 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170000] due to args.save_total_limit\n",
            "{'loss': 1.9994, 'learning_rate': 4.8061375824056574e-05, 'epoch': 0.78}\n",
            "  4% 171500/4423240 [4:30:54<101:09:15, 11.68it/s][INFO|trainer.py:1937] 2021-09-13 14:26:28,787 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:26:28,794 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:26:28,994 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:26:29,001 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:26:29,024 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:26:29,655 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-170500] due to args.save_total_limit\n",
            "{'loss': 1.9747, 'learning_rate': 4.8055723858529045e-05, 'epoch': 0.78}\n",
            "  4% 172000/4423240 [4:31:44<113:31:29, 10.40it/s][INFO|trainer.py:1937] 2021-09-13 14:27:18,030 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:27:18,039 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:27:18,227 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:27:18,234 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:27:18,239 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:27:18,876 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171000] due to args.save_total_limit\n",
            "{'loss': 1.9747, 'learning_rate': 4.805007189300152e-05, 'epoch': 0.78}\n",
            "  4% 172500/4423240 [4:32:32<123:12:48,  9.58it/s][INFO|trainer.py:1937] 2021-09-13 14:28:06,342 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:28:06,349 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:28:06,534 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:28:06,539 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:28:06,544 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:28:07,563 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-171500] due to args.save_total_limit\n",
            "{'loss': 1.906, 'learning_rate': 4.804441992747398e-05, 'epoch': 0.78}\n",
            "  4% 173000/4423240 [4:33:18<103:14:07, 11.44it/s][INFO|trainer.py:1937] 2021-09-13 14:28:52,288 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:28:52,295 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:28:52,482 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:28:52,487 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:28:52,491 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:28:53,383 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172000] due to args.save_total_limit\n",
            "{'loss': 1.9285, 'learning_rate': 4.803876796194645e-05, 'epoch': 0.78}\n",
            "  4% 173500/4423240 [4:34:06<112:53:55, 10.46it/s][INFO|trainer.py:1937] 2021-09-13 14:29:39,863 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:29:39,870 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:29:40,076 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:29:40,083 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:29:40,092 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:29:40,694 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-172500] due to args.save_total_limit\n",
            "{'loss': 1.9493, 'learning_rate': 4.803311599641892e-05, 'epoch': 0.79}\n",
            "  4% 174000/4423240 [4:34:53<112:31:59, 10.49it/s][INFO|trainer.py:1937] 2021-09-13 14:30:27,242 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:30:27,259 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:30:27,439 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:30:27,445 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:30:27,450 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:30:28,038 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173000] due to args.save_total_limit\n",
            "{'loss': 1.987, 'learning_rate': 4.802746403089138e-05, 'epoch': 0.79}\n",
            "  4% 174500/4423240 [4:35:42<104:05:27, 11.34it/s][INFO|trainer.py:1937] 2021-09-13 14:31:15,981 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:31:15,988 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:31:16,189 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:31:16,195 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:31:16,216 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:31:16,823 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-173500] due to args.save_total_limit\n",
            "{'loss': 1.958, 'learning_rate': 4.802181206536385e-05, 'epoch': 0.79}\n",
            "  4% 175000/4423240 [4:36:27<121:06:43,  9.74it/s][INFO|trainer.py:1937] 2021-09-13 14:32:00,951 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:32:00,958 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:32:01,130 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:32:01,135 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:32:01,139 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:32:01,775 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174000] due to args.save_total_limit\n",
            "{'loss': 1.9285, 'learning_rate': 4.8016160099836324e-05, 'epoch': 0.79}\n",
            "  4% 175500/4423240 [4:37:13<87:32:53, 13.48it/s][INFO|trainer.py:1937] 2021-09-13 14:32:47,370 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:32:47,378 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:32:47,549 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:32:47,554 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:32:47,559 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:32:48,546 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-174500] due to args.save_total_limit\n",
            "{'loss': 1.9758, 'learning_rate': 4.801050813430879e-05, 'epoch': 0.8}\n",
            "  4% 176000/4423240 [4:38:01<108:51:23, 10.84it/s][INFO|trainer.py:1937] 2021-09-13 14:33:35,098 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:33:35,105 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:33:35,266 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:33:35,272 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:33:35,277 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:33:35,877 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175000] due to args.save_total_limit\n",
            "{'loss': 1.9788, 'learning_rate': 4.800485616878126e-05, 'epoch': 0.8}\n",
            "  4% 176500/4423240 [4:38:45<88:43:29, 13.30it/s][INFO|trainer.py:1937] 2021-09-13 14:34:19,628 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:34:19,639 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:34:19,812 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:34:19,834 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:34:19,839 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:34:20,420 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-175500] due to args.save_total_limit\n",
            "{'loss': 1.9358, 'learning_rate': 4.7999204203253725e-05, 'epoch': 0.8}\n",
            "  4% 177000/4423240 [4:39:32<110:38:49, 10.66it/s][INFO|trainer.py:1937] 2021-09-13 14:35:06,185 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:35:06,205 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:35:06,394 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:35:06,400 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:35:06,404 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:35:06,977 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176000] due to args.save_total_limit\n",
            "{'loss': 2.0352, 'learning_rate': 4.799355223772619e-05, 'epoch': 0.8}\n",
            "  4% 177500/4423240 [4:40:19<99:43:24, 11.83it/s][INFO|trainer.py:1937] 2021-09-13 14:35:52,927 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:35:52,934 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:35:53,113 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:35:53,120 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:35:53,143 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:35:53,731 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-176500] due to args.save_total_limit\n",
            "{'loss': 2.0291, 'learning_rate': 4.798790027219866e-05, 'epoch': 0.8}\n",
            "  4% 178000/4423240 [4:41:06<111:48:19, 10.55it/s][INFO|trainer.py:1937] 2021-09-13 14:36:40,163 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:36:40,170 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:36:40,358 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:36:40,369 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:36:40,374 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:36:40,940 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177000] due to args.save_total_limit\n",
            "{'loss': 1.8722, 'learning_rate': 4.798224830667113e-05, 'epoch': 0.81}\n",
            "  4% 178500/4423240 [4:41:52<89:46:31, 13.13it/s][INFO|trainer.py:1937] 2021-09-13 14:37:26,302 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:37:26,309 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:37:26,503 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:37:26,509 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:37:26,514 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:37:27,519 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-177500] due to args.save_total_limit\n",
            "{'loss': 1.9959, 'learning_rate': 4.79765963411436e-05, 'epoch': 0.81}\n",
            "  4% 179000/4423240 [4:42:39<91:57:17, 12.82it/s][INFO|trainer.py:1937] 2021-09-13 14:38:12,969 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:38:12,976 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:38:13,148 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:38:13,154 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:38:13,159 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:38:13,766 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178000] due to args.save_total_limit\n",
            "{'loss': 2.0853, 'learning_rate': 4.797094437561607e-05, 'epoch': 0.81}\n",
            "  4% 179500/4423240 [4:43:26<98:21:19, 11.99it/s][INFO|trainer.py:1937] 2021-09-13 14:39:00,407 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:39:00,414 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:39:00,627 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:39:00,633 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:39:00,639 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:39:01,218 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-178500] due to args.save_total_limit\n",
            "{'loss': 2.1115, 'learning_rate': 4.796529241008853e-05, 'epoch': 0.81}\n",
            "  4% 180000/4423240 [4:44:14<116:02:06, 10.16it/s][INFO|trainer.py:1937] 2021-09-13 14:39:48,483 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:39:48,499 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:39:48,699 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:39:48,706 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:39:48,729 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:39:49,323 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179000] due to args.save_total_limit\n",
            "{'loss': 1.9634, 'learning_rate': 4.7959640444561004e-05, 'epoch': 0.82}\n",
            "  4% 180500/4423240 [4:45:03<100:40:18, 11.71it/s][INFO|trainer.py:1937] 2021-09-13 14:40:37,537 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:40:37,545 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:40:37,727 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:40:37,733 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:40:37,738 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:40:38,362 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-179500] due to args.save_total_limit\n",
            "{'loss': 1.9668, 'learning_rate': 4.7953988479033475e-05, 'epoch': 0.82}\n",
            "  4% 181000/4423240 [4:45:51<79:29:41, 14.82it/s][INFO|trainer.py:1937] 2021-09-13 14:41:25,597 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:41:25,604 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:41:25,785 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:41:25,790 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:41:25,794 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:41:26,770 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180000] due to args.save_total_limit\n",
            "{'loss': 1.9887, 'learning_rate': 4.794833651350594e-05, 'epoch': 0.82}\n",
            "  4% 181500/4423240 [4:46:39<109:48:35, 10.73it/s][INFO|trainer.py:1937] 2021-09-13 14:42:13,291 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:42:13,297 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:42:13,476 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:42:13,482 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:42:13,487 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:42:14,087 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-180500] due to args.save_total_limit\n",
            "{'loss': 1.9543, 'learning_rate': 4.7942684547978404e-05, 'epoch': 0.82}\n",
            "  4% 182000/4423240 [4:47:26<108:33:26, 10.85it/s][INFO|trainer.py:1937] 2021-09-13 14:43:00,470 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:43:00,477 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:43:00,675 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:43:00,682 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:43:00,687 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:43:01,268 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181000] due to args.save_total_limit\n",
            "{'loss': 1.9403, 'learning_rate': 4.7937032582450876e-05, 'epoch': 0.83}\n",
            "  4% 182500/4423240 [4:48:11<95:08:32, 12.38it/s][INFO|trainer.py:1937] 2021-09-13 14:43:44,912 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:43:44,918 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:43:45,109 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:43:45,115 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:43:45,121 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:43:45,710 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-181500] due to args.save_total_limit\n",
            "{'loss': 1.9248, 'learning_rate': 4.793138061692334e-05, 'epoch': 0.83}\n",
            "  4% 183000/4423240 [4:48:57<84:57:24, 13.86it/s][INFO|trainer.py:1937] 2021-09-13 14:44:31,797 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:44:31,805 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:44:31,984 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:44:31,989 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:44:31,994 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:44:32,626 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182000] due to args.save_total_limit\n",
            "{'loss': 1.8642, 'learning_rate': 4.792572865139581e-05, 'epoch': 0.83}\n",
            "  4% 183500/4423240 [4:49:44<118:31:07,  9.94it/s][INFO|trainer.py:1937] 2021-09-13 14:45:18,231 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:45:18,239 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:45:18,437 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:45:18,442 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:45:18,446 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:45:19,427 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-182500] due to args.save_total_limit\n",
            "{'loss': 2.0968, 'learning_rate': 4.792007668586828e-05, 'epoch': 0.83}\n",
            "  4% 184000/4423240 [4:50:34<113:44:17, 10.35it/s][INFO|trainer.py:1937] 2021-09-13 14:46:07,889 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:46:07,897 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:46:08,071 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:46:08,077 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:46:08,088 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:46:08,716 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183000] due to args.save_total_limit\n",
            "{'loss': 2.0089, 'learning_rate': 4.791442472034075e-05, 'epoch': 0.83}\n",
            "  4% 184500/4423240 [4:51:23<98:05:01, 12.00it/s][INFO|trainer.py:1937] 2021-09-13 14:46:56,812 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:46:56,820 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:46:56,992 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:46:56,998 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:46:57,003 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:46:57,585 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-183500] due to args.save_total_limit\n",
            "{'loss': 1.9365, 'learning_rate': 4.790877275481321e-05, 'epoch': 0.84}\n",
            "  4% 185000/4423240 [4:52:11<100:40:13, 11.69it/s][INFO|trainer.py:1937] 2021-09-13 14:47:45,435 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:47:45,458 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:47:45,636 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:47:45,642 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:47:45,647 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:47:46,242 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184000] due to args.save_total_limit\n",
            "{'loss': 1.9897, 'learning_rate': 4.7903120789285683e-05, 'epoch': 0.84}\n",
            "  4% 185500/4423240 [4:52:59<106:30:05, 11.05it/s][INFO|trainer.py:1937] 2021-09-13 14:48:33,437 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:48:33,443 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:48:33,618 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:48:33,623 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:48:33,628 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:48:34,224 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-184500] due to args.save_total_limit\n",
            "{'loss': 1.9539, 'learning_rate': 4.7897468823758155e-05, 'epoch': 0.84}\n",
            "  4% 186000/4423240 [4:53:48<174:47:28,  6.73it/s][INFO|trainer.py:1937] 2021-09-13 14:49:22,185 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:49:22,191 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:49:22,355 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:49:22,363 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:49:22,373 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:49:22,986 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185000] due to args.save_total_limit\n",
            "{'loss': 2.0081, 'learning_rate': 4.789181685823062e-05, 'epoch': 0.84}\n",
            "  4% 186500/4423240 [4:54:35<93:41:40, 12.56it/s][INFO|trainer.py:1937] 2021-09-13 14:50:09,197 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:50:09,203 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:50:09,378 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:50:09,384 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:50:09,391 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:50:10,405 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-185500] due to args.save_total_limit\n",
            "{'loss': 1.8746, 'learning_rate': 4.788616489270309e-05, 'epoch': 0.85}\n",
            "  4% 187000/4423240 [4:55:24<120:35:18,  9.76it/s][INFO|trainer.py:1937] 2021-09-13 14:50:57,873 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:50:57,879 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:50:58,061 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:50:58,067 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:50:58,072 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:50:58,696 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186000] due to args.save_total_limit\n",
            "{'loss': 1.8802, 'learning_rate': 4.7880512927175555e-05, 'epoch': 0.85}\n",
            "  4% 187500/4423240 [4:56:10<111:13:20, 10.58it/s][INFO|trainer.py:1937] 2021-09-13 14:51:44,000 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:51:44,009 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:51:44,169 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:51:44,176 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:51:44,181 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:51:44,795 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-186500] due to args.save_total_limit\n",
            "{'loss': 1.9055, 'learning_rate': 4.7874860961648027e-05, 'epoch': 0.85}\n",
            "  4% 188000/4423240 [4:56:56<102:49:38, 11.44it/s][INFO|trainer.py:1937] 2021-09-13 14:52:30,641 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:52:30,649 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:52:30,825 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:52:30,850 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:52:30,855 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:52:31,469 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187000] due to args.save_total_limit\n",
            "{'loss': 2.0081, 'learning_rate': 4.78692089961205e-05, 'epoch': 0.85}\n",
            "  4% 188500/4423240 [4:57:42<104:22:31, 11.27it/s][INFO|trainer.py:1937] 2021-09-13 14:53:16,535 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:53:16,542 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:53:16,736 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:53:16,742 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:53:16,747 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:53:17,318 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-187500] due to args.save_total_limit\n",
            "{'loss': 1.9495, 'learning_rate': 4.786355703059296e-05, 'epoch': 0.85}\n",
            "  4% 189000/4423240 [4:58:28<101:47:00, 11.56it/s][INFO|trainer.py:1937] 2021-09-13 14:54:02,639 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:54:02,646 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:54:02,797 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:54:02,803 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:54:02,807 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:54:03,757 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188000] due to args.save_total_limit\n",
            "{'loss': 1.944, 'learning_rate': 4.785790506506543e-05, 'epoch': 0.86}\n",
            "  4% 189500/4423240 [4:59:16<100:02:16, 11.76it/s][INFO|trainer.py:1937] 2021-09-13 14:54:50,156 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:54:50,163 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:54:50,330 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:54:50,336 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:54:50,341 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:54:50,943 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-188500] due to args.save_total_limit\n",
            "{'loss': 1.88, 'learning_rate': 4.78522530995379e-05, 'epoch': 0.86}\n",
            "  4% 190000/4423240 [5:00:03<91:12:57, 12.89it/s][INFO|trainer.py:1937] 2021-09-13 14:55:37,441 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:55:37,448 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:55:37,619 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:55:37,625 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:55:37,629 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:55:38,241 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189000] due to args.save_total_limit\n",
            "{'loss': 1.9647, 'learning_rate': 4.784660113401036e-05, 'epoch': 0.86}\n",
            "  4% 190500/4423240 [5:00:51<93:30:30, 12.57it/s][INFO|trainer.py:1937] 2021-09-13 14:56:24,812 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:56:24,818 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:56:25,026 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:56:25,034 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:56:25,041 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:56:25,660 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-189500] due to args.save_total_limit\n",
            "{'loss': 1.9198, 'learning_rate': 4.7840949168482834e-05, 'epoch': 0.86}\n",
            "  4% 191000/4423240 [5:01:35<99:26:06, 11.82it/s][INFO|trainer.py:1937] 2021-09-13 14:57:09,754 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:57:09,761 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:57:09,938 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:57:09,944 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:57:09,948 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:57:10,538 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190000] due to args.save_total_limit\n",
            "{'loss': 1.9262, 'learning_rate': 4.7835297202955306e-05, 'epoch': 0.87}\n",
            "  4% 191500/4423240 [5:02:22<89:07:56, 13.19it/s][INFO|trainer.py:1937] 2021-09-13 14:57:56,177 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:57:56,183 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:57:56,355 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:57:56,360 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:57:56,365 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:57:57,129 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-190500] due to args.save_total_limit\n",
            "{'loss': 1.8521, 'learning_rate': 4.782964523742777e-05, 'epoch': 0.87}\n",
            "  4% 192000/4423240 [5:03:07<97:05:44, 12.11it/s][INFO|trainer.py:1937] 2021-09-13 14:58:41,623 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:58:41,629 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:58:41,806 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:58:41,812 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:58:41,817 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:58:42,398 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191000] due to args.save_total_limit\n",
            "{'loss': 2.0033, 'learning_rate': 4.7823993271900235e-05, 'epoch': 0.87}\n",
            "  4% 192500/4423240 [5:03:54<99:57:28, 11.76it/s][INFO|trainer.py:1937] 2021-09-13 14:59:27,920 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 14:59:27,927 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 14:59:28,091 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 14:59:28,097 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 14:59:28,102 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 14:59:28,676 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-191500] due to args.save_total_limit\n",
            "{'loss': 1.9575, 'learning_rate': 4.7818341306372706e-05, 'epoch': 0.87}\n",
            "  4% 193000/4423240 [5:04:41<93:19:28, 12.59it/s][INFO|trainer.py:1937] 2021-09-13 15:00:15,797 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:00:15,804 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:00:15,980 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:00:15,985 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:00:15,991 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:00:16,614 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192000] due to args.save_total_limit\n",
            "{'loss': 1.9727, 'learning_rate': 4.781268934084517e-05, 'epoch': 0.87}\n",
            "  4% 193500/4423240 [5:05:29<87:08:01, 13.48it/s][INFO|trainer.py:1937] 2021-09-13 15:01:03,541 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:01:03,549 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:01:03,710 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:01:03,716 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:01:03,738 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:01:04,369 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-192500] due to args.save_total_limit\n",
            "{'loss': 1.9912, 'learning_rate': 4.780703737531764e-05, 'epoch': 0.88}\n",
            "  4% 194000/4423240 [5:06:20<127:21:41,  9.22it/s][INFO|trainer.py:1937] 2021-09-13 15:01:53,852 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:01:53,858 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:01:54,022 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:01:54,028 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:01:54,032 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:01:54,642 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193000] due to args.save_total_limit\n",
            "{'loss': 1.9265, 'learning_rate': 4.780138540979011e-05, 'epoch': 0.88}\n",
            "  4% 194500/4423240 [5:07:07<85:14:53, 13.78it/s][INFO|trainer.py:1937] 2021-09-13 15:02:40,820 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:02:40,832 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:02:41,012 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:02:41,018 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:02:41,022 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:02:42,003 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-193500] due to args.save_total_limit\n",
            "{'loss': 1.8895, 'learning_rate': 4.779573344426258e-05, 'epoch': 0.88}\n",
            "  4% 195000/4423240 [5:07:54<112:37:02, 10.43it/s][INFO|trainer.py:1937] 2021-09-13 15:03:28,463 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:03:28,472 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:03:28,646 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:03:28,652 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:03:28,657 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:03:29,185 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194000] due to args.save_total_limit\n",
            "{'loss': 1.9715, 'learning_rate': 4.779008147873505e-05, 'epoch': 0.88}\n",
            "  4% 195500/4423240 [5:08:42<119:27:57,  9.83it/s][INFO|trainer.py:1937] 2021-09-13 15:04:16,433 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:04:16,440 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:04:16,615 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:04:16,621 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:04:16,626 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:04:17,206 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-194500] due to args.save_total_limit\n",
            "{'loss': 1.9467, 'learning_rate': 4.7784429513207514e-05, 'epoch': 0.89}\n",
            "  4% 196000/4423240 [5:09:28<109:19:40, 10.74it/s][INFO|trainer.py:1937] 2021-09-13 15:05:02,398 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:05:02,405 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:05:02,583 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:05:02,608 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:05:02,613 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:05:03,221 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195000] due to args.save_total_limit\n",
            "{'loss': 2.0083, 'learning_rate': 4.777877754767998e-05, 'epoch': 0.89}\n",
            "  4% 196500/4423240 [5:10:15<110:09:55, 10.66it/s][INFO|trainer.py:1937] 2021-09-13 15:05:49,642 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:05:49,649 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:05:49,814 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:05:49,820 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:05:49,824 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:05:50,435 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-195500] due to args.save_total_limit\n",
            "{'loss': 1.9186, 'learning_rate': 4.777312558215245e-05, 'epoch': 0.89}\n",
            "  4% 197000/4423240 [5:11:01<98:39:32, 11.90it/s][INFO|trainer.py:1937] 2021-09-13 15:06:35,381 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:06:35,390 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:06:35,563 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:06:35,569 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:06:35,574 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:06:36,569 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196000] due to args.save_total_limit\n",
            "{'loss': 1.8487, 'learning_rate': 4.776747361662492e-05, 'epoch': 0.89}\n",
            "  4% 197500/4423240 [5:11:47<156:04:46,  7.52it/s][INFO|trainer.py:1937] 2021-09-13 15:07:21,258 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:07:21,265 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:07:21,460 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:07:21,466 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:07:21,471 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:07:22,049 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-196500] due to args.save_total_limit\n",
            "{'loss': 1.8839, 'learning_rate': 4.7761821651097386e-05, 'epoch': 0.9}\n",
            "  4% 198000/4423240 [5:12:33<91:20:47, 12.85it/s][INFO|trainer.py:1937] 2021-09-13 15:08:07,703 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:08:07,709 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:08:07,892 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:08:07,898 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:08:07,903 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:08:08,497 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197000] due to args.save_total_limit\n",
            "{'loss': 2.0011, 'learning_rate': 4.775616968556986e-05, 'epoch': 0.9}\n",
            "  4% 198500/4423240 [5:13:19<94:20:23, 12.44it/s][INFO|trainer.py:1937] 2021-09-13 15:08:53,504 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:08:53,526 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:08:53,698 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:08:53,704 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:08:53,710 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:08:54,601 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-197500] due to args.save_total_limit\n",
            "{'loss': 1.9156, 'learning_rate': 4.775051772004233e-05, 'epoch': 0.9}\n",
            "  4% 199000/4423240 [5:14:06<102:35:46, 11.44it/s][INFO|trainer.py:1937] 2021-09-13 15:09:40,057 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:09:40,066 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:09:40,244 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:09:40,255 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:09:40,278 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:09:40,888 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198000] due to args.save_total_limit\n",
            "{'loss': 1.9507, 'learning_rate': 4.774486575451479e-05, 'epoch': 0.9}\n",
            "  5% 199500/4423240 [5:14:52<114:59:21, 10.20it/s][INFO|trainer.py:1937] 2021-09-13 15:10:26,110 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:10:26,125 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:10:26,306 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:10:26,312 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:10:26,316 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:10:26,928 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-198500] due to args.save_total_limit\n",
            "{'loss': 1.7972, 'learning_rate': 4.773921378898726e-05, 'epoch': 0.9}\n",
            "  5% 200000/4423240 [5:15:39<84:15:02, 13.92it/s][INFO|trainer.py:1937] 2021-09-13 15:11:13,668 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:11:13,674 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:11:13,848 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:11:13,854 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:11:13,860 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:11:14,857 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199000] due to args.save_total_limit\n",
            "{'loss': 1.8631, 'learning_rate': 4.773356182345973e-05, 'epoch': 0.91}\n",
            "  5% 200500/4423240 [5:16:26<86:48:25, 13.51it/s][INFO|trainer.py:1937] 2021-09-13 15:12:00,480 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:12:00,487 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:12:00,648 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:12:00,654 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:12:00,658 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:12:01,263 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-199500] due to args.save_total_limit\n",
            "{'loss': 2.0025, 'learning_rate': 4.772790985793219e-05, 'epoch': 0.91}\n",
            "  5% 201000/4423240 [5:17:14<78:39:49, 14.91it/s][INFO|trainer.py:1937] 2021-09-13 15:12:48,164 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:12:48,171 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:12:48,386 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:12:48,391 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:12:48,396 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:12:49,001 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200000] due to args.save_total_limit\n",
            "{'loss': 1.9223, 'learning_rate': 4.7722257892404665e-05, 'epoch': 0.91}\n",
            "  5% 201500/4423240 [5:18:01<98:59:07, 11.85it/s][INFO|trainer.py:1937] 2021-09-13 15:13:35,612 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:13:35,631 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:13:35,801 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:13:35,824 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:13:35,830 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:13:36,439 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-200500] due to args.save_total_limit\n",
            "{'loss': 1.8805, 'learning_rate': 4.7716605926877136e-05, 'epoch': 0.91}\n",
            "  5% 202000/4423240 [5:18:48<95:45:18, 12.25it/s][INFO|trainer.py:1937] 2021-09-13 15:14:22,527 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:14:22,535 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:14:22,710 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:14:22,717 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:14:22,722 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:14:23,363 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201000] due to args.save_total_limit\n",
            "{'loss': 1.9555, 'learning_rate': 4.77109539613496e-05, 'epoch': 0.92}\n",
            "  5% 202500/4423240 [5:19:36<107:41:42, 10.89it/s][INFO|trainer.py:1937] 2021-09-13 15:15:10,239 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:15:10,247 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:15:10,413 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:15:10,418 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:15:10,424 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:15:11,433 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-201500] due to args.save_total_limit\n",
            "{'loss': 2.0188, 'learning_rate': 4.770530199582207e-05, 'epoch': 0.92}\n",
            "  5% 203000/4423240 [5:20:24<117:40:15,  9.96it/s][INFO|trainer.py:1937] 2021-09-13 15:15:58,174 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:15:58,180 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:15:58,345 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:15:58,351 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:15:58,369 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:15:58,996 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202000] due to args.save_total_limit\n",
            "{'loss': 1.9693, 'learning_rate': 4.7699650030294536e-05, 'epoch': 0.92}\n",
            "  5% 203500/4423240 [5:21:11<94:35:58, 12.39it/s][INFO|trainer.py:1937] 2021-09-13 15:16:45,774 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:16:45,781 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:16:45,948 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:16:45,954 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:16:45,958 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:16:46,524 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-202500] due to args.save_total_limit\n",
            "{'loss': 1.9115, 'learning_rate': 4.7693998064767e-05, 'epoch': 0.92}\n",
            "  5% 204000/4423240 [5:22:00<141:11:37,  8.30it/s][INFO|trainer.py:1937] 2021-09-13 15:17:34,377 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:17:34,385 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:17:34,558 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:17:34,565 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:17:34,570 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:17:35,155 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203000] due to args.save_total_limit\n",
            "{'loss': 2.0097, 'learning_rate': 4.768834609923947e-05, 'epoch': 0.92}\n",
            "  5% 204500/4423240 [5:22:47<194:47:07,  6.02it/s][INFO|trainer.py:1937] 2021-09-13 15:18:21,150 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:18:21,157 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:18:21,352 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:18:21,359 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:18:21,382 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:18:21,976 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-203500] due to args.save_total_limit\n",
            "{'loss': 1.9487, 'learning_rate': 4.7682694133711944e-05, 'epoch': 0.93}\n",
            "  5% 205000/4423240 [5:23:33<88:34:34, 13.23it/s][INFO|trainer.py:1937] 2021-09-13 15:19:07,318 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:19:07,324 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:19:07,503 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:19:07,509 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:19:07,514 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:19:08,096 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204000] due to args.save_total_limit\n",
            "{'loss': 1.8948, 'learning_rate': 4.767704216818441e-05, 'epoch': 0.93}\n",
            "  5% 205500/4423240 [5:24:17<76:08:09, 15.39it/s][INFO|trainer.py:1937] 2021-09-13 15:19:51,652 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:19:51,659 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:19:51,829 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:19:51,834 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:19:51,839 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:19:52,824 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-204500] due to args.save_total_limit\n",
            "{'loss': 1.9058, 'learning_rate': 4.767139020265688e-05, 'epoch': 0.93}\n",
            "  5% 206000/4423240 [5:25:08<89:02:50, 13.16it/s][INFO|trainer.py:1937] 2021-09-13 15:20:42,039 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:20:42,046 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:20:42,235 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:20:42,241 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:20:42,246 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:20:42,857 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205000] due to args.save_total_limit\n",
            "{'loss': 1.9198, 'learning_rate': 4.766573823712935e-05, 'epoch': 0.93}\n",
            "  5% 206500/4423240 [5:25:56<85:50:27, 13.65it/s][INFO|trainer.py:1937] 2021-09-13 15:21:30,316 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:21:30,323 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:21:30,489 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:21:30,496 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:21:30,502 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:21:31,118 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-205500] due to args.save_total_limit\n",
            "{'loss': 1.8323, 'learning_rate': 4.7660086271601816e-05, 'epoch': 0.94}\n",
            "  5% 207000/4423240 [5:26:43<91:39:59, 12.78it/s][INFO|trainer.py:1937] 2021-09-13 15:22:16,872 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:22:16,880 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:22:17,036 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:22:17,057 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:22:17,062 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:22:17,594 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206000] due to args.save_total_limit\n",
            "{'loss': 1.8506, 'learning_rate': 4.765443430607428e-05, 'epoch': 0.94}\n",
            "  5% 207500/4423240 [5:27:30<117:48:47,  9.94it/s][INFO|trainer.py:1937] 2021-09-13 15:23:03,820 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:23:03,827 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:23:04,011 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:23:04,016 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:23:04,021 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:23:04,599 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-206500] due to args.save_total_limit\n",
            "{'loss': 2.0021, 'learning_rate': 4.764878234054675e-05, 'epoch': 0.94}\n",
            "  5% 208000/4423240 [5:28:16<87:30:08, 13.38it/s][INFO|trainer.py:1937] 2021-09-13 15:23:50,548 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:23:50,554 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:23:50,730 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:23:50,736 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:23:50,740 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:23:51,710 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207000] due to args.save_total_limit\n",
            "{'loss': 1.8863, 'learning_rate': 4.7643130375019216e-05, 'epoch': 0.94}\n",
            "  5% 208500/4423240 [5:29:03<94:15:54, 12.42it/s][INFO|trainer.py:1937] 2021-09-13 15:24:37,581 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:24:37,590 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:24:37,756 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:24:37,762 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:24:37,766 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:24:38,310 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-207500] due to args.save_total_limit\n",
            "{'loss': 1.9373, 'learning_rate': 4.763747840949169e-05, 'epoch': 0.95}\n",
            "  5% 209000/4423240 [5:29:49<94:57:09, 12.33it/s][INFO|trainer.py:1937] 2021-09-13 15:25:23,665 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:25:23,671 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:25:23,856 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:25:23,862 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:25:23,867 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:25:24,471 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208000] due to args.save_total_limit\n",
            "{'loss': 1.9724, 'learning_rate': 4.763182644396416e-05, 'epoch': 0.95}\n",
            "  5% 209500/4423240 [5:30:37<93:31:09, 12.52it/s][INFO|trainer.py:1937] 2021-09-13 15:26:11,790 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:26:11,811 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:26:11,974 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:26:11,980 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:26:11,985 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:26:12,575 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-208500] due to args.save_total_limit\n",
            "{'loss': 1.9273, 'learning_rate': 4.762617447843662e-05, 'epoch': 0.95}\n",
            "  5% 210000/4423240 [5:31:26<108:05:08, 10.83it/s][INFO|trainer.py:1937] 2021-09-13 15:27:00,682 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:27:00,688 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:27:00,859 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:27:00,865 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:27:00,869 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:27:01,520 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209000] due to args.save_total_limit\n",
            "{'loss': 1.8765, 'learning_rate': 4.7620522512909095e-05, 'epoch': 0.95}\n",
            "  5% 210500/4423240 [5:32:11<139:03:05,  8.42it/s][INFO|trainer.py:1937] 2021-09-13 15:27:45,242 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:27:45,249 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:27:45,411 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:27:45,417 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:27:45,423 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:27:46,037 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-209500] due to args.save_total_limit\n",
            "{'loss': 1.882, 'learning_rate': 4.761487054738156e-05, 'epoch': 0.95}\n",
            "  5% 211000/4423240 [5:32:57<104:54:23, 11.15it/s][INFO|trainer.py:1937] 2021-09-13 15:28:31,787 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:28:31,793 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:28:31,974 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:28:31,980 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:28:31,985 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:28:32,944 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210000] due to args.save_total_limit\n",
            "{'loss': 1.9171, 'learning_rate': 4.7609218581854024e-05, 'epoch': 0.96}\n",
            "  5% 211500/4423240 [5:33:45<89:13:45, 13.11it/s][INFO|trainer.py:1937] 2021-09-13 15:29:19,091 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:29:19,098 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:29:19,280 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:29:19,286 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:29:19,291 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:29:19,873 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-210500] due to args.save_total_limit\n",
            "{'loss': 1.9748, 'learning_rate': 4.7603566616326495e-05, 'epoch': 0.96}\n",
            "  5% 212000/4423240 [5:34:33<136:04:06,  8.60it/s][INFO|trainer.py:1937] 2021-09-13 15:30:07,748 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:30:07,754 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:30:07,941 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:30:07,947 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:30:07,951 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:30:08,495 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211000] due to args.save_total_limit\n",
            "{'loss': 1.9111, 'learning_rate': 4.7597914650798966e-05, 'epoch': 0.96}\n",
            "  5% 212500/4423240 [5:35:21<122:13:20,  9.57it/s][INFO|trainer.py:1937] 2021-09-13 15:30:55,241 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:30:55,248 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:30:55,431 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:30:55,437 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:30:55,458 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:30:56,077 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-211500] due to args.save_total_limit\n",
            "{'loss': 1.8576, 'learning_rate': 4.759226268527143e-05, 'epoch': 0.96}\n",
            "  5% 213000/4423240 [5:36:07<101:54:30, 11.48it/s][INFO|trainer.py:1937] 2021-09-13 15:31:41,355 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:31:41,362 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:31:41,548 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:31:41,554 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:31:41,559 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:31:42,177 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212000] due to args.save_total_limit\n",
            "{'loss': 1.9171, 'learning_rate': 4.75866107197439e-05, 'epoch': 0.97}\n",
            "  5% 213500/4423240 [5:36:55<96:38:56, 12.10it/s][INFO|trainer.py:1937] 2021-09-13 15:32:29,291 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:32:29,299 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:32:29,470 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:32:29,475 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:32:29,480 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:32:30,375 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-212500] due to args.save_total_limit\n",
            "{'loss': 1.8295, 'learning_rate': 4.7580958754216374e-05, 'epoch': 0.97}\n",
            "  5% 214000/4423240 [5:37:41<90:24:55, 12.93it/s][INFO|trainer.py:1937] 2021-09-13 15:33:15,178 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:33:15,185 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:33:15,344 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:33:15,349 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:33:15,354 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:33:15,960 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213000] due to args.save_total_limit\n",
            "{'loss': 1.9008, 'learning_rate': 4.757530678868884e-05, 'epoch': 0.97}\n",
            "  5% 214500/4423240 [5:38:27<94:57:35, 12.31it/s][INFO|trainer.py:1937] 2021-09-13 15:34:00,930 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:34:00,948 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:34:01,115 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:34:01,121 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:34:01,126 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:34:01,682 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-213500] due to args.save_total_limit\n",
            "{'loss': 1.8701, 'learning_rate': 4.75696548231613e-05, 'epoch': 0.97}\n",
            "  5% 215000/4423240 [5:39:16<176:42:59,  6.61it/s][INFO|trainer.py:1937] 2021-09-13 15:34:50,387 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:34:50,394 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:34:50,606 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:34:50,612 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:34:50,618 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:34:51,222 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214000] due to args.save_total_limit\n",
            "{'loss': 1.965, 'learning_rate': 4.7564002857633774e-05, 'epoch': 0.97}\n",
            "  5% 215500/4423240 [5:40:03<87:11:26, 13.41it/s][INFO|trainer.py:1937] 2021-09-13 15:35:37,173 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:35:37,187 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:35:37,368 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:35:37,374 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:35:37,379 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:35:37,988 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-214500] due to args.save_total_limit\n",
            "{'loss': 1.9069, 'learning_rate': 4.755835089210624e-05, 'epoch': 0.98}\n",
            "  5% 216000/4423240 [5:40:49<118:53:38,  9.83it/s][INFO|trainer.py:1937] 2021-09-13 15:36:23,420 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:36:23,427 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:36:23,590 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:36:23,601 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:36:23,623 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:36:24,611 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215000] due to args.save_total_limit\n",
            "{'loss': 1.9014, 'learning_rate': 4.755269892657871e-05, 'epoch': 0.98}\n",
            "  5% 216500/4423240 [5:41:38<88:19:41, 13.23it/s][INFO|trainer.py:1937] 2021-09-13 15:37:12,026 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:37:12,032 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:37:12,213 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:37:12,220 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:37:12,225 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:37:12,817 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-215500] due to args.save_total_limit\n",
            "{'loss': 1.9035, 'learning_rate': 4.754704696105118e-05, 'epoch': 0.98}\n",
            "  5% 217000/4423240 [5:42:25<114:58:37, 10.16it/s][INFO|trainer.py:1937] 2021-09-13 15:37:59,070 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:37:59,082 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:37:59,271 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:37:59,277 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:37:59,283 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:37:59,925 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216000] due to args.save_total_limit\n",
            "{'loss': 2.0283, 'learning_rate': 4.7541394995523646e-05, 'epoch': 0.98}\n",
            "  5% 217500/4423240 [5:43:13<97:56:45, 11.93it/s][INFO|trainer.py:1937] 2021-09-13 15:38:47,478 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:38:47,487 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:38:47,667 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:38:47,673 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:38:47,679 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:38:48,315 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-216500] due to args.save_total_limit\n",
            "{'loss': 1.7994, 'learning_rate': 4.753574302999612e-05, 'epoch': 0.99}\n",
            "  5% 218000/4423240 [5:44:00<151:27:42,  7.71it/s][INFO|trainer.py:1937] 2021-09-13 15:39:34,028 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:39:34,035 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:39:34,205 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:39:34,210 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:39:34,233 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:39:34,841 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217000] due to args.save_total_limit\n",
            "{'loss': 1.9405, 'learning_rate': 4.753009106446858e-05, 'epoch': 0.99}\n",
            "  5% 218500/4423240 [5:44:47<86:00:41, 13.58it/s][INFO|trainer.py:1937] 2021-09-13 15:40:21,417 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:40:21,423 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:40:21,590 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:40:21,596 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:40:21,601 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:40:22,221 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-217500] due to args.save_total_limit\n",
            "{'loss': 1.8234, 'learning_rate': 4.7524439098941046e-05, 'epoch': 0.99}\n",
            "  5% 219000/4423240 [5:45:33<139:27:06,  8.37it/s][INFO|trainer.py:1937] 2021-09-13 15:41:07,008 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:41:07,015 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:41:07,188 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:41:07,194 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:41:07,199 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:41:08,192 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218000] due to args.save_total_limit\n",
            "{'loss': 1.8523, 'learning_rate': 4.751878713341352e-05, 'epoch': 0.99}\n",
            "  5% 219500/4423240 [5:46:20<102:21:24, 11.41it/s][INFO|trainer.py:1937] 2021-09-13 15:41:54,056 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:41:54,062 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:41:54,242 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:41:54,248 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:41:54,252 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:41:54,840 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-218500] due to args.save_total_limit\n",
            "{'loss': 1.8186, 'learning_rate': 4.751313516788599e-05, 'epoch': 0.99}\n",
            "  5% 220000/4423240 [5:47:06<98:56:54, 11.80it/s][INFO|trainer.py:1937] 2021-09-13 15:42:40,528 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:42:40,536 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:42:40,735 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:42:40,741 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:42:40,746 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:42:41,351 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219000] due to args.save_total_limit\n",
            "{'loss': 1.8458, 'learning_rate': 4.7507483202358454e-05, 'epoch': 1.0}\n",
            "  5% 220500/4423240 [5:47:53<122:01:41,  9.57it/s][INFO|trainer.py:1937] 2021-09-13 15:43:27,515 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:43:27,526 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:43:27,751 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:43:27,757 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:43:27,762 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:43:28,329 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-219500] due to args.save_total_limit\n",
            "{'loss': 1.886, 'learning_rate': 4.7501831236830925e-05, 'epoch': 1.0}\n",
            "  5% 221000/4423240 [5:48:41<122:17:03,  9.55it/s][INFO|trainer.py:1937] 2021-09-13 15:44:14,985 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:44:14,992 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:44:15,177 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:44:15,182 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:44:15,188 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:44:15,819 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220000] due to args.save_total_limit\n",
            "{'loss': 1.9648, 'learning_rate': 4.749617927130339e-05, 'epoch': 1.0}\n",
            "  5% 221500/4423240 [5:49:30<116:06:32, 10.05it/s][INFO|trainer.py:1937] 2021-09-13 15:45:03,890 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:45:03,897 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:45:04,099 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:45:04,105 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:45:04,110 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:45:05,163 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-220500] due to args.save_total_limit\n",
            "{'loss': 1.8861, 'learning_rate': 4.749052730577586e-05, 'epoch': 1.0}\n",
            "  5% 222000/4423240 [5:50:18<86:18:27, 13.52it/s][INFO|trainer.py:1937] 2021-09-13 15:45:52,661 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:45:52,670 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:45:52,844 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:45:52,850 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:45:52,855 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:45:53,483 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221000] due to args.save_total_limit\n",
            "{'loss': 1.841, 'learning_rate': 4.7484875340248325e-05, 'epoch': 1.01}\n",
            "  5% 222500/4423240 [5:51:06<109:46:01, 10.63it/s][INFO|trainer.py:1937] 2021-09-13 15:46:39,836 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:46:39,842 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:46:40,031 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:46:40,037 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:46:40,042 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:46:40,680 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-221500] due to args.save_total_limit\n",
            "{'loss': 2.0121, 'learning_rate': 4.74792233747208e-05, 'epoch': 1.01}\n",
            "  5% 223000/4423240 [5:51:56<103:47:49, 11.24it/s][INFO|trainer.py:1937] 2021-09-13 15:47:30,362 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:47:30,375 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:47:30,559 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:47:30,566 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:47:30,571 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:47:31,191 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222000] due to args.save_total_limit\n",
            "{'loss': 1.9189, 'learning_rate': 4.747357140919326e-05, 'epoch': 1.01}\n",
            "  5% 223500/4423240 [5:52:43<107:58:15, 10.80it/s][INFO|trainer.py:1937] 2021-09-13 15:48:17,212 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:48:17,220 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:48:17,412 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:48:17,419 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:48:17,424 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:48:18,063 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-222500] due to args.save_total_limit\n",
            "{'loss': 1.9209, 'learning_rate': 4.746791944366573e-05, 'epoch': 1.01}\n",
            "  5% 224000/4423240 [5:53:28<98:46:21, 11.81it/s][INFO|trainer.py:1937] 2021-09-13 15:49:02,303 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:49:02,309 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:49:02,475 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:49:02,481 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:49:02,486 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:49:03,145 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223000] due to args.save_total_limit\n",
            "{'loss': 1.9057, 'learning_rate': 4.74622674781382e-05, 'epoch': 1.02}\n",
            "  5% 224500/4423240 [5:54:18<102:38:14, 11.36it/s][INFO|trainer.py:1937] 2021-09-13 15:49:51,952 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:49:51,960 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:49:52,137 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:49:52,143 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:49:52,149 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:49:53,127 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-223500] due to args.save_total_limit\n",
            "{'loss': 1.8734, 'learning_rate': 4.745661551261067e-05, 'epoch': 1.02}\n",
            "  5% 225000/4423240 [5:55:06<88:56:54, 13.11it/s][INFO|trainer.py:1937] 2021-09-13 15:50:39,996 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:50:40,003 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:50:40,174 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:50:40,179 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:50:40,184 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:50:40,782 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224000] due to args.save_total_limit\n",
            "{'loss': 1.8098, 'learning_rate': 4.745096354708314e-05, 'epoch': 1.02}\n",
            "  5% 225500/4423240 [5:55:54<90:20:29, 12.91it/s][INFO|trainer.py:1937] 2021-09-13 15:51:28,214 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:51:28,221 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:51:28,424 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:51:28,430 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:51:28,440 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:51:29,043 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-224500] due to args.save_total_limit\n",
            "{'loss': 1.8988, 'learning_rate': 4.7445311581555604e-05, 'epoch': 1.02}\n",
            "  5% 226000/4423240 [5:56:41<126:28:50,  9.22it/s][INFO|trainer.py:1937] 2021-09-13 15:52:15,507 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:52:15,513 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:52:15,727 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:52:15,733 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:52:15,738 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:52:16,329 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225000] due to args.save_total_limit\n",
            "{'loss': 1.942, 'learning_rate': 4.743965961602807e-05, 'epoch': 1.02}\n",
            "  5% 226500/4423240 [5:57:27<114:01:47, 10.22it/s][INFO|trainer.py:1937] 2021-09-13 15:53:01,711 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:53:01,733 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:53:01,909 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:53:01,915 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:53:01,920 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:53:02,566 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-225500] due to args.save_total_limit\n",
            "{'loss': 1.8286, 'learning_rate': 4.743400765050054e-05, 'epoch': 1.03}\n",
            "  5% 227000/4423240 [5:58:15<132:08:26,  8.82it/s][INFO|trainer.py:1937] 2021-09-13 15:53:49,132 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:53:49,138 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:53:49,317 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:53:49,323 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:53:49,327 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:53:49,959 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226000] due to args.save_total_limit\n",
            "{'loss': 1.7912, 'learning_rate': 4.7428355684973005e-05, 'epoch': 1.03}\n",
            "  5% 227500/4423240 [5:59:00<91:07:41, 12.79it/s][INFO|trainer.py:1937] 2021-09-13 15:54:34,457 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:54:34,467 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:54:34,681 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:54:34,687 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:54:34,693 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:54:35,644 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-226500] due to args.save_total_limit\n",
            "{'loss': 1.8061, 'learning_rate': 4.7422703719445476e-05, 'epoch': 1.03}\n",
            "  5% 228000/4423240 [5:59:47<89:46:10, 12.98it/s][INFO|trainer.py:1937] 2021-09-13 15:55:21,086 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:55:21,092 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:55:21,261 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:55:21,266 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:55:21,270 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:55:21,867 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227000] due to args.save_total_limit\n",
            "{'loss': 1.9184, 'learning_rate': 4.741705175391795e-05, 'epoch': 1.03}\n",
            "  5% 228500/4423240 [6:00:37<113:54:53, 10.23it/s][INFO|trainer.py:1937] 2021-09-13 15:56:10,811 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:56:10,818 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:56:10,982 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:56:10,987 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:56:10,992 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:56:11,587 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-227500] due to args.save_total_limit\n",
            "{'loss': 1.7797, 'learning_rate': 4.741139978839041e-05, 'epoch': 1.04}\n",
            "  5% 229000/4423240 [6:01:22<94:03:51, 12.39it/s][INFO|trainer.py:1937] 2021-09-13 15:56:56,323 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:56:56,329 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:56:56,505 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:56:56,531 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:56:56,537 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:56:57,168 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228000] due to args.save_total_limit\n",
            "{'loss': 1.9365, 'learning_rate': 4.7405747822862883e-05, 'epoch': 1.04}\n",
            "  5% 229500/4423240 [6:02:09<157:32:29,  7.39it/s][INFO|trainer.py:1937] 2021-09-13 15:57:42,988 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:57:42,994 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:57:43,171 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:57:43,178 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:57:43,182 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:57:43,802 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-228500] due to args.save_total_limit\n",
            "{'loss': 1.8564, 'learning_rate': 4.740009585733535e-05, 'epoch': 1.04}\n",
            "  5% 230000/4423240 [6:02:56<95:44:45, 12.17it/s][INFO|trainer.py:1937] 2021-09-13 15:58:30,455 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:58:30,463 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:58:30,659 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:58:30,665 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:58:30,670 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:58:31,664 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229000] due to args.save_total_limit\n",
            "{'loss': 1.8543, 'learning_rate': 4.739444389180781e-05, 'epoch': 1.04}\n",
            "  5% 230500/4423240 [6:03:41<137:14:06,  8.49it/s][INFO|trainer.py:1937] 2021-09-13 15:59:15,715 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 15:59:15,721 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 15:59:15,907 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 15:59:15,913 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 15:59:15,918 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 15:59:16,555 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-229500] due to args.save_total_limit\n",
            "{'loss': 1.8255, 'learning_rate': 4.7388791926280284e-05, 'epoch': 1.04}\n",
            "  5% 231000/4423240 [6:04:29<109:02:06, 10.68it/s][INFO|trainer.py:1937] 2021-09-13 16:00:03,615 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:00:03,622 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:00:03,806 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:00:03,812 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:00:03,817 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:00:04,399 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230000] due to args.save_total_limit\n",
            "{'loss': 1.932, 'learning_rate': 4.7383139960752755e-05, 'epoch': 1.05}\n",
            "  5% 231500/4423240 [6:05:17<103:31:11, 11.25it/s][INFO|trainer.py:1937] 2021-09-13 16:00:51,309 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:00:51,331 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:00:51,514 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:00:51,521 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:00:51,526 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:00:52,137 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-230500] due to args.save_total_limit\n",
            "{'loss': 1.8811, 'learning_rate': 4.737748799522522e-05, 'epoch': 1.05}\n",
            "  5% 232000/4423240 [6:06:05<102:20:30, 11.38it/s][INFO|trainer.py:1937] 2021-09-13 16:01:39,153 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:01:39,160 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:01:39,337 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:01:39,342 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:01:39,347 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:01:40,087 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231000] due to args.save_total_limit\n",
            "{'loss': 1.9197, 'learning_rate': 4.737183602969769e-05, 'epoch': 1.05}\n",
            "  5% 232500/4423240 [6:06:52<115:44:13, 10.06it/s][INFO|trainer.py:1937] 2021-09-13 16:02:26,640 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:02:26,648 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:02:26,834 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:02:26,839 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:02:26,844 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:02:27,858 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-231500] due to args.save_total_limit\n",
            "{'loss': 1.8559, 'learning_rate': 4.736618406417016e-05, 'epoch': 1.05}\n",
            "  5% 233000/4423240 [6:07:40<124:19:24,  9.36it/s][INFO|trainer.py:1937] 2021-09-13 16:03:14,447 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:03:14,453 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:03:14,656 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:03:14,662 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:03:14,667 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:03:15,353 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232000] due to args.save_total_limit\n",
            "{'loss': 1.9301, 'learning_rate': 4.736053209864263e-05, 'epoch': 1.06}\n",
            "  5% 233500/4423240 [6:08:29<160:58:31,  7.23it/s][INFO|trainer.py:1937] 2021-09-13 16:04:03,774 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:04:03,781 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:04:03,941 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:04:03,947 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:04:03,951 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:04:04,544 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-232500] due to args.save_total_limit\n",
            "{'loss': 1.8116, 'learning_rate': 4.735488013311509e-05, 'epoch': 1.06}\n",
            "  5% 234000/4423240 [6:09:17<90:31:01, 12.86it/s][INFO|trainer.py:1937] 2021-09-13 16:04:51,109 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:04:51,115 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:04:51,281 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:04:51,290 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:04:51,295 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:04:51,918 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233000] due to args.save_total_limit\n",
            "{'loss': 1.8431, 'learning_rate': 4.734922816758756e-05, 'epoch': 1.06}\n",
            "  5% 234500/4423240 [6:10:03<99:40:14, 11.67it/s][INFO|trainer.py:1937] 2021-09-13 16:05:37,476 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:05:37,483 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:05:37,666 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:05:37,691 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:05:37,696 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:05:38,291 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-233500] due to args.save_total_limit\n",
            "{'loss': 1.8784, 'learning_rate': 4.734357620206003e-05, 'epoch': 1.06}\n",
            "  5% 235000/4423240 [6:10:52<205:13:08,  5.67it/s][INFO|trainer.py:1937] 2021-09-13 16:06:26,058 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:06:26,065 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:06:26,281 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:06:26,291 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:06:26,302 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:06:26,982 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234000] due to args.save_total_limit\n",
            "{'loss': 1.8713, 'learning_rate': 4.73379242365325e-05, 'epoch': 1.06}\n",
            "  5% 235500/4423240 [6:11:40<105:00:31, 11.08it/s][INFO|trainer.py:1937] 2021-09-13 16:07:14,588 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:07:14,595 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:07:14,769 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:07:14,775 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:07:14,780 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:07:15,766 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-234500] due to args.save_total_limit\n",
            "{'loss': 1.8533, 'learning_rate': 4.733227227100497e-05, 'epoch': 1.07}\n",
            "  5% 236000/4423240 [6:12:26<121:13:22,  9.59it/s][INFO|trainer.py:1937] 2021-09-13 16:07:59,866 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:07:59,873 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:08:00,069 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:08:00,076 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:08:00,082 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:08:00,659 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235000] due to args.save_total_limit\n",
            "{'loss': 1.9556, 'learning_rate': 4.7326620305477435e-05, 'epoch': 1.07}\n",
            "  5% 236500/4423240 [6:13:14<100:23:02, 11.59it/s][INFO|trainer.py:1937] 2021-09-13 16:08:48,065 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:08:48,072 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:08:48,243 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:08:48,249 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:08:48,254 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:08:48,833 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-235500] due to args.save_total_limit\n",
            "{'loss': 1.8791, 'learning_rate': 4.7320968339949906e-05, 'epoch': 1.07}\n",
            "  5% 237000/4423240 [6:13:59<100:09:46, 11.61it/s][INFO|trainer.py:1937] 2021-09-13 16:09:32,924 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:09:32,931 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:09:33,145 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:09:33,151 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:09:33,156 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:09:33,751 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236000] due to args.save_total_limit\n",
            "{'loss': 1.829, 'learning_rate': 4.731531637442237e-05, 'epoch': 1.07}\n",
            "  5% 237500/4423240 [6:14:45<121:05:22,  9.60it/s][INFO|trainer.py:1937] 2021-09-13 16:10:19,021 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:10:19,030 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:10:19,192 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:10:19,198 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:10:19,203 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:10:19,829 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-236500] due to args.save_total_limit\n",
            "{'loss': 1.8847, 'learning_rate': 4.7309664408894835e-05, 'epoch': 1.08}\n",
            "  5% 238000/4423240 [6:15:31<93:32:58, 12.43it/s][INFO|trainer.py:1937] 2021-09-13 16:11:05,403 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:11:05,410 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:11:05,589 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:11:05,595 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:11:05,599 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:11:06,590 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237000] due to args.save_total_limit\n",
            "{'loss': 1.8198, 'learning_rate': 4.730401244336731e-05, 'epoch': 1.08}\n",
            "  5% 238500/4423240 [6:16:17<97:11:17, 11.96it/s][INFO|trainer.py:1937] 2021-09-13 16:11:51,419 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:11:51,426 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:11:51,617 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:11:51,625 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:11:51,630 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:11:52,251 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-237500] due to args.save_total_limit\n",
            "{'loss': 1.915, 'learning_rate': 4.729836047783978e-05, 'epoch': 1.08}\n",
            "  5% 239000/4423240 [6:17:05<97:49:16, 11.88it/s][INFO|trainer.py:1937] 2021-09-13 16:12:38,990 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:12:38,997 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:12:39,171 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:12:39,176 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:12:39,181 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:12:39,782 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238000] due to args.save_total_limit\n",
            "{'loss': 1.8473, 'learning_rate': 4.729270851231224e-05, 'epoch': 1.08}\n",
            "  5% 239500/4423240 [6:17:51<112:49:11, 10.30it/s][INFO|trainer.py:1937] 2021-09-13 16:13:25,452 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:13:25,462 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:13:25,650 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:13:25,657 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:13:25,663 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:13:26,317 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-238500] due to args.save_total_limit\n",
            "{'loss': 1.832, 'learning_rate': 4.7287056546784714e-05, 'epoch': 1.09}\n",
            "  5% 240000/4423240 [6:18:37<82:07:41, 14.15it/s][INFO|trainer.py:1937] 2021-09-13 16:14:11,084 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:14:11,091 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:14:11,283 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:14:11,290 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:14:11,314 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:14:11,930 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239000] due to args.save_total_limit\n",
            "{'loss': 1.8312, 'learning_rate': 4.7281404581257185e-05, 'epoch': 1.09}\n",
            "  5% 240500/4423240 [6:19:24<109:26:44, 10.62it/s][INFO|trainer.py:1937] 2021-09-13 16:14:57,913 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:14:57,918 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:14:58,106 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:14:58,111 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:14:58,115 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:14:58,721 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-239500] due to args.save_total_limit\n",
            "{'loss': 1.8881, 'learning_rate': 4.727575261572964e-05, 'epoch': 1.09}\n",
            "  5% 241000/4423240 [6:20:09<111:08:48, 10.45it/s][INFO|trainer.py:1937] 2021-09-13 16:15:43,754 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:15:43,762 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:15:43,966 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:15:43,972 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:15:43,976 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:15:44,957 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240000] due to args.save_total_limit\n",
            "{'loss': 1.9117, 'learning_rate': 4.7270100650202114e-05, 'epoch': 1.09}\n",
            "  5% 241500/4423240 [6:20:58<109:52:20, 10.57it/s][INFO|trainer.py:1937] 2021-09-13 16:16:32,330 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:16:32,337 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:16:32,513 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:16:32,518 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:16:32,522 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:16:33,148 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-240500] due to args.save_total_limit\n",
            "{'loss': 1.9358, 'learning_rate': 4.7264448684674586e-05, 'epoch': 1.09}\n",
            "  5% 242000/4423240 [6:21:45<101:38:10, 11.43it/s][INFO|trainer.py:1937] 2021-09-13 16:17:19,609 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:17:19,616 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:17:19,801 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:17:19,806 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:17:19,811 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:17:20,403 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241000] due to args.save_total_limit\n",
            "{'loss': 1.9619, 'learning_rate': 4.725879671914705e-05, 'epoch': 1.1}\n",
            "  5% 242500/4423240 [6:22:33<88:06:55, 13.18it/s][INFO|trainer.py:1937] 2021-09-13 16:18:06,854 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:18:06,861 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:18:07,076 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:18:07,080 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:18:07,085 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:18:07,698 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-241500] due to args.save_total_limit\n",
            "{'loss': 1.9729, 'learning_rate': 4.725314475361952e-05, 'epoch': 1.1}\n",
            "  5% 243000/4423240 [6:23:22<90:16:18, 12.86it/s][INFO|trainer.py:1937] 2021-09-13 16:18:55,931 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:18:55,952 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:18:56,143 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:18:56,148 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:18:56,156 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:18:56,774 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242000] due to args.save_total_limit\n",
            "{'loss': 1.8681, 'learning_rate': 4.724749278809199e-05, 'epoch': 1.1}\n",
            "  6% 243500/4423240 [6:24:09<201:05:55,  5.77it/s][INFO|trainer.py:1937] 2021-09-13 16:19:43,532 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:19:43,538 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:19:43,711 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:19:43,717 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:19:43,722 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:19:44,714 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-242500] due to args.save_total_limit\n",
            "{'loss': 1.8827, 'learning_rate': 4.724184082256446e-05, 'epoch': 1.1}\n",
            "  6% 244000/4423240 [6:24:57<102:22:12, 11.34it/s][INFO|trainer.py:1937] 2021-09-13 16:20:31,762 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:20:31,769 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:20:31,942 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:20:31,955 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:20:31,960 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:20:32,588 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243000] due to args.save_total_limit\n",
            "{'loss': 1.8456, 'learning_rate': 4.723618885703692e-05, 'epoch': 1.11}\n",
            "  6% 244500/4423240 [6:25:43<101:37:21, 11.42it/s][INFO|trainer.py:1937] 2021-09-13 16:21:17,744 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:21:17,751 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:21:17,929 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:21:17,936 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:21:17,941 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:21:18,570 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-243500] due to args.save_total_limit\n",
            "{'loss': 1.8268, 'learning_rate': 4.723053689150939e-05, 'epoch': 1.11}\n",
            "  6% 245000/4423240 [6:26:31<130:47:16,  8.87it/s][INFO|trainer.py:1937] 2021-09-13 16:22:05,635 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:22:05,643 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:22:05,824 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:22:05,830 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:22:05,835 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:22:06,572 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244000] due to args.save_total_limit\n",
            "{'loss': 1.9486, 'learning_rate': 4.722488492598186e-05, 'epoch': 1.11}\n",
            "  6% 245500/4423240 [6:27:19<144:56:27,  8.01it/s][INFO|trainer.py:1937] 2021-09-13 16:22:53,443 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:22:53,451 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:22:53,628 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:22:53,634 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:22:53,639 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:22:54,262 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-244500] due to args.save_total_limit\n",
            "{'loss': 1.8453, 'learning_rate': 4.721923296045433e-05, 'epoch': 1.11}\n",
            "  6% 246000/4423240 [6:28:07<128:21:15,  9.04it/s][INFO|trainer.py:1937] 2021-09-13 16:23:41,217 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:23:41,224 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:23:41,405 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:23:41,410 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:23:41,415 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:23:42,020 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245000] due to args.save_total_limit\n",
            "{'loss': 1.9333, 'learning_rate': 4.72135809949268e-05, 'epoch': 1.11}\n",
            "  6% 246500/4423240 [6:28:55<112:20:11, 10.33it/s][INFO|trainer.py:1937] 2021-09-13 16:24:29,527 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:24:29,534 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:24:29,710 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:24:29,716 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:24:29,721 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:24:30,704 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-245500] due to args.save_total_limit\n",
            "{'loss': 1.8597, 'learning_rate': 4.7207929029399265e-05, 'epoch': 1.12}\n",
            "  6% 247000/4423240 [6:29:44<113:19:23, 10.24it/s][INFO|trainer.py:1937] 2021-09-13 16:25:18,426 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:25:18,434 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:25:18,607 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:25:18,613 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:25:18,619 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:25:19,237 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246000] due to args.save_total_limit\n",
            "{'loss': 1.85, 'learning_rate': 4.7202277063871737e-05, 'epoch': 1.12}\n",
            "  6% 247500/4423240 [6:30:33<110:41:22, 10.48it/s][INFO|trainer.py:1937] 2021-09-13 16:26:06,918 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:26:06,925 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:26:07,112 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:26:07,118 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:26:07,128 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:26:07,757 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-246500] due to args.save_total_limit\n",
            "{'loss': 2.0345, 'learning_rate': 4.719662509834421e-05, 'epoch': 1.12}\n",
            "  6% 248000/4423240 [6:31:22<100:52:33, 11.50it/s][INFO|trainer.py:1937] 2021-09-13 16:26:56,559 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:26:56,566 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:26:56,726 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:26:56,749 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:26:56,754 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:26:57,358 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247000] due to args.save_total_limit\n",
            "{'loss': 1.8967, 'learning_rate': 4.7190973132816666e-05, 'epoch': 1.12}\n",
            "  6% 248500/4423240 [6:32:07<101:37:07, 11.41it/s][INFO|trainer.py:1937] 2021-09-13 16:27:41,581 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:27:41,589 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:27:41,761 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:27:41,767 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:27:41,771 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:27:42,405 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-247500] due to args.save_total_limit\n",
            "{'loss': 1.8524, 'learning_rate': 4.718532116728914e-05, 'epoch': 1.13}\n",
            "  6% 249000/4423240 [6:32:52<85:51:23, 13.51it/s][INFO|trainer.py:1937] 2021-09-13 16:28:26,356 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:28:26,363 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:28:26,549 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:28:26,555 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:28:26,560 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:28:27,552 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248000] due to args.save_total_limit\n",
            "{'loss': 1.8521, 'learning_rate': 4.717966920176161e-05, 'epoch': 1.13}\n",
            "  6% 249500/4423240 [6:33:38<84:44:56, 13.68it/s][INFO|trainer.py:1937] 2021-09-13 16:29:12,574 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:29:12,582 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:29:12,778 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:29:12,784 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:29:12,789 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:29:13,472 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-248500] due to args.save_total_limit\n",
            "{'loss': 1.8919, 'learning_rate': 4.717401723623407e-05, 'epoch': 1.13}\n",
            "  6% 250000/4423240 [6:34:28<112:41:05, 10.29it/s][INFO|trainer.py:1937] 2021-09-13 16:30:02,719 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:30:02,726 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:30:02,932 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:30:02,939 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:30:02,944 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:30:03,599 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249000] due to args.save_total_limit\n",
            "{'loss': 1.876, 'learning_rate': 4.7168365270706544e-05, 'epoch': 1.13}\n",
            "  6% 250500/4423240 [6:35:16<100:25:25, 11.54it/s][INFO|trainer.py:1937] 2021-09-13 16:30:49,904 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:30:49,923 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:30:50,107 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:30:50,117 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:30:50,121 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:30:50,744 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-249500] due to args.save_total_limit\n",
            "{'loss': 1.924, 'learning_rate': 4.7162713305179016e-05, 'epoch': 1.13}\n",
            "  6% 251000/4423240 [6:36:04<100:04:53, 11.58it/s][INFO|trainer.py:1937] 2021-09-13 16:31:38,139 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:31:38,146 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:31:38,338 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:31:38,345 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:31:38,351 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:31:38,982 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250000] due to args.save_total_limit\n",
            "{'loss': 1.7825, 'learning_rate': 4.715706133965148e-05, 'epoch': 1.14}\n",
            "  6% 251500/4423240 [6:36:52<99:08:21, 11.69it/s][INFO|trainer.py:1937] 2021-09-13 16:32:26,233 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:32:26,240 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:32:26,426 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:32:26,432 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:32:26,437 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:32:27,130 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-250500] due to args.save_total_limit\n",
            "{'loss': 1.9054, 'learning_rate': 4.7151409374123945e-05, 'epoch': 1.14}\n",
            "  6% 252000/4423240 [6:37:39<100:55:50, 11.48it/s][INFO|trainer.py:1937] 2021-09-13 16:33:13,767 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:33:13,773 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:33:13,955 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:33:13,961 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:33:13,966 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:33:14,912 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251000] due to args.save_total_limit\n",
            "{'loss': 1.8764, 'learning_rate': 4.7145757408596416e-05, 'epoch': 1.14}\n",
            "  6% 252500/4423240 [6:38:28<116:26:08,  9.95it/s][INFO|trainer.py:1937] 2021-09-13 16:34:02,598 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:34:02,605 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:34:02,792 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:34:02,798 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:34:02,803 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:34:03,400 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-251500] due to args.save_total_limit\n",
            "{'loss': 1.8354, 'learning_rate': 4.714010544306888e-05, 'epoch': 1.14}\n",
            "  6% 253000/4423240 [6:39:14<90:07:52, 12.85it/s][INFO|trainer.py:1937] 2021-09-13 16:34:48,214 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:34:48,220 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:34:48,416 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:34:48,421 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:34:48,426 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:34:49,042 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252000] due to args.save_total_limit\n",
            "{'loss': 1.8991, 'learning_rate': 4.713445347754135e-05, 'epoch': 1.15}\n",
            "  6% 253500/4423240 [6:40:02<98:38:03, 11.74it/s][INFO|trainer.py:1937] 2021-09-13 16:35:36,364 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:35:36,371 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:35:36,560 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:35:36,581 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:35:36,586 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:35:37,186 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-252500] due to args.save_total_limit\n",
            "{'loss': 1.7908, 'learning_rate': 4.712880151201382e-05, 'epoch': 1.15}\n",
            "  6% 254000/4423240 [6:40:48<158:47:00,  7.29it/s][INFO|trainer.py:1937] 2021-09-13 16:36:22,325 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:36:22,332 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:36:22,496 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:36:22,502 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:36:22,506 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:36:23,193 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253000] due to args.save_total_limit\n",
            "{'loss': 1.9169, 'learning_rate': 4.712314954648629e-05, 'epoch': 1.15}\n",
            "  6% 254500/4423240 [6:41:34<93:51:24, 12.34it/s][INFO|trainer.py:1937] 2021-09-13 16:37:08,691 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:37:08,698 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:37:08,880 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:37:08,887 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:37:08,900 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:37:09,720 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-253500] due to args.save_total_limit\n",
            "{'loss': 1.9136, 'learning_rate': 4.711749758095876e-05, 'epoch': 1.15}\n",
            "  6% 255000/4423240 [6:42:22<92:48:17, 12.48it/s][INFO|trainer.py:1937] 2021-09-13 16:37:56,328 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:37:56,335 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:37:56,511 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:37:56,517 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:37:56,522 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:37:57,110 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254000] due to args.save_total_limit\n",
            "{'loss': 1.8703, 'learning_rate': 4.7111845615431224e-05, 'epoch': 1.16}\n",
            "  6% 255500/4423240 [6:43:09<94:33:00, 12.24it/s][INFO|trainer.py:1937] 2021-09-13 16:38:43,025 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:38:43,031 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:38:43,214 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:38:43,219 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:38:43,223 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:38:43,797 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-254500] due to args.save_total_limit\n",
            "{'loss': 1.8683, 'learning_rate': 4.710619364990369e-05, 'epoch': 1.16}\n",
            "  6% 256000/4423240 [6:43:58<106:37:48, 10.86it/s][INFO|trainer.py:1937] 2021-09-13 16:39:32,634 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:39:32,643 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:39:32,813 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:39:32,819 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:39:32,823 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:39:33,385 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255000] due to args.save_total_limit\n",
            "{'loss': 1.9133, 'learning_rate': 4.710054168437616e-05, 'epoch': 1.16}\n",
            "  6% 256500/4423240 [6:44:44<122:20:01,  9.46it/s][INFO|trainer.py:1937] 2021-09-13 16:40:18,608 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:40:18,614 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:40:18,788 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:40:18,798 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:40:18,821 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:40:19,411 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-255500] due to args.save_total_limit\n",
            "{'loss': 1.8549, 'learning_rate': 4.709488971884863e-05, 'epoch': 1.16}\n",
            "  6% 257000/4423240 [6:45:31<99:32:16, 11.63it/s][INFO|trainer.py:1937] 2021-09-13 16:41:05,322 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:41:05,331 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:41:05,523 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:41:05,529 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:41:05,535 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:41:06,241 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256000] due to args.save_total_limit\n",
            "{'loss': 1.9135, 'learning_rate': 4.7089237753321096e-05, 'epoch': 1.16}\n",
            "  6% 257500/4423240 [6:46:21<83:25:10, 13.87it/s][INFO|trainer.py:1937] 2021-09-13 16:41:54,812 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:41:54,819 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:41:54,981 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:41:54,987 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:41:54,991 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:41:55,939 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-256500] due to args.save_total_limit\n",
            "{'loss': 1.9059, 'learning_rate': 4.708358578779357e-05, 'epoch': 1.17}\n",
            "  6% 258000/4423240 [6:47:08<91:00:23, 12.71it/s][INFO|trainer.py:1937] 2021-09-13 16:42:42,411 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:42:42,417 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:42:42,605 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:42:42,610 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:42:42,622 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:42:43,201 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257000] due to args.save_total_limit\n",
            "{'loss': 1.8994, 'learning_rate': 4.707793382226604e-05, 'epoch': 1.17}\n",
            "  6% 258500/4423240 [6:47:57<92:13:36, 12.54it/s][INFO|trainer.py:1937] 2021-09-13 16:43:31,041 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:43:31,048 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:43:31,239 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:43:31,245 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:43:31,250 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:43:31,876 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-257500] due to args.save_total_limit\n",
            "{'loss': 1.794, 'learning_rate': 4.70722818567385e-05, 'epoch': 1.17}\n",
            "  6% 259000/4423240 [6:48:43<104:44:42, 11.04it/s][INFO|trainer.py:1937] 2021-09-13 16:44:17,691 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:44:17,699 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:44:17,873 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:44:17,880 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:44:17,885 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:44:18,514 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258000] due to args.save_total_limit\n",
            "{'loss': 1.8057, 'learning_rate': 4.706662989121097e-05, 'epoch': 1.17}\n",
            "  6% 259500/4423240 [6:49:33<125:47:36,  9.19it/s][INFO|trainer.py:1937] 2021-09-13 16:45:07,079 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:45:07,085 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:45:07,257 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:45:07,263 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:45:07,267 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:45:07,869 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-258500] due to args.save_total_limit\n",
            "{'loss': 1.9325, 'learning_rate': 4.706097792568344e-05, 'epoch': 1.18}\n",
            "  6% 260000/4423240 [6:50:21<109:47:54, 10.53it/s][INFO|trainer.py:1937] 2021-09-13 16:45:55,182 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:45:55,191 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:45:55,373 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:45:55,380 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:45:55,386 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:45:56,373 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259000] due to args.save_total_limit\n",
            "{'loss': 1.9028, 'learning_rate': 4.70553259601559e-05, 'epoch': 1.18}\n",
            "  6% 260500/4423240 [6:51:09<128:50:47,  8.97it/s][INFO|trainer.py:1937] 2021-09-13 16:46:43,763 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:46:43,770 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:46:43,941 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:46:43,947 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:46:43,952 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:46:44,610 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-259500] due to args.save_total_limit\n",
            "{'loss': 1.9854, 'learning_rate': 4.7049673994628375e-05, 'epoch': 1.18}\n",
            "  6% 261000/4423240 [6:52:00<142:07:27,  8.13it/s][INFO|trainer.py:1937] 2021-09-13 16:47:34,098 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:47:34,105 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:47:34,280 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:47:34,290 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:47:34,312 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:47:34,908 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260000] due to args.save_total_limit\n",
            "{'loss': 2.0243, 'learning_rate': 4.7044022029100846e-05, 'epoch': 1.18}\n",
            "  6% 261500/4423240 [6:52:46<120:06:29,  9.62it/s][INFO|trainer.py:1937] 2021-09-13 16:48:20,789 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:48:20,798 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:48:20,993 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:48:20,999 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:48:21,004 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:48:21,574 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-260500] due to args.save_total_limit\n",
            "{'loss': 1.8053, 'learning_rate': 4.703837006357331e-05, 'epoch': 1.18}\n",
            "  6% 262000/4423240 [6:53:33<106:34:20, 10.85it/s][INFO|trainer.py:1937] 2021-09-13 16:49:07,571 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:49:07,577 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:49:07,746 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:49:07,752 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:49:07,757 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:49:08,393 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261000] due to args.save_total_limit\n",
            "{'loss': 1.8099, 'learning_rate': 4.703271809804578e-05, 'epoch': 1.19}\n",
            "  6% 262500/4423240 [6:54:21<106:08:53, 10.89it/s][INFO|trainer.py:1937] 2021-09-13 16:49:55,592 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:49:55,604 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:49:55,806 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:49:55,812 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:49:55,817 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:49:56,442 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-261500] due to args.save_total_limit\n",
            "{'loss': 1.9341, 'learning_rate': 4.7027066132518246e-05, 'epoch': 1.19}\n",
            "  6% 263000/4423240 [6:55:09<87:40:13, 13.18it/s][INFO|trainer.py:1937] 2021-09-13 16:50:43,675 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:50:43,682 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:50:43,869 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:50:43,880 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:50:43,884 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:50:45,052 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262000] due to args.save_total_limit\n",
            "{'loss': 1.9426, 'learning_rate': 4.702141416699071e-05, 'epoch': 1.19}\n",
            "  6% 263500/4423240 [6:55:57<94:39:32, 12.21it/s][INFO|trainer.py:1937] 2021-09-13 16:51:31,014 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:51:31,021 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:51:31,188 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:51:31,194 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:51:31,208 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:51:31,839 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-262500] due to args.save_total_limit\n",
            "{'loss': 1.8842, 'learning_rate': 4.701576220146318e-05, 'epoch': 1.19}\n",
            "  6% 264000/4423240 [6:56:44<94:29:26, 12.23it/s][INFO|trainer.py:1937] 2021-09-13 16:52:18,063 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:52:18,069 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:52:18,255 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:52:18,261 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:52:18,267 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:52:18,901 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263000] due to args.save_total_limit\n",
            "{'loss': 1.8623, 'learning_rate': 4.7010110235935654e-05, 'epoch': 1.2}\n",
            "  6% 264500/4423240 [6:57:30<84:49:16, 13.62it/s][INFO|trainer.py:1937] 2021-09-13 16:53:04,699 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:53:04,707 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:53:04,889 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:53:04,913 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:53:04,920 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:53:05,543 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-263500] due to args.save_total_limit\n",
            "{'loss': 1.7615, 'learning_rate': 4.700445827040812e-05, 'epoch': 1.2}\n",
            "  6% 265000/4423240 [6:58:16<101:04:48, 11.43it/s][INFO|trainer.py:1937] 2021-09-13 16:53:49,813 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:53:49,820 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:53:50,003 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:53:50,010 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:53:50,015 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:53:50,639 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264000] due to args.save_total_limit\n",
            "{'loss': 1.9299, 'learning_rate': 4.699880630488059e-05, 'epoch': 1.2}\n",
            "  6% 265500/4423240 [6:59:06<79:22:40, 14.55it/s][INFO|trainer.py:1937] 2021-09-13 16:54:40,782 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:54:40,790 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:54:40,961 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:54:40,967 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:54:40,972 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:54:41,843 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-264500] due to args.save_total_limit\n",
            "{'loss': 1.7236, 'learning_rate': 4.6993154339353054e-05, 'epoch': 1.2}\n",
            "  6% 266000/4423240 [6:59:52<98:24:13, 11.74it/s][INFO|trainer.py:1937] 2021-09-13 16:55:26,258 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:55:26,265 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:55:26,477 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:55:26,484 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:55:26,489 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:55:27,136 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265000] due to args.save_total_limit\n",
            "{'loss': 1.8497, 'learning_rate': 4.6987502373825525e-05, 'epoch': 1.2}\n",
            "  6% 266500/4423240 [7:00:39<101:58:02, 11.32it/s][INFO|trainer.py:1937] 2021-09-13 16:56:13,231 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:56:13,238 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:56:13,409 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:56:13,415 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:56:13,420 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:56:14,106 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-265500] due to args.save_total_limit\n",
            "{'loss': 1.8923, 'learning_rate': 4.698185040829799e-05, 'epoch': 1.21}\n",
            "  6% 267000/4423240 [7:01:27<96:52:20, 11.92it/s][INFO|trainer.py:1937] 2021-09-13 16:57:01,483 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:57:01,490 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:57:01,717 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:57:01,724 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:57:01,729 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:57:02,350 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266000] due to args.save_total_limit\n",
            "{'loss': 1.857, 'learning_rate': 4.697619844277046e-05, 'epoch': 1.21}\n",
            "  6% 267500/4423240 [7:02:14<96:39:49, 11.94it/s][INFO|trainer.py:1937] 2021-09-13 16:57:48,481 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:57:48,492 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:57:48,671 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:57:48,677 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:57:48,683 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:57:49,333 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-266500] due to args.save_total_limit\n",
            "{'loss': 1.8426, 'learning_rate': 4.6970546477242926e-05, 'epoch': 1.21}\n",
            "  6% 268000/4423240 [7:03:03<140:58:29,  8.19it/s][INFO|trainer.py:1937] 2021-09-13 16:58:37,173 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:58:37,179 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:58:37,365 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:58:37,372 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:58:37,377 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:58:38,043 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267000] due to args.save_total_limit\n",
            "{'loss': 1.8156, 'learning_rate': 4.69648945117154e-05, 'epoch': 1.21}\n",
            "  6% 268500/4423240 [7:03:51<124:13:29,  9.29it/s][INFO|trainer.py:1937] 2021-09-13 16:59:25,542 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 16:59:25,550 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 16:59:25,734 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 16:59:25,741 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 16:59:25,746 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 16:59:26,734 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-267500] due to args.save_total_limit\n",
            "{'loss': 1.9496, 'learning_rate': 4.695924254618786e-05, 'epoch': 1.22}\n",
            "  6% 269000/4423240 [7:04:39<111:22:12, 10.36it/s][INFO|trainer.py:1937] 2021-09-13 17:00:13,594 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:00:13,600 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:00:13,789 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:00:13,796 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:00:13,801 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:00:14,435 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268000] due to args.save_total_limit\n",
            "{'loss': 1.8814, 'learning_rate': 4.695359058066033e-05, 'epoch': 1.22}\n",
            "  6% 269500/4423240 [7:05:28<91:16:16, 12.64it/s][INFO|trainer.py:1937] 2021-09-13 17:01:02,325 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:01:02,332 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:01:02,519 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:01:02,524 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:01:02,530 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:01:03,155 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-268500] due to args.save_total_limit\n",
            "{'loss': 1.8337, 'learning_rate': 4.6947938615132804e-05, 'epoch': 1.22}\n",
            "  6% 270000/4423240 [7:06:15<96:11:04, 11.99it/s][INFO|trainer.py:1937] 2021-09-13 17:01:48,970 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:01:48,977 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:01:49,147 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:01:49,152 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:01:49,178 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:01:49,814 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269000] due to args.save_total_limit\n",
            "{'loss': 1.8333, 'learning_rate': 4.694228664960527e-05, 'epoch': 1.22}\n",
            "  6% 270500/4423240 [7:07:01<95:35:50, 12.07it/s][INFO|trainer.py:1937] 2021-09-13 17:02:35,173 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:02:35,180 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:02:35,368 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:02:35,374 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:02:35,378 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:02:36,057 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-269500] due to args.save_total_limit\n",
            "{'loss': 1.918, 'learning_rate': 4.6936634684077734e-05, 'epoch': 1.23}\n",
            "  6% 271000/4423240 [7:07:50<91:52:24, 12.55it/s][INFO|trainer.py:1937] 2021-09-13 17:03:24,707 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:03:24,716 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:03:24,889 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:03:24,896 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:03:24,900 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:03:25,949 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270000] due to args.save_total_limit\n",
            "{'loss': 1.8726, 'learning_rate': 4.6930982718550205e-05, 'epoch': 1.23}\n",
            "  6% 271500/4423240 [7:08:38<96:58:19, 11.89it/s][INFO|trainer.py:1937] 2021-09-13 17:04:12,171 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:04:12,178 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:04:12,366 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:04:12,375 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:04:12,380 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:04:13,015 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-270500] due to args.save_total_limit\n",
            "{'loss': 1.9243, 'learning_rate': 4.692533075302267e-05, 'epoch': 1.23}\n",
            "  6% 272000/4423240 [7:09:25<87:52:01, 13.12it/s][INFO|trainer.py:1937] 2021-09-13 17:04:59,653 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:04:59,661 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:04:59,860 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:04:59,865 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:04:59,869 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:05:00,507 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271000] due to args.save_total_limit\n",
            "{'loss': 1.8449, 'learning_rate': 4.691967878749514e-05, 'epoch': 1.23}\n",
            "  6% 272500/4423240 [7:10:13<101:24:30, 11.37it/s][INFO|trainer.py:1937] 2021-09-13 17:05:47,087 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:05:47,114 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:05:47,315 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:05:47,321 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:05:47,330 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:05:47,974 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-271500] due to args.save_total_limit\n",
            "{'loss': 1.7851, 'learning_rate': 4.691402682196761e-05, 'epoch': 1.23}\n",
            "  6% 273000/4423240 [7:10:59<100:43:00, 11.45it/s][INFO|trainer.py:1937] 2021-09-13 17:06:33,071 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:06:33,078 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:06:33,260 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:06:33,282 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:06:33,288 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:06:33,875 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272000] due to args.save_total_limit\n",
            "{'loss': 1.8531, 'learning_rate': 4.690837485644008e-05, 'epoch': 1.24}\n",
            "  6% 273500/4423240 [7:11:45<84:54:39, 13.58it/s][INFO|trainer.py:1937] 2021-09-13 17:07:19,209 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:07:19,217 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:07:19,396 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:07:19,401 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:07:19,406 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:07:20,069 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-272500] due to args.save_total_limit\n",
            "{'loss': 1.8068, 'learning_rate': 4.690272289091255e-05, 'epoch': 1.24}\n",
            "  6% 274000/4423240 [7:12:33<110:58:15, 10.39it/s][INFO|trainer.py:1937] 2021-09-13 17:08:07,479 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:08:07,487 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:08:07,679 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:08:07,684 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:08:07,689 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:08:08,566 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273000] due to args.save_total_limit\n",
            "{'loss': 1.8369, 'learning_rate': 4.689707092538501e-05, 'epoch': 1.24}\n",
            "  6% 274500/4423240 [7:13:22<117:58:34,  9.77it/s][INFO|trainer.py:1937] 2021-09-13 17:08:56,191 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:08:56,198 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:08:56,393 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:08:56,406 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:08:56,412 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:08:57,015 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-273500] due to args.save_total_limit\n",
            "{'loss': 1.8574, 'learning_rate': 4.689141895985748e-05, 'epoch': 1.24}\n",
            "  6% 275000/4423240 [7:14:09<93:32:36, 12.32it/s][INFO|trainer.py:1937] 2021-09-13 17:09:43,003 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:09:43,011 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:09:43,182 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:09:43,187 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:09:43,192 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:09:43,950 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274000] due to args.save_total_limit\n",
            "{'loss': 1.8402, 'learning_rate': 4.688576699432995e-05, 'epoch': 1.25}\n",
            "  6% 275500/4423240 [7:14:57<113:39:03, 10.14it/s][INFO|trainer.py:1937] 2021-09-13 17:10:31,194 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:10:31,201 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:10:31,397 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:10:31,404 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:10:31,409 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:10:32,061 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-274500] due to args.save_total_limit\n",
            "{'loss': 1.7121, 'learning_rate': 4.688011502880242e-05, 'epoch': 1.25}\n",
            "  6% 276000/4423240 [7:15:42<84:51:36, 13.58it/s][INFO|trainer.py:1937] 2021-09-13 17:11:16,303 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:11:16,309 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:11:16,499 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:11:16,505 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:11:16,510 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:11:17,129 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275000] due to args.save_total_limit\n",
            "{'loss': 1.9118, 'learning_rate': 4.6874463063274884e-05, 'epoch': 1.25}\n",
            "  6% 276500/4423240 [7:16:28<101:09:09, 11.39it/s][INFO|trainer.py:1937] 2021-09-13 17:12:01,943 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:12:01,956 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:12:02,239 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:12:02,249 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:12:02,253 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:12:03,227 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-275500] due to args.save_total_limit\n",
            "{'loss': 1.8786, 'learning_rate': 4.6868811097747356e-05, 'epoch': 1.25}\n",
            "  6% 277000/4423240 [7:17:16<140:19:27,  8.21it/s][INFO|trainer.py:1937] 2021-09-13 17:12:50,691 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:12:50,709 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:12:50,893 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:12:50,900 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:12:50,910 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:12:51,539 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276000] due to args.save_total_limit\n",
            "{'loss': 1.8666, 'learning_rate': 4.686315913221983e-05, 'epoch': 1.25}\n",
            "  6% 277500/4423240 [7:18:05<101:41:58, 11.32it/s][INFO|trainer.py:1937] 2021-09-13 17:13:39,613 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:13:39,621 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:13:39,782 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:13:39,788 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:13:39,792 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:13:40,405 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-276500] due to args.save_total_limit\n",
            "{'loss': 1.8844, 'learning_rate': 4.685750716669229e-05, 'epoch': 1.26}\n",
            "  6% 278000/4423240 [7:18:53<82:54:43, 13.89it/s][INFO|trainer.py:1937] 2021-09-13 17:14:27,060 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:14:27,068 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:14:27,267 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:14:27,274 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:14:27,290 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:14:27,950 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277000] due to args.save_total_limit\n",
            "{'loss': 1.7762, 'learning_rate': 4.6851855201164756e-05, 'epoch': 1.26}\n",
            "  6% 278500/4423240 [7:19:40<103:54:23, 11.08it/s][INFO|trainer.py:1937] 2021-09-13 17:15:14,357 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:15:14,365 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:15:14,549 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:15:14,554 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:15:14,560 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:15:15,157 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-277500] due to args.save_total_limit\n",
            "{'loss': 1.7536, 'learning_rate': 4.684620323563723e-05, 'epoch': 1.26}\n",
            "  6% 279000/4423240 [7:20:28<148:24:14,  7.76it/s][INFO|trainer.py:1937] 2021-09-13 17:16:01,901 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:16:01,908 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:16:02,091 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:16:02,097 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:16:02,102 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:16:02,693 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278000] due to args.save_total_limit\n",
            "{'loss': 1.7284, 'learning_rate': 4.684055127010969e-05, 'epoch': 1.26}\n",
            "  6% 279500/4423240 [7:21:14<103:56:18, 11.07it/s][INFO|trainer.py:1937] 2021-09-13 17:16:47,861 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:16:47,868 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:16:48,061 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:16:48,067 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:16:48,071 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:16:49,074 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-278500] due to args.save_total_limit\n",
            "{'loss': 1.8791, 'learning_rate': 4.6834899304582164e-05, 'epoch': 1.27}\n",
            "  6% 280000/4423240 [7:22:02<101:44:17, 11.31it/s][INFO|trainer.py:1937] 2021-09-13 17:17:36,613 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:17:36,620 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:17:36,792 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:17:36,798 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:17:36,806 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:17:37,428 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279000] due to args.save_total_limit\n",
            "{'loss': 1.9096, 'learning_rate': 4.6829247339054635e-05, 'epoch': 1.27}\n",
            "  6% 280500/4423240 [7:22:49<102:11:04, 11.26it/s][INFO|trainer.py:1937] 2021-09-13 17:18:22,931 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:18:22,939 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:18:23,131 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:18:23,137 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:18:23,142 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:18:23,759 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-279500] due to args.save_total_limit\n",
            "{'loss': 1.7591, 'learning_rate': 4.68235953735271e-05, 'epoch': 1.27}\n",
            "  6% 281000/4423240 [7:23:32<117:24:49,  9.80it/s][INFO|trainer.py:1937] 2021-09-13 17:19:06,288 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:19:06,296 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:19:06,495 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:19:06,501 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:19:06,525 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:19:07,126 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280000] due to args.save_total_limit\n",
            "{'loss': 1.8045, 'learning_rate': 4.681794340799957e-05, 'epoch': 1.27}\n",
            "  6% 281500/4423240 [7:24:18<93:02:02, 12.37it/s][INFO|trainer.py:1937] 2021-09-13 17:19:52,375 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:19:52,383 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:19:52,575 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:19:52,581 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:19:52,587 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:19:53,226 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-280500] due to args.save_total_limit\n",
            "{'loss': 1.8628, 'learning_rate': 4.6812291442472035e-05, 'epoch': 1.28}\n",
            "  6% 282000/4423240 [7:25:05<95:27:01, 12.05it/s][INFO|trainer.py:1937] 2021-09-13 17:20:39,268 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:20:39,276 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:20:39,482 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:20:39,488 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:20:39,494 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:20:40,515 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281000] due to args.save_total_limit\n",
            "{'loss': 1.8224, 'learning_rate': 4.68066394769445e-05, 'epoch': 1.28}\n",
            "  6% 282500/4423240 [7:25:52<103:04:29, 11.16it/s][INFO|trainer.py:1937] 2021-09-13 17:21:26,629 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:21:26,636 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:21:26,825 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:21:26,830 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:21:26,835 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:21:27,493 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-281500] due to args.save_total_limit\n",
            "{'loss': 1.8308, 'learning_rate': 4.680098751141697e-05, 'epoch': 1.28}\n",
            "  6% 283000/4423240 [7:26:43<108:36:27, 10.59it/s][INFO|trainer.py:1937] 2021-09-13 17:22:17,130 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:22:17,136 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:22:17,315 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:22:17,321 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:22:17,327 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:22:17,925 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282000] due to args.save_total_limit\n",
            "{'loss': 1.8996, 'learning_rate': 4.679533554588944e-05, 'epoch': 1.28}\n",
            "  6% 283500/4423240 [7:27:32<92:50:58, 12.38it/s][INFO|trainer.py:1937] 2021-09-13 17:23:06,735 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:23:06,756 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:23:06,937 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:23:06,943 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:23:06,949 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:23:07,539 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-282500] due to args.save_total_limit\n",
            "{'loss': 1.8868, 'learning_rate': 4.678968358036191e-05, 'epoch': 1.28}\n",
            "  6% 284000/4423240 [7:28:20<115:31:26,  9.95it/s][INFO|trainer.py:1937] 2021-09-13 17:23:54,281 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:23:54,294 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:23:54,536 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:23:54,542 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:23:54,548 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:23:55,186 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283000] due to args.save_total_limit\n",
            "{'loss': 1.7526, 'learning_rate': 4.678403161483438e-05, 'epoch': 1.29}\n",
            "  6% 284500/4423240 [7:29:07<105:26:22, 10.90it/s][INFO|trainer.py:1937] 2021-09-13 17:24:41,729 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:24:41,735 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:24:41,906 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:24:41,912 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:24:41,917 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:24:42,766 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-283500] due to args.save_total_limit\n",
            "{'loss': 1.7507, 'learning_rate': 4.677837964930685e-05, 'epoch': 1.29}\n",
            "  6% 285000/4423240 [7:29:52<166:19:42,  6.91it/s][INFO|trainer.py:1937] 2021-09-13 17:25:26,770 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:25:26,776 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:25:26,994 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:25:27,000 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:25:27,004 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:25:27,967 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284000] due to args.save_total_limit\n",
            "{'loss': 1.8936, 'learning_rate': 4.6772727683779314e-05, 'epoch': 1.29}\n",
            "  6% 285500/4423240 [7:30:41<132:02:47,  8.70it/s][INFO|trainer.py:1937] 2021-09-13 17:26:15,304 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:26:15,311 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:26:15,511 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:26:15,517 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:26:15,539 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:26:16,227 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-284500] due to args.save_total_limit\n",
            "{'loss': 1.957, 'learning_rate': 4.676707571825178e-05, 'epoch': 1.29}\n",
            "  6% 286000/4423240 [7:31:31<96:26:40, 11.92it/s][INFO|trainer.py:1937] 2021-09-13 17:27:04,939 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:27:04,947 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:27:05,134 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:27:05,140 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:27:05,145 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:27:05,757 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285000] due to args.save_total_limit\n",
            "{'loss': 1.8606, 'learning_rate': 4.676142375272425e-05, 'epoch': 1.3}\n",
            "  6% 286500/4423240 [7:32:18<115:37:44,  9.94it/s][INFO|trainer.py:1937] 2021-09-13 17:27:52,760 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:27:52,768 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:27:52,986 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:27:53,010 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:27:53,015 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:27:53,740 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-285500] due to args.save_total_limit\n",
            "{'loss': 1.8664, 'learning_rate': 4.6755771787196715e-05, 'epoch': 1.3}\n",
            "  6% 287000/4423240 [7:33:06<114:41:54, 10.02it/s][INFO|trainer.py:1937] 2021-09-13 17:28:39,969 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:28:39,977 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:28:40,175 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:28:40,181 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:28:40,188 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:28:40,915 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286000] due to args.save_total_limit\n",
            "{'loss': 1.9375, 'learning_rate': 4.6750119821669186e-05, 'epoch': 1.3}\n",
            "  6% 287500/4423240 [7:33:55<97:07:23, 11.83it/s][INFO|trainer.py:1937] 2021-09-13 17:29:29,285 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:29:29,293 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:29:29,502 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:29:29,510 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:29:29,516 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:29:30,526 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-286500] due to args.save_total_limit\n",
            "{'loss': 1.8197, 'learning_rate': 4.674446785614166e-05, 'epoch': 1.3}\n",
            "  7% 288000/4423240 [7:34:44<98:35:23, 11.65it/s][INFO|trainer.py:1937] 2021-09-13 17:30:18,607 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:30:18,615 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:30:18,818 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:30:18,824 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:30:18,829 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:30:19,494 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287000] due to args.save_total_limit\n",
            "{'loss': 1.7738, 'learning_rate': 4.673881589061412e-05, 'epoch': 1.3}\n",
            "  7% 288500/4423240 [7:35:31<99:25:19, 11.55it/s][INFO|trainer.py:1937] 2021-09-13 17:31:04,993 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:31:05,001 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:31:05,208 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:31:05,215 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:31:05,222 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:31:05,944 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-287500] due to args.save_total_limit\n",
            "{'loss': 1.8648, 'learning_rate': 4.6733163925086593e-05, 'epoch': 1.31}\n",
            "  7% 289000/4423240 [7:36:18<101:05:20, 11.36it/s][INFO|trainer.py:1937] 2021-09-13 17:31:52,478 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:31:52,488 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:31:52,699 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:31:52,712 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:31:52,722 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:31:53,413 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288000] due to args.save_total_limit\n",
            "{'loss': 1.9619, 'learning_rate': 4.672751195955906e-05, 'epoch': 1.31}\n",
            "  7% 289500/4423240 [7:37:06<85:22:06, 13.45it/s][INFO|trainer.py:1937] 2021-09-13 17:32:40,509 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:32:40,517 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:32:40,717 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:32:40,723 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:32:40,728 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:32:41,408 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-288500] due to args.save_total_limit\n",
            "{'loss': 1.8608, 'learning_rate': 4.672185999403152e-05, 'epoch': 1.31}\n",
            "  7% 290000/4423240 [7:37:53<121:53:12,  9.42it/s][INFO|trainer.py:1937] 2021-09-13 17:33:27,479 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:33:27,485 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:33:27,678 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:33:27,693 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:33:27,704 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:33:28,781 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289000] due to args.save_total_limit\n",
            "{'loss': 1.8835, 'learning_rate': 4.6716208028503994e-05, 'epoch': 1.31}\n",
            "  7% 290500/4423240 [7:38:39<91:50:02, 12.50it/s][INFO|trainer.py:1937] 2021-09-13 17:34:13,709 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:34:13,718 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:34:13,896 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:34:13,902 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:34:13,908 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:34:14,552 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-289500] due to args.save_total_limit\n",
            "{'loss': 1.8842, 'learning_rate': 4.6710556062976465e-05, 'epoch': 1.32}\n",
            "  7% 291000/4423240 [7:39:29<104:16:05, 11.01it/s][INFO|trainer.py:1937] 2021-09-13 17:35:03,645 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:35:03,653 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:35:03,847 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:35:03,852 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:35:03,857 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:35:04,532 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290000] due to args.save_total_limit\n",
            "{'loss': 1.8798, 'learning_rate': 4.670490409744893e-05, 'epoch': 1.32}\n",
            "  7% 291500/4423240 [7:40:16<118:58:15,  9.65it/s][INFO|trainer.py:1937] 2021-09-13 17:35:50,375 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:35:50,385 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:35:50,579 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:35:50,591 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:35:50,600 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:35:51,219 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-290500] due to args.save_total_limit\n",
            "{'loss': 1.9247, 'learning_rate': 4.66992521319214e-05, 'epoch': 1.32}\n",
            "  7% 292000/4423240 [7:41:04<97:05:44, 11.82it/s][INFO|trainer.py:1937] 2021-09-13 17:36:38,172 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:36:38,179 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:36:38,369 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:36:38,375 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:36:38,398 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:36:39,059 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291000] due to args.save_total_limit\n",
            "{'loss': 1.8104, 'learning_rate': 4.669360016639387e-05, 'epoch': 1.32}\n",
            "  7% 292500/4423240 [7:41:51<126:20:36,  9.08it/s][INFO|trainer.py:1937] 2021-09-13 17:37:25,369 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:37:25,376 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:37:25,549 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:37:25,556 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:37:25,562 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:37:26,216 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-291500] due to args.save_total_limit\n",
            "{'loss': 1.8244, 'learning_rate': 4.668794820086634e-05, 'epoch': 1.32}\n",
            "  7% 293000/4423240 [7:42:38<105:54:18, 10.83it/s][INFO|trainer.py:1937] 2021-09-13 17:38:12,773 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:38:12,779 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:38:12,961 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:38:12,967 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:38:12,977 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:38:13,935 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292000] due to args.save_total_limit\n",
            "{'loss': 1.8473, 'learning_rate': 4.66822962353388e-05, 'epoch': 1.33}\n",
            "  7% 293500/4423240 [7:43:26<113:06:04, 10.14it/s][INFO|trainer.py:1937] 2021-09-13 17:39:00,461 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:39:00,468 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:39:00,633 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:39:00,639 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:39:00,644 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:39:01,295 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-292500] due to args.save_total_limit\n",
            "{'loss': 1.836, 'learning_rate': 4.667664426981127e-05, 'epoch': 1.33}\n",
            "  7% 294000/4423240 [7:44:13<89:31:04, 12.81it/s][INFO|trainer.py:1937] 2021-09-13 17:39:47,269 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:39:47,280 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:39:47,511 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:39:47,525 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:39:47,533 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:39:48,331 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293000] due to args.save_total_limit\n",
            "{'loss': 1.8271, 'learning_rate': 4.667099230428374e-05, 'epoch': 1.33}\n",
            "  7% 294500/4423240 [7:45:00<116:15:32,  9.86it/s][INFO|trainer.py:1937] 2021-09-13 17:40:33,968 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:40:33,977 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:40:34,175 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:40:34,181 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:40:34,186 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:40:34,804 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-293500] due to args.save_total_limit\n",
            "{'loss': 1.852, 'learning_rate': 4.666534033875621e-05, 'epoch': 1.33}\n",
            "  7% 295000/4423240 [7:45:47<94:39:13, 12.12it/s][INFO|trainer.py:1937] 2021-09-13 17:41:21,694 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:41:21,702 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:41:21,896 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:41:21,903 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:41:21,908 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:41:22,561 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294000] due to args.save_total_limit\n",
            "{'loss': 1.7809, 'learning_rate': 4.665968837322868e-05, 'epoch': 1.34}\n",
            "  7% 295500/4423240 [7:46:35<120:06:18,  9.55it/s][INFO|trainer.py:1937] 2021-09-13 17:42:09,665 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:42:09,673 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:42:09,860 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:42:09,866 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:42:09,871 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:42:10,856 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-294500] due to args.save_total_limit\n",
            "{'loss': 1.8639, 'learning_rate': 4.6654036407701145e-05, 'epoch': 1.34}\n",
            "  7% 296000/4423240 [7:47:25<121:31:08,  9.43it/s][INFO|trainer.py:1937] 2021-09-13 17:42:59,324 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:42:59,331 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:42:59,516 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:42:59,528 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:42:59,537 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:43:00,176 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295000] due to args.save_total_limit\n",
            "{'loss': 1.8382, 'learning_rate': 4.6648384442173616e-05, 'epoch': 1.34}\n",
            "  7% 296500/4423240 [7:48:14<99:54:20, 11.47it/s][INFO|trainer.py:1937] 2021-09-13 17:43:48,382 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:43:48,390 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:43:48,567 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:43:48,573 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:43:48,579 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:43:49,199 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-295500] due to args.save_total_limit\n",
            "{'loss': 1.8225, 'learning_rate': 4.664273247664608e-05, 'epoch': 1.34}\n",
            "  7% 297000/4423240 [7:49:00<116:19:40,  9.85it/s][INFO|trainer.py:1937] 2021-09-13 17:44:34,671 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:44:34,680 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:44:34,850 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:44:34,857 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:44:34,862 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:44:35,501 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296000] due to args.save_total_limit\n",
            "{'loss': 1.8226, 'learning_rate': 4.6637080511118545e-05, 'epoch': 1.35}\n",
            "  7% 297500/4423240 [7:49:47<90:37:12, 12.65it/s][INFO|trainer.py:1937] 2021-09-13 17:45:21,426 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:45:21,434 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:45:21,622 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:45:21,627 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:45:21,649 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:45:22,198 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-296500] due to args.save_total_limit\n",
            "{'loss': 1.8393, 'learning_rate': 4.6631428545591017e-05, 'epoch': 1.35}\n",
            "  7% 298000/4423240 [7:50:34<93:33:16, 12.25it/s][INFO|trainer.py:1937] 2021-09-13 17:46:07,955 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:46:07,961 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:46:08,163 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:46:08,169 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:46:08,174 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:46:08,822 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297000] due to args.save_total_limit\n",
            "{'loss': 1.8643, 'learning_rate': 4.662577658006349e-05, 'epoch': 1.35}\n",
            "  7% 298500/4423240 [7:51:23<86:02:05, 13.32it/s][INFO|trainer.py:1937] 2021-09-13 17:46:56,816 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:46:56,823 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:46:57,005 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:46:57,014 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:46:57,023 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:46:58,010 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-297500] due to args.save_total_limit\n",
            "{'loss': 1.8543, 'learning_rate': 4.662012461453595e-05, 'epoch': 1.35}\n",
            "  7% 299000/4423240 [7:52:10<83:16:11, 13.76it/s][INFO|trainer.py:1937] 2021-09-13 17:47:44,106 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:47:44,114 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:47:44,290 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:47:44,296 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:47:44,301 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:47:44,919 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298000] due to args.save_total_limit\n",
            "{'loss': 1.7832, 'learning_rate': 4.6614472649008424e-05, 'epoch': 1.35}\n",
            "  7% 299500/4423240 [7:52:57<89:04:24, 12.86it/s][INFO|trainer.py:1937] 2021-09-13 17:48:31,771 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:48:31,778 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:48:31,970 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:48:31,976 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:48:31,981 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:48:32,623 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-298500] due to args.save_total_limit\n",
            "{'loss': 1.9739, 'learning_rate': 4.660882068348089e-05, 'epoch': 1.36}\n",
            "  7% 300000/4423240 [7:53:45<101:18:14, 11.31it/s][INFO|trainer.py:1937] 2021-09-13 17:49:19,670 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:49:19,677 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:49:19,885 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:49:19,891 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:49:19,897 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:49:20,502 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299000] due to args.save_total_limit\n",
            "{'loss': 1.7654, 'learning_rate': 4.660316871795335e-05, 'epoch': 1.36}\n",
            "  7% 300500/4423240 [7:54:31<121:34:26,  9.42it/s][INFO|trainer.py:1937] 2021-09-13 17:50:05,680 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:50:05,687 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:50:05,881 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:50:05,887 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:50:05,893 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:50:06,560 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-299500] due to args.save_total_limit\n",
            "{'loss': 1.7266, 'learning_rate': 4.6597516752425824e-05, 'epoch': 1.36}\n",
            "  7% 301000/4423240 [7:55:18<115:43:58,  9.89it/s][INFO|trainer.py:1937] 2021-09-13 17:50:52,473 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:50:52,480 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:50:52,656 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:50:52,664 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:50:52,684 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:50:53,533 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300000] due to args.save_total_limit\n",
            "{'loss': 1.8429, 'learning_rate': 4.6591864786898296e-05, 'epoch': 1.36}\n",
            "  7% 301500/4423240 [7:56:07<99:27:41, 11.51it/s][INFO|trainer.py:1937] 2021-09-13 17:51:41,173 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:51:41,180 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:51:41,368 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:51:41,375 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:51:41,380 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:51:42,041 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-300500] due to args.save_total_limit\n",
            "{'loss': 1.7907, 'learning_rate': 4.658621282137076e-05, 'epoch': 1.37}\n",
            "  7% 302000/4423240 [7:56:57<121:21:25,  9.43it/s][INFO|trainer.py:1937] 2021-09-13 17:52:31,388 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:52:31,396 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:52:31,581 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:52:31,588 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:52:31,593 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:52:32,254 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301000] due to args.save_total_limit\n",
            "{'loss': 1.752, 'learning_rate': 4.658056085584323e-05, 'epoch': 1.37}\n",
            "  7% 302500/4423240 [7:57:44<99:35:44, 11.49it/s][INFO|trainer.py:1937] 2021-09-13 17:53:17,950 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:53:17,971 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:53:18,169 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:53:18,193 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:53:18,202 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:53:18,973 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-301500] due to args.save_total_limit\n",
            "{'loss': 1.872, 'learning_rate': 4.6574908890315696e-05, 'epoch': 1.37}\n",
            "  7% 303000/4423240 [7:58:30<86:47:11, 13.19it/s][INFO|trainer.py:1937] 2021-09-13 17:54:04,007 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:54:04,016 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:54:04,233 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:54:04,240 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:54:04,246 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:54:04,918 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302000] due to args.save_total_limit\n",
            "{'loss': 1.8837, 'learning_rate': 4.656925692478817e-05, 'epoch': 1.37}\n",
            "  7% 303500/4423240 [7:59:17<97:32:29, 11.73it/s][INFO|trainer.py:1937] 2021-09-13 17:54:51,657 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:54:51,664 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:54:51,858 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:54:51,864 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:54:51,869 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:54:52,504 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-302500] due to args.save_total_limit\n",
            "{'loss': 1.798, 'learning_rate': 4.656360495926064e-05, 'epoch': 1.37}\n",
            "  7% 304000/4423240 [8:00:06<89:40:28, 12.76it/s][INFO|trainer.py:1937] 2021-09-13 17:55:39,894 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:55:39,902 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:55:40,085 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:55:40,092 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:55:40,097 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:55:41,029 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303000] due to args.save_total_limit\n",
            "{'loss': 1.829, 'learning_rate': 4.65579529937331e-05, 'epoch': 1.38}\n",
            "  7% 304500/4423240 [8:00:54<85:54:57, 13.32it/s][INFO|trainer.py:1937] 2021-09-13 17:56:28,256 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:56:28,264 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:56:28,462 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:56:28,469 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:56:28,475 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:56:29,094 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-303500] due to args.save_total_limit\n",
            "{'loss': 1.8619, 'learning_rate': 4.655230102820557e-05, 'epoch': 1.38}\n",
            "  7% 305000/4423240 [8:01:42<159:11:48,  7.19it/s][INFO|trainer.py:1937] 2021-09-13 17:57:16,499 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:57:16,508 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:57:16,683 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:57:16,702 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:57:16,707 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:57:17,357 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304000] due to args.save_total_limit\n",
            "{'loss': 1.7579, 'learning_rate': 4.654664906267804e-05, 'epoch': 1.38}\n",
            "  7% 305500/4423240 [8:02:29<96:14:31, 11.88it/s][INFO|trainer.py:1937] 2021-09-13 17:58:03,282 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:58:03,288 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:58:03,477 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:58:03,498 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:58:03,504 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:58:04,142 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-304500] due to args.save_total_limit\n",
            "{'loss': 1.7508, 'learning_rate': 4.654099709715051e-05, 'epoch': 1.38}\n",
            "  7% 306000/4423240 [8:03:14<103:35:24, 11.04it/s][INFO|trainer.py:1937] 2021-09-13 17:58:48,024 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:58:48,031 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:58:48,206 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:58:48,211 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:58:48,216 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:58:48,876 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305000] due to args.save_total_limit\n",
            "{'loss': 1.7422, 'learning_rate': 4.6535345131622975e-05, 'epoch': 1.39}\n",
            "  7% 306500/4423240 [8:04:00<108:37:33, 10.53it/s][INFO|trainer.py:1937] 2021-09-13 17:59:34,447 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 17:59:34,454 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 17:59:34,652 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 17:59:34,659 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 17:59:34,665 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 17:59:35,776 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-305500] due to args.save_total_limit\n",
            "{'loss': 1.8518, 'learning_rate': 4.6529693166095446e-05, 'epoch': 1.39}\n",
            "  7% 307000/4423240 [8:04:49<133:36:21,  8.56it/s][INFO|trainer.py:1937] 2021-09-13 18:00:22,888 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:00:22,896 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:00:23,073 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:00:23,080 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:00:23,085 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:00:23,740 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306000] due to args.save_total_limit\n",
            "{'loss': 1.7967, 'learning_rate': 4.652404120056791e-05, 'epoch': 1.39}\n",
            "  7% 307500/4423240 [8:05:34<83:42:51, 13.66it/s][INFO|trainer.py:1937] 2021-09-13 18:01:08,225 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:01:08,231 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:01:08,444 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:01:08,450 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:01:08,456 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:01:09,115 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-306500] due to args.save_total_limit\n",
            "{'loss': 1.9177, 'learning_rate': 4.6518389235040376e-05, 'epoch': 1.39}\n",
            "  7% 308000/4423240 [8:06:25<93:14:16, 12.26it/s][INFO|trainer.py:1937] 2021-09-13 18:01:59,796 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:01:59,821 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:02:00,025 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:02:00,031 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:02:00,036 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:02:00,682 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307000] due to args.save_total_limit\n",
            "{'loss': 1.7394, 'learning_rate': 4.651273726951285e-05, 'epoch': 1.39}\n",
            "  7% 308500/4423240 [8:07:13<117:11:22,  9.75it/s][INFO|trainer.py:1937] 2021-09-13 18:02:47,367 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:02:47,375 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:02:47,565 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:02:47,572 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:02:47,578 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:02:48,248 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-307500] due to args.save_total_limit\n",
            "{'loss': 1.8028, 'learning_rate': 4.650708530398532e-05, 'epoch': 1.4}\n",
            "  7% 309000/4423240 [8:08:01<88:41:25, 12.89it/s][INFO|trainer.py:1937] 2021-09-13 18:03:35,452 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:03:35,464 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:03:35,670 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:03:35,675 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:03:35,680 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:03:36,315 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308000] due to args.save_total_limit\n",
            "{'loss': 1.7232, 'learning_rate': 4.650143333845778e-05, 'epoch': 1.4}\n",
            "  7% 309500/4423240 [8:08:50<142:04:00,  8.04it/s][INFO|trainer.py:1937] 2021-09-13 18:04:24,411 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:04:24,419 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:04:24,625 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:04:24,632 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:04:24,637 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:04:25,827 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-308500] due to args.save_total_limit\n",
            "{'loss': 1.7532, 'learning_rate': 4.6495781372930254e-05, 'epoch': 1.4}\n",
            "  7% 310000/4423240 [8:09:37<91:16:24, 12.52it/s][INFO|trainer.py:1937] 2021-09-13 18:05:11,205 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:05:11,211 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:05:11,409 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:05:11,414 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:05:11,419 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:05:12,058 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309000] due to args.save_total_limit\n",
            "{'loss': 1.8413, 'learning_rate': 4.649012940740272e-05, 'epoch': 1.4}\n",
            "  7% 310500/4423240 [8:10:25<105:51:51, 10.79it/s][INFO|trainer.py:1937] 2021-09-13 18:05:59,350 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:05:59,357 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:05:59,529 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:05:59,535 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:05:59,540 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:06:00,208 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-309500] due to args.save_total_limit\n",
            "{'loss': 1.8046, 'learning_rate': 4.648447744187519e-05, 'epoch': 1.41}\n",
            "  7% 311000/4423240 [8:11:13<105:36:07, 10.82it/s][INFO|trainer.py:1937] 2021-09-13 18:06:46,846 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:06:46,867 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:06:47,053 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:06:47,059 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:06:47,064 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:06:47,713 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310000] due to args.save_total_limit\n",
            "{'loss': 1.8295, 'learning_rate': 4.647882547634766e-05, 'epoch': 1.41}\n",
            "  7% 311500/4423240 [8:12:00<122:44:21,  9.31it/s][INFO|trainer.py:1937] 2021-09-13 18:07:34,450 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:07:34,458 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:07:34,639 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:07:34,645 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:07:34,650 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:07:35,309 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-310500] due to args.save_total_limit\n",
            "{'loss': 1.862, 'learning_rate': 4.6473173510820126e-05, 'epoch': 1.41}\n",
            "  7% 312000/4423240 [8:12:51<107:22:52, 10.64it/s][INFO|trainer.py:1937] 2021-09-13 18:08:25,231 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:08:25,238 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:08:25,433 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:08:25,440 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:08:25,446 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:08:26,113 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311000] due to args.save_total_limit\n",
            "{'loss': 1.9156, 'learning_rate': 4.646752154529259e-05, 'epoch': 1.41}\n",
            "  7% 312500/4423240 [8:13:43<105:32:03, 10.82it/s][INFO|trainer.py:1937] 2021-09-13 18:09:17,585 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:09:17,593 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:09:17,767 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:09:17,773 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:09:17,777 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:09:18,375 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-311500] due to args.save_total_limit\n",
            "{'loss': 1.7561, 'learning_rate': 4.646186957976506e-05, 'epoch': 1.42}\n",
            "  7% 313000/4423240 [8:14:32<96:10:26, 11.87it/s][INFO|trainer.py:1937] 2021-09-13 18:10:06,760 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:10:06,766 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:10:06,952 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:10:06,958 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:10:06,964 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:10:07,560 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312000] due to args.save_total_limit\n",
            "{'loss': 1.8412, 'learning_rate': 4.6456217614237526e-05, 'epoch': 1.42}\n",
            "  7% 313500/4423240 [8:15:23<100:07:51, 11.40it/s][INFO|trainer.py:1937] 2021-09-13 18:10:57,047 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:10:57,057 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:10:57,247 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:10:57,252 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:10:57,257 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:10:57,879 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-312500] due to args.save_total_limit\n",
            "{'loss': 1.7913, 'learning_rate': 4.645056564871e-05, 'epoch': 1.42}\n",
            "  7% 314000/4423240 [8:16:10<85:55:07, 13.29it/s][INFO|trainer.py:1937] 2021-09-13 18:11:44,524 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:11:44,533 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:11:44,795 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:11:44,804 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:11:44,827 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:11:45,726 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313000] due to args.save_total_limit\n",
            "{'loss': 1.851, 'learning_rate': 4.644491368318247e-05, 'epoch': 1.42}\n",
            "  7% 314500/4423240 [8:16:56<122:08:43,  9.34it/s][INFO|trainer.py:1937] 2021-09-13 18:12:30,243 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:12:30,252 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:12:30,502 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:12:30,508 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:12:30,512 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:12:31,205 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-313500] due to args.save_total_limit\n",
            "{'loss': 1.7672, 'learning_rate': 4.6439261717654934e-05, 'epoch': 1.42}\n",
            "  7% 315000/4423240 [8:17:44<108:17:24, 10.54it/s][INFO|trainer.py:1937] 2021-09-13 18:13:17,988 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:13:17,998 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:13:18,191 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:13:18,198 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:13:18,203 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:13:19,218 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314000] due to args.save_total_limit\n",
            "{'loss': 1.7933, 'learning_rate': 4.64336097521274e-05, 'epoch': 1.43}\n",
            "  7% 315500/4423240 [8:18:31<104:41:56, 10.90it/s][INFO|trainer.py:1937] 2021-09-13 18:14:05,800 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:14:05,808 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:14:05,996 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:14:06,002 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:14:06,007 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:14:06,648 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-314500] due to args.save_total_limit\n",
            "{'loss': 1.7029, 'learning_rate': 4.642795778659987e-05, 'epoch': 1.43}\n",
            "  7% 316000/4423240 [8:19:20<92:53:08, 12.28it/s][INFO|trainer.py:1937] 2021-09-13 18:14:53,870 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:14:53,879 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:14:54,062 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:14:54,080 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:14:54,085 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:14:54,728 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315000] due to args.save_total_limit\n",
            "{'loss': 1.8716, 'learning_rate': 4.6422305821072334e-05, 'epoch': 1.43}\n",
            "  7% 316500/4423240 [8:20:08<80:09:07, 14.23it/s][INFO|trainer.py:1937] 2021-09-13 18:15:41,863 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:15:41,871 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:15:42,099 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:15:42,105 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:15:42,110 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:15:42,784 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-315500] due to args.save_total_limit\n",
            "{'loss': 1.8051, 'learning_rate': 4.6416653855544805e-05, 'epoch': 1.43}\n",
            "  7% 317000/4423240 [8:20:57<105:44:28, 10.79it/s][INFO|trainer.py:1937] 2021-09-13 18:16:30,844 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:16:30,852 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:16:31,040 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:16:31,045 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:16:31,050 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:16:31,703 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316000] due to args.save_total_limit\n",
            "{'loss': 1.8908, 'learning_rate': 4.641100189001728e-05, 'epoch': 1.44}\n",
            "  7% 317500/4423240 [8:21:46<108:58:05, 10.47it/s][INFO|trainer.py:1937] 2021-09-13 18:17:20,645 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:17:20,654 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:17:20,870 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:17:20,880 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:17:20,885 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:17:21,735 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-316500] due to args.save_total_limit\n",
            "{'loss': 1.7692, 'learning_rate': 4.640534992448974e-05, 'epoch': 1.44}\n",
            "  7% 318000/4423240 [8:22:36<137:08:54,  8.31it/s][INFO|trainer.py:1937] 2021-09-13 18:18:09,938 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:18:09,944 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:18:10,124 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:18:10,129 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:18:10,134 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:18:11,132 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317000] due to args.save_total_limit\n",
            "{'loss': 1.8122, 'learning_rate': 4.639969795896221e-05, 'epoch': 1.44}\n",
            "  7% 318500/4423240 [8:23:27<149:54:09,  7.61it/s][INFO|trainer.py:1937] 2021-09-13 18:19:01,364 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:19:01,372 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:19:01,547 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:19:01,554 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:19:01,566 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:19:02,241 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-317500] due to args.save_total_limit\n",
            "{'loss': 1.9113, 'learning_rate': 4.6394045993434684e-05, 'epoch': 1.44}\n",
            "  7% 319000/4423240 [8:24:17<103:05:47, 11.06it/s][INFO|trainer.py:1937] 2021-09-13 18:19:51,002 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:19:51,011 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:19:51,195 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:19:51,202 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:19:51,207 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:19:51,816 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318000] due to args.save_total_limit\n",
            "{'loss': 1.6489, 'learning_rate': 4.638839402790714e-05, 'epoch': 1.44}\n",
            "  7% 319500/4423240 [8:25:06<138:59:29,  8.20it/s][INFO|trainer.py:1937] 2021-09-13 18:20:40,470 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:20:40,478 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:20:40,678 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:20:40,705 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:20:40,710 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:20:41,360 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-318500] due to args.save_total_limit\n",
            "{'loss': 1.7653, 'learning_rate': 4.638274206237961e-05, 'epoch': 1.45}\n",
            "  7% 320000/4423240 [8:25:54<85:00:55, 13.41it/s][INFO|trainer.py:1937] 2021-09-13 18:21:28,300 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:21:28,311 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:21:28,543 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:21:28,550 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:21:28,555 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:21:29,427 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319000] due to args.save_total_limit\n",
            "{'loss': 1.8606, 'learning_rate': 4.6377090096852085e-05, 'epoch': 1.45}\n",
            "  7% 320500/4423240 [8:26:43<137:24:53,  8.29it/s][INFO|trainer.py:1937] 2021-09-13 18:22:17,363 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:22:17,373 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:22:17,551 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:22:17,557 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:22:17,563 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:22:18,585 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-319500] due to args.save_total_limit\n",
            "{'loss': 1.7598, 'learning_rate': 4.637143813132455e-05, 'epoch': 1.45}\n",
            "  7% 321000/4423240 [8:27:32<105:29:42, 10.80it/s][INFO|trainer.py:1937] 2021-09-13 18:23:06,564 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:23:06,572 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:23:06,757 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:23:06,762 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:23:06,767 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:23:07,409 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320000] due to args.save_total_limit\n",
            "{'loss': 1.7739, 'learning_rate': 4.636578616579702e-05, 'epoch': 1.45}\n",
            "  7% 321500/4423240 [8:28:20<106:01:11, 10.75it/s][INFO|trainer.py:1937] 2021-09-13 18:23:54,346 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:23:54,355 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:23:54,541 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:23:54,548 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:23:54,567 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:23:55,240 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-320500] due to args.save_total_limit\n",
            "{'loss': 1.7125, 'learning_rate': 4.636013420026949e-05, 'epoch': 1.46}\n",
            "  7% 322000/4423240 [8:29:09<101:58:52, 11.17it/s][INFO|trainer.py:1937] 2021-09-13 18:24:43,206 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:24:43,227 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:24:43,411 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:24:43,417 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:24:43,435 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:24:44,125 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321000] due to args.save_total_limit\n",
            "{'loss': 1.8182, 'learning_rate': 4.6354482234741956e-05, 'epoch': 1.46}\n",
            "  7% 322500/4423240 [8:29:57<148:23:21,  7.68it/s][INFO|trainer.py:1937] 2021-09-13 18:25:31,217 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:25:31,225 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:25:31,411 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:25:31,419 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:25:31,424 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:25:32,065 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-321500] due to args.save_total_limit\n",
            "{'loss': 1.7946, 'learning_rate': 4.634883026921442e-05, 'epoch': 1.46}\n",
            "  7% 323000/4423240 [8:30:46<102:52:19, 11.07it/s][INFO|trainer.py:1937] 2021-09-13 18:26:20,360 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:26:20,367 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:26:20,536 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:26:20,542 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:26:20,547 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:26:21,518 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322000] due to args.save_total_limit\n",
            "{'loss': 1.752, 'learning_rate': 4.634317830368689e-05, 'epoch': 1.46}\n",
            "  7% 323500/4423240 [8:31:34<90:36:10, 12.57it/s][INFO|trainer.py:1937] 2021-09-13 18:27:08,175 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:27:08,183 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:27:08,370 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:27:08,377 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:27:08,382 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:27:09,020 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-322500] due to args.save_total_limit\n",
            "{'loss': 1.7306, 'learning_rate': 4.633752633815936e-05, 'epoch': 1.46}\n",
            "  7% 324000/4423240 [8:32:20<81:13:18, 14.02it/s][INFO|trainer.py:1937] 2021-09-13 18:27:54,790 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:27:54,796 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:27:54,985 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:27:54,989 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:27:54,994 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:27:55,632 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323000] due to args.save_total_limit\n",
            "{'loss': 1.7552, 'learning_rate': 4.633187437263183e-05, 'epoch': 1.47}\n",
            "  7% 324500/4423240 [8:33:09<95:30:38, 11.92it/s][INFO|trainer.py:1937] 2021-09-13 18:28:43,572 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:28:43,580 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:28:43,787 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:28:43,792 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:28:43,797 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:28:44,485 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-323500] due to args.save_total_limit\n",
            "{'loss': 1.7964, 'learning_rate': 4.63262224071043e-05, 'epoch': 1.47}\n",
            "  7% 325000/4423240 [8:33:58<129:58:59,  8.76it/s][INFO|trainer.py:1937] 2021-09-13 18:29:32,304 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:29:32,312 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:29:32,495 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:29:32,519 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:29:32,526 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:29:33,148 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324000] due to args.save_total_limit\n",
            "{'loss': 1.7958, 'learning_rate': 4.6320570441576764e-05, 'epoch': 1.47}\n",
            "  7% 325500/4423240 [8:34:47<143:23:02,  7.94it/s][INFO|trainer.py:1937] 2021-09-13 18:30:21,552 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:30:21,559 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:30:21,763 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:30:21,769 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:30:21,775 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:30:22,415 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-324500] due to args.save_total_limit\n",
            "{'loss': 1.6567, 'learning_rate': 4.6314918476049235e-05, 'epoch': 1.47}\n",
            "  7% 326000/4423240 [8:35:37<102:50:30, 11.07it/s][INFO|trainer.py:1937] 2021-09-13 18:31:11,024 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:31:11,031 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:31:11,223 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:31:11,229 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:31:11,235 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:31:12,215 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325000] due to args.save_total_limit\n",
            "{'loss': 1.8386, 'learning_rate': 4.630926651052171e-05, 'epoch': 1.48}\n",
            "  7% 326500/4423240 [8:36:23<87:21:01, 13.03it/s][INFO|trainer.py:1937] 2021-09-13 18:31:57,603 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:31:57,611 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:31:57,789 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:31:57,794 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:31:57,802 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:31:58,413 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-325500] due to args.save_total_limit\n",
            "{'loss': 1.873, 'learning_rate': 4.6303614544994164e-05, 'epoch': 1.48}\n",
            "  7% 327000/4423240 [8:37:15<139:44:39,  8.14it/s][INFO|trainer.py:1937] 2021-09-13 18:32:48,847 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:32:48,855 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:32:49,049 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:32:49,059 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:32:49,069 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:32:49,709 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326000] due to args.save_total_limit\n",
            "{'loss': 1.8346, 'learning_rate': 4.6297962579466636e-05, 'epoch': 1.48}\n",
            "  7% 327500/4423240 [8:38:03<143:15:15,  7.94it/s][INFO|trainer.py:1937] 2021-09-13 18:33:37,449 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:33:37,473 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:33:37,651 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:33:37,658 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:33:37,663 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:33:38,259 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-326500] due to args.save_total_limit\n",
            "{'loss': 1.7883, 'learning_rate': 4.629231061393911e-05, 'epoch': 1.48}\n",
            "  7% 328000/4423240 [8:38:52<115:28:18,  9.85it/s][INFO|trainer.py:1937] 2021-09-13 18:34:26,603 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:34:26,611 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:34:26,787 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:34:26,793 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:34:26,798 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:34:27,436 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327000] due to args.save_total_limit\n",
            "{'loss': 1.796, 'learning_rate': 4.628665864841157e-05, 'epoch': 1.49}\n",
            "  7% 328500/4423240 [8:39:42<102:38:34, 11.08it/s][INFO|trainer.py:1937] 2021-09-13 18:35:16,781 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:35:16,789 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:35:16,961 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:35:16,967 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:35:16,972 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:35:17,976 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-327500] due to args.save_total_limit\n",
            "{'loss': 1.8154, 'learning_rate': 4.628100668288404e-05, 'epoch': 1.49}\n",
            "  7% 329000/4423240 [8:40:32<106:06:28, 10.72it/s][INFO|trainer.py:1937] 2021-09-13 18:36:06,519 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:36:06,526 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:36:06,701 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:36:06,707 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:36:06,729 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:36:07,387 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328000] due to args.save_total_limit\n",
            "{'loss': 1.754, 'learning_rate': 4.6275354717356514e-05, 'epoch': 1.49}\n",
            "  7% 329500/4423240 [8:41:21<106:02:04, 10.72it/s][INFO|trainer.py:1937] 2021-09-13 18:36:55,020 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:36:55,028 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:36:55,202 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:36:55,208 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:36:55,213 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:36:55,901 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-328500] due to args.save_total_limit\n",
            "{'loss': 1.9326, 'learning_rate': 4.626970275182898e-05, 'epoch': 1.49}\n",
            "  7% 330000/4423240 [8:42:11<106:57:33, 10.63it/s][INFO|trainer.py:1937] 2021-09-13 18:37:45,554 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:37:45,562 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:37:45,753 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:37:45,761 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:37:45,767 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:37:46,424 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329000] due to args.save_total_limit\n",
            "{'loss': 1.8137, 'learning_rate': 4.6264050786301444e-05, 'epoch': 1.49}\n",
            "  7% 330500/4423240 [8:42:58<106:37:17, 10.66it/s][INFO|trainer.py:1937] 2021-09-13 18:38:32,665 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:38:32,675 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:38:32,945 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:38:32,970 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:38:32,975 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:38:33,807 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-329500] due to args.save_total_limit\n",
            "{'loss': 1.7574, 'learning_rate': 4.6258398820773915e-05, 'epoch': 1.5}\n",
            "  7% 331000/4423240 [8:43:46<94:31:28, 12.03it/s][INFO|trainer.py:1937] 2021-09-13 18:39:20,742 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:39:20,751 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:39:20,925 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:39:20,932 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:39:20,937 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:39:21,596 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330000] due to args.save_total_limit\n",
            "{'loss': 1.7435, 'learning_rate': 4.625274685524638e-05, 'epoch': 1.5}\n",
            "  7% 331500/4423240 [8:44:35<97:26:41, 11.66it/s][INFO|trainer.py:1937] 2021-09-13 18:40:09,224 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:40:09,232 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:40:09,423 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:40:09,430 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:40:09,436 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:40:10,240 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-330500] due to args.save_total_limit\n",
            "{'loss': 1.8524, 'learning_rate': 4.624709488971885e-05, 'epoch': 1.5}\n",
            "  8% 332000/4423240 [8:45:24<134:34:00,  8.45it/s][INFO|trainer.py:1937] 2021-09-13 18:40:57,838 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:40:57,845 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:40:58,064 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:40:58,070 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:40:58,075 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:40:58,709 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331000] due to args.save_total_limit\n",
            "{'loss': 1.74, 'learning_rate': 4.624144292419132e-05, 'epoch': 1.5}\n",
            "  8% 332500/4423240 [8:46:13<93:55:21, 12.10it/s][INFO|trainer.py:1937] 2021-09-13 18:41:46,887 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:41:46,896 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:41:47,089 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:41:47,094 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:41:47,099 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:41:47,733 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-331500] due to args.save_total_limit\n",
            "{'loss': 1.7692, 'learning_rate': 4.623579095866379e-05, 'epoch': 1.51}\n",
            "  8% 333000/4423240 [8:47:01<91:50:23, 12.37it/s][INFO|trainer.py:1937] 2021-09-13 18:42:34,983 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:42:34,997 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:42:35,187 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:42:35,193 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:42:35,198 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:42:35,846 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332000] due to args.save_total_limit\n",
            "{'loss': 1.6832, 'learning_rate': 4.623013899313626e-05, 'epoch': 1.51}\n",
            "  8% 333500/4423240 [8:47:47<98:59:11, 11.48it/s][INFO|trainer.py:1937] 2021-09-13 18:43:20,853 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:43:20,862 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:43:21,052 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:43:21,058 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:43:21,082 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:43:21,727 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-332500] due to args.save_total_limit\n",
            "{'loss': 1.8086, 'learning_rate': 4.622448702760873e-05, 'epoch': 1.51}\n",
            "  8% 334000/4423240 [8:48:36<149:16:53,  7.61it/s][INFO|trainer.py:1937] 2021-09-13 18:44:10,786 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:44:10,792 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:44:10,979 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:44:10,985 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:44:10,990 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:44:11,735 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333000] due to args.save_total_limit\n",
            "{'loss': 1.7772, 'learning_rate': 4.621883506208119e-05, 'epoch': 1.51}\n",
            "  8% 334500/4423240 [8:49:26<99:40:09, 11.40it/s][INFO|trainer.py:1937] 2021-09-13 18:45:00,578 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:45:00,585 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:45:00,771 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:45:00,778 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:45:00,783 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:45:01,760 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-333500] due to args.save_total_limit\n",
            "{'loss': 1.7931, 'learning_rate': 4.621318309655366e-05, 'epoch': 1.51}\n",
            "  8% 335000/4423240 [8:50:16<117:00:47,  9.71it/s][INFO|trainer.py:1937] 2021-09-13 18:45:50,146 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:45:50,152 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:45:50,340 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:45:50,347 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:45:50,353 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:45:51,011 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334000] due to args.save_total_limit\n",
            "{'loss': 1.8755, 'learning_rate': 4.620753113102613e-05, 'epoch': 1.52}\n",
            "  8% 335500/4423240 [8:51:06<108:07:32, 10.50it/s][INFO|trainer.py:1937] 2021-09-13 18:46:40,555 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:46:40,562 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:46:40,759 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:46:40,766 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:46:40,771 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:46:41,427 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-334500] due to args.save_total_limit\n",
            "{'loss': 1.7531, 'learning_rate': 4.6201879165498594e-05, 'epoch': 1.52}\n",
            "  8% 336000/4423240 [8:51:56<147:05:40,  7.72it/s][INFO|trainer.py:1937] 2021-09-13 18:47:30,761 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:47:30,773 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:47:30,971 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:47:30,994 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:47:30,999 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:47:31,839 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335000] due to args.save_total_limit\n",
            "{'loss': 1.808, 'learning_rate': 4.6196227199971066e-05, 'epoch': 1.52}\n",
            "  8% 336500/4423240 [8:52:45<104:01:13, 10.91it/s][INFO|trainer.py:1937] 2021-09-13 18:48:19,488 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:48:19,496 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:48:19,676 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:48:19,683 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:48:19,688 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:48:20,320 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-335500] due to args.save_total_limit\n",
            "{'loss': 1.898, 'learning_rate': 4.619057523444354e-05, 'epoch': 1.52}\n",
            "  8% 337000/4423240 [8:53:36<92:37:19, 12.25it/s][INFO|trainer.py:1937] 2021-09-13 18:49:10,615 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:49:10,623 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:49:10,822 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:49:10,828 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:49:10,834 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:49:11,443 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336000] due to args.save_total_limit\n",
            "{'loss': 1.8691, 'learning_rate': 4.6184923268916e-05, 'epoch': 1.53}\n",
            "  8% 337500/4423240 [8:54:27<82:40:13, 13.73it/s][INFO|trainer.py:1937] 2021-09-13 18:50:01,211 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:50:01,220 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:50:01,417 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:50:01,435 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:50:01,447 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:50:02,079 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-336500] due to args.save_total_limit\n",
            "{'loss': 1.8299, 'learning_rate': 4.6179271303388466e-05, 'epoch': 1.53}\n",
            "  8% 338000/4423240 [8:55:17<104:50:31, 10.82it/s][INFO|trainer.py:1937] 2021-09-13 18:50:51,464 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:50:51,474 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:50:51,709 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:50:51,715 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:50:51,720 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:50:52,708 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337000] due to args.save_total_limit\n",
            "{'loss': 1.759, 'learning_rate': 4.617361933786094e-05, 'epoch': 1.53}\n",
            "  8% 338500/4423240 [8:56:07<92:20:50, 12.29it/s][INFO|trainer.py:1937] 2021-09-13 18:51:41,536 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:51:41,544 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:51:41,737 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:51:41,743 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:51:41,747 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:51:42,360 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-337500] due to args.save_total_limit\n",
            "{'loss': 1.7863, 'learning_rate': 4.61679673723334e-05, 'epoch': 1.53}\n",
            "  8% 339000/4423240 [8:56:55<100:30:45, 11.29it/s][INFO|trainer.py:1937] 2021-09-13 18:52:29,775 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:52:29,783 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:52:29,967 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:52:29,974 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:52:29,998 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:52:30,644 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338000] due to args.save_total_limit\n",
            "{'loss': 1.8191, 'learning_rate': 4.6162315406805873e-05, 'epoch': 1.54}\n",
            "  8% 339500/4423240 [8:57:45<123:32:13,  9.18it/s][INFO|trainer.py:1937] 2021-09-13 18:53:19,213 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:53:19,222 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:53:19,409 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:53:19,415 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:53:19,421 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:53:20,199 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-338500] due to args.save_total_limit\n",
            "{'loss': 1.6998, 'learning_rate': 4.6156663441278345e-05, 'epoch': 1.54}\n",
            "  8% 340000/4423240 [8:58:34<172:20:56,  6.58it/s][INFO|trainer.py:1937] 2021-09-13 18:54:08,275 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:54:08,282 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:54:08,471 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:54:08,478 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:54:08,484 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:54:09,463 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339000] due to args.save_total_limit\n",
            "{'loss': 1.8701, 'learning_rate': 4.615101147575081e-05, 'epoch': 1.54}\n",
            "  8% 340500/4423240 [8:59:23<88:32:40, 12.81it/s][INFO|trainer.py:1937] 2021-09-13 18:54:57,670 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:54:57,676 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:54:57,883 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:54:57,889 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:54:57,895 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:54:58,889 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-339500] due to args.save_total_limit\n",
            "{'loss': 1.7052, 'learning_rate': 4.614535951022328e-05, 'epoch': 1.54}\n",
            "  8% 341000/4423240 [9:00:10<98:07:24, 11.56it/s][INFO|trainer.py:1937] 2021-09-13 18:55:44,582 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:55:44,590 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:55:44,783 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:55:44,792 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:55:44,798 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:55:45,462 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340000] due to args.save_total_limit\n",
            "{'loss': 1.7369, 'learning_rate': 4.6139707544695745e-05, 'epoch': 1.54}\n",
            "  8% 341500/4423240 [9:01:01<88:11:18, 12.86it/s][INFO|trainer.py:1937] 2021-09-13 18:56:34,846 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:56:34,853 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:56:35,051 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:56:35,058 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:56:35,063 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:56:35,712 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-340500] due to args.save_total_limit\n",
            "{'loss': 1.7611, 'learning_rate': 4.613405557916821e-05, 'epoch': 1.55}\n",
            "  8% 342000/4423240 [9:01:51<109:34:23, 10.35it/s][INFO|trainer.py:1937] 2021-09-13 18:57:25,216 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:57:25,223 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:57:25,413 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:57:25,422 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:57:25,432 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:57:26,141 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341000] due to args.save_total_limit\n",
            "{'loss': 1.8154, 'learning_rate': 4.612840361364068e-05, 'epoch': 1.55}\n",
            "  8% 342500/4423240 [9:02:39<110:22:33, 10.27it/s][INFO|trainer.py:1937] 2021-09-13 18:58:13,795 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:58:13,803 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:58:13,998 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:58:14,004 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:58:14,009 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:58:14,725 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-341500] due to args.save_total_limit\n",
            "{'loss': 1.9095, 'learning_rate': 4.612275164811315e-05, 'epoch': 1.55}\n",
            "  8% 343000/4423240 [9:03:29<127:42:38,  8.87it/s][INFO|trainer.py:1937] 2021-09-13 18:59:03,432 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:59:03,439 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:59:03,634 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:59:03,641 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:59:03,648 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:59:04,570 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342000] due to args.save_total_limit\n",
            "{'loss': 1.81, 'learning_rate': 4.611709968258562e-05, 'epoch': 1.55}\n",
            "  8% 343500/4423240 [9:04:17<89:10:30, 12.71it/s][INFO|trainer.py:1937] 2021-09-13 18:59:51,580 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 18:59:51,587 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 18:59:51,789 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 18:59:51,798 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 18:59:51,808 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 18:59:52,869 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-342500] due to args.save_total_limit\n",
            "{'loss': 1.7209, 'learning_rate': 4.611144771705809e-05, 'epoch': 1.56}\n",
            "  8% 344000/4423240 [9:05:05<95:17:27, 11.89it/s][INFO|trainer.py:1937] 2021-09-13 19:00:39,633 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:00:39,641 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:00:39,851 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:00:39,857 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:00:39,863 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:00:40,640 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343000] due to args.save_total_limit\n",
            "{'loss': 1.8265, 'learning_rate': 4.610579575153055e-05, 'epoch': 1.56}\n",
            "  8% 344500/4423240 [9:05:54<161:14:46,  7.03it/s][INFO|trainer.py:1937] 2021-09-13 19:01:28,253 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:01:28,261 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:01:28,436 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:01:28,460 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:01:28,466 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:01:29,602 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-343500] due to args.save_total_limit\n",
            "{'loss': 1.7084, 'learning_rate': 4.6100143786003024e-05, 'epoch': 1.56}\n",
            "  8% 345000/4423240 [9:06:42<92:33:24, 12.24it/s][INFO|trainer.py:1937] 2021-09-13 19:02:15,915 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:02:15,923 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:02:16,141 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:02:16,147 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:02:16,152 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:02:16,969 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344000] due to args.save_total_limit\n",
            "{'loss': 1.8235, 'learning_rate': 4.609449182047549e-05, 'epoch': 1.56}\n",
            "  8% 345500/4423240 [9:07:32<227:30:31,  4.98it/s][INFO|trainer.py:1937] 2021-09-13 19:03:06,558 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:03:06,566 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:03:06,768 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:03:06,777 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:03:06,793 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:03:07,905 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-344500] due to args.save_total_limit\n",
            "{'loss': 1.7844, 'learning_rate': 4.608883985494796e-05, 'epoch': 1.56}\n",
            "  8% 346000/4423240 [9:08:22<85:47:08, 13.20it/s][INFO|trainer.py:1937] 2021-09-13 19:03:56,021 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:03:56,028 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:03:56,226 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:03:56,233 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:03:56,238 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:03:56,859 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345000] due to args.save_total_limit\n",
            "{'loss': 1.7168, 'learning_rate': 4.6083187889420425e-05, 'epoch': 1.57}\n",
            "  8% 346500/4423240 [9:09:09<127:45:33,  8.86it/s][INFO|trainer.py:1937] 2021-09-13 19:04:43,490 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:04:43,497 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:04:43,673 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:04:43,679 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:04:43,684 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:04:44,331 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-345500] due to args.save_total_limit\n",
            "{'loss': 1.7675, 'learning_rate': 4.6077535923892896e-05, 'epoch': 1.57}\n",
            "  8% 347000/4423240 [9:09:57<128:10:58,  8.83it/s][INFO|trainer.py:1937] 2021-09-13 19:05:31,785 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:05:31,806 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:05:31,981 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:05:31,993 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:05:31,998 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:05:32,628 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346000] due to args.save_total_limit\n",
            "{'loss': 1.8769, 'learning_rate': 4.607188395836536e-05, 'epoch': 1.57}\n",
            "  8% 347500/4423240 [9:10:44<116:06:06,  9.75it/s][INFO|trainer.py:1937] 2021-09-13 19:06:18,411 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:06:18,433 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:06:18,637 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:06:18,643 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:06:18,649 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:06:19,269 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-346500] due to args.save_total_limit\n",
            "{'loss': 1.8683, 'learning_rate': 4.606623199283783e-05, 'epoch': 1.57}\n",
            "  8% 348000/4423240 [9:11:32<78:07:22, 14.49it/s][INFO|trainer.py:1937] 2021-09-13 19:07:06,169 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:07:06,177 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:07:06,354 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:07:06,360 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:07:06,372 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:07:07,055 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347000] due to args.save_total_limit\n",
            "{'loss': 1.8761, 'learning_rate': 4.60605800273103e-05, 'epoch': 1.58}\n",
            "  8% 348500/4423240 [9:12:24<113:14:38,  9.99it/s][INFO|trainer.py:1937] 2021-09-13 19:07:57,879 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:07:57,887 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:07:58,062 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:07:58,068 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:07:58,073 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:07:59,032 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-347500] due to args.save_total_limit\n",
            "{'loss': 1.7531, 'learning_rate': 4.605492806178277e-05, 'epoch': 1.58}\n",
            "  8% 349000/4423240 [9:13:11<89:29:55, 12.65it/s][INFO|trainer.py:1937] 2021-09-13 19:08:45,119 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:08:45,131 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:08:45,309 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:08:45,315 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:08:45,320 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:08:45,982 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348000] due to args.save_total_limit\n",
            "{'loss': 1.7735, 'learning_rate': 4.604927609625523e-05, 'epoch': 1.58}\n",
            "  8% 349500/4423240 [9:13:58<106:43:06, 10.60it/s][INFO|trainer.py:1937] 2021-09-13 19:09:32,231 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:09:32,238 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:09:32,432 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:09:32,443 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:09:32,452 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:09:33,126 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-348500] due to args.save_total_limit\n",
            "{'loss': 1.817, 'learning_rate': 4.6043624130727704e-05, 'epoch': 1.58}\n",
            "  8% 350000/4423240 [9:14:45<89:40:58, 12.62it/s][INFO|trainer.py:1937] 2021-09-13 19:10:19,578 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:10:19,586 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:10:19,776 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:10:19,802 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:10:19,808 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:10:20,511 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349000] due to args.save_total_limit\n",
            "{'loss': 1.8545, 'learning_rate': 4.603797216520017e-05, 'epoch': 1.58}\n",
            "  8% 350500/4423240 [9:15:33<95:56:33, 11.79it/s][INFO|trainer.py:1937] 2021-09-13 19:11:07,613 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:11:07,622 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:11:07,802 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:11:07,808 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:11:07,813 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:11:08,500 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-349500] due to args.save_total_limit\n",
            "{'loss': 1.7823, 'learning_rate': 4.603232019967264e-05, 'epoch': 1.59}\n",
            "  8% 351000/4423240 [9:16:23<112:03:56, 10.09it/s][INFO|trainer.py:1937] 2021-09-13 19:11:56,956 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:11:56,963 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:11:57,148 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:11:57,154 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:11:57,159 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:11:58,179 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350000] due to args.save_total_limit\n",
            "{'loss': 1.779, 'learning_rate': 4.602666823414511e-05, 'epoch': 1.59}\n",
            "  8% 351500/4423240 [9:17:10<84:06:34, 13.45it/s][INFO|trainer.py:1937] 2021-09-13 19:12:44,520 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:12:44,527 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:12:44,731 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:12:44,737 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:12:44,743 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:12:45,401 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-350500] due to args.save_total_limit\n",
            "{'loss': 1.8356, 'learning_rate': 4.6021016268617576e-05, 'epoch': 1.59}\n",
            "  8% 352000/4423240 [9:17:59<78:38:36, 14.38it/s][INFO|trainer.py:1937] 2021-09-13 19:13:33,458 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:13:33,466 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:13:33,669 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:13:33,676 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:13:33,682 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:13:34,399 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351000] due to args.save_total_limit\n",
            "{'loss': 1.7943, 'learning_rate': 4.601536430309005e-05, 'epoch': 1.59}\n",
            "  8% 352500/4423240 [9:18:48<120:11:34,  9.41it/s][INFO|trainer.py:1937] 2021-09-13 19:14:22,420 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:14:22,426 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:14:22,633 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:14:22,640 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:14:22,644 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:14:23,342 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-351500] due to args.save_total_limit\n",
            "{'loss': 1.6866, 'learning_rate': 4.600971233756251e-05, 'epoch': 1.6}\n",
            "  8% 353000/4423240 [9:19:35<107:26:46, 10.52it/s][INFO|trainer.py:1937] 2021-09-13 19:15:09,126 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:15:09,132 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:15:09,332 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:15:09,339 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:15:09,345 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:15:10,012 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352000] due to args.save_total_limit\n",
            "{'loss': 1.6663, 'learning_rate': 4.6004060372034976e-05, 'epoch': 1.6}\n",
            "  8% 353500/4423240 [9:20:23<86:02:40, 13.14it/s][INFO|trainer.py:1937] 2021-09-13 19:15:57,692 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:15:57,698 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:15:57,952 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:15:57,960 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:15:57,971 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:15:58,929 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-352500] due to args.save_total_limit\n",
            "{'loss': 1.756, 'learning_rate': 4.599840840650745e-05, 'epoch': 1.6}\n",
            "  8% 354000/4423240 [9:21:11<104:20:29, 10.83it/s][INFO|trainer.py:1937] 2021-09-13 19:16:45,766 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:16:45,774 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:16:45,987 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:16:45,993 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:16:45,998 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:16:46,675 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353000] due to args.save_total_limit\n",
            "{'loss': 1.6729, 'learning_rate': 4.599275644097992e-05, 'epoch': 1.6}\n",
            "  8% 354500/4423240 [9:21:58<104:28:33, 10.82it/s][INFO|trainer.py:1937] 2021-09-13 19:17:32,701 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:17:32,708 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:17:32,939 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:17:32,945 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:17:32,950 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:17:33,627 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-353500] due to args.save_total_limit\n",
            "{'loss': 1.7656, 'learning_rate': 4.598710447545238e-05, 'epoch': 1.61}\n",
            "  8% 355000/4423240 [9:22:47<95:56:32, 11.78it/s][INFO|trainer.py:1937] 2021-09-13 19:18:20,959 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:18:20,966 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:18:21,151 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:18:21,157 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:18:21,162 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:18:21,812 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354000] due to args.save_total_limit\n",
            "{'loss': 1.7107, 'learning_rate': 4.5981452509924855e-05, 'epoch': 1.61}\n",
            "  8% 355500/4423240 [9:23:35<113:03:39,  9.99it/s][INFO|trainer.py:1937] 2021-09-13 19:19:08,823 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:19:08,831 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:19:09,036 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:19:09,042 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:19:09,072 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:19:09,827 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-354500] due to args.save_total_limit\n",
            "{'loss': 1.7035, 'learning_rate': 4.5975800544397326e-05, 'epoch': 1.61}\n",
            "  8% 356000/4423240 [9:24:22<91:21:40, 12.37it/s][INFO|trainer.py:1937] 2021-09-13 19:19:56,135 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:19:56,143 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:19:56,336 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:19:56,342 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:19:56,347 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:19:57,025 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355000] due to args.save_total_limit\n",
            "{'loss': 1.8757, 'learning_rate': 4.597014857886979e-05, 'epoch': 1.61}\n",
            "  8% 356500/4423240 [9:25:11<97:53:04, 11.54it/s][INFO|trainer.py:1937] 2021-09-13 19:20:45,724 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:20:45,733 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:20:45,921 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:20:45,928 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:20:45,934 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:20:46,899 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-355500] due to args.save_total_limit\n",
            "{'loss': 1.7263, 'learning_rate': 4.5964496613342255e-05, 'epoch': 1.61}\n",
            "  8% 357000/4423240 [9:25:59<146:23:49,  7.72it/s][INFO|trainer.py:1937] 2021-09-13 19:21:33,712 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:21:33,718 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:21:33,888 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:21:33,894 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:21:33,899 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:21:34,591 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356000] due to args.save_total_limit\n",
            "{'loss': 1.757, 'learning_rate': 4.5958844647814726e-05, 'epoch': 1.62}\n",
            "  8% 357500/4423240 [9:26:47<98:21:04, 11.48it/s][INFO|trainer.py:1937] 2021-09-13 19:22:21,109 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:22:21,117 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:22:21,301 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:22:21,307 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:22:21,312 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:22:21,984 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-356500] due to args.save_total_limit\n",
            "{'loss': 1.8528, 'learning_rate': 4.595319268228719e-05, 'epoch': 1.62}\n",
            "  8% 358000/4423240 [9:27:36<141:42:50,  7.97it/s][INFO|trainer.py:1937] 2021-09-13 19:23:10,408 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:23:10,425 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:23:10,614 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:23:10,640 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:23:10,646 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:23:11,316 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357000] due to args.save_total_limit\n",
            "{'loss': 1.8351, 'learning_rate': 4.594754071675966e-05, 'epoch': 1.62}\n",
            "  8% 358500/4423240 [9:28:26<117:53:38,  9.58it/s][INFO|trainer.py:1937] 2021-09-13 19:23:59,955 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:23:59,963 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:24:00,134 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:24:00,141 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:24:00,146 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:24:00,807 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-357500] due to args.save_total_limit\n",
            "{'loss': 1.7554, 'learning_rate': 4.5941888751232134e-05, 'epoch': 1.62}\n",
            "  8% 359000/4423240 [9:29:13<95:55:27, 11.77it/s][INFO|trainer.py:1937] 2021-09-13 19:24:46,876 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:24:46,885 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:24:47,064 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:24:47,070 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:24:47,075 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:24:47,959 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358000] due to args.save_total_limit\n",
            "{'loss': 1.7126, 'learning_rate': 4.59362367857046e-05, 'epoch': 1.63}\n",
            "  8% 359500/4423240 [9:30:00<142:34:09,  7.92it/s][INFO|trainer.py:1937] 2021-09-13 19:25:34,802 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:25:34,809 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:25:35,023 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:25:35,030 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:25:35,035 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:25:35,721 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-358500] due to args.save_total_limit\n",
            "{'loss': 1.8406, 'learning_rate': 4.593058482017707e-05, 'epoch': 1.63}\n",
            "  8% 360000/4423240 [9:30:51<159:10:49,  7.09it/s][INFO|trainer.py:1937] 2021-09-13 19:26:24,985 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:26:24,993 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:26:25,179 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:26:25,186 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:26:25,191 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:26:25,827 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359000] due to args.save_total_limit\n",
            "{'loss': 1.8035, 'learning_rate': 4.5924932854649534e-05, 'epoch': 1.63}\n",
            "  8% 360500/4423240 [9:31:38<93:05:29, 12.12it/s][INFO|trainer.py:1937] 2021-09-13 19:27:11,992 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:27:12,021 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:27:12,301 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:27:12,310 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:27:12,318 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:27:13,489 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-359500] due to args.save_total_limit\n",
            "{'loss': 1.8498, 'learning_rate': 4.5919280889122e-05, 'epoch': 1.63}\n",
            "  8% 361000/4423240 [9:32:26<140:43:34,  8.02it/s][INFO|trainer.py:1937] 2021-09-13 19:28:00,127 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:28:00,137 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:28:00,316 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:28:00,322 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:28:00,327 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:28:01,333 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360000] due to args.save_total_limit\n",
            "{'loss': 1.6592, 'learning_rate': 4.591362892359447e-05, 'epoch': 1.63}\n",
            "  8% 361500/4423240 [9:33:13<109:41:49, 10.29it/s][INFO|trainer.py:1937] 2021-09-13 19:28:47,234 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:28:47,243 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:28:47,419 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:28:47,425 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:28:47,430 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:28:48,462 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-360500] due to args.save_total_limit\n",
            "{'loss': 1.7765, 'learning_rate': 4.590797695806694e-05, 'epoch': 1.64}\n",
            "  8% 362000/4423240 [9:34:00<99:52:20, 11.30it/s][INFO|trainer.py:1937] 2021-09-13 19:29:34,331 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:29:34,342 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:29:34,547 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:29:34,553 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:29:34,558 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:29:35,848 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361000] due to args.save_total_limit\n",
            "{'loss': 1.7791, 'learning_rate': 4.5902324992539406e-05, 'epoch': 1.64}\n",
            "  8% 362500/4423240 [9:34:48<101:10:27, 11.15it/s][INFO|trainer.py:1937] 2021-09-13 19:30:22,674 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:30:22,687 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:30:22,939 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:30:22,951 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:30:22,960 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:30:23,973 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-361500] due to args.save_total_limit\n",
            "{'loss': 1.7032, 'learning_rate': 4.589667302701188e-05, 'epoch': 1.64}\n",
            "  8% 363000/4423240 [9:35:33<96:49:55, 11.65it/s][INFO|trainer.py:1937] 2021-09-13 19:31:07,535 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:31:07,543 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:31:07,729 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:31:07,749 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:31:07,755 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:31:08,574 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362000] due to args.save_total_limit\n",
            "{'loss': 1.7352, 'learning_rate': 4.589102106148435e-05, 'epoch': 1.64}\n",
            "  8% 363500/4423240 [9:36:21<90:50:46, 12.41it/s][INFO|trainer.py:1937] 2021-09-13 19:31:55,215 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:31:55,222 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:31:55,395 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:31:55,421 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:31:55,439 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:31:56,210 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-362500] due to args.save_total_limit\n",
            "{'loss': 1.8755, 'learning_rate': 4.5885369095956806e-05, 'epoch': 1.65}\n",
            "  8% 364000/4423240 [9:37:09<87:55:48, 12.82it/s][INFO|trainer.py:1937] 2021-09-13 19:32:43,340 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:32:43,348 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:32:43,536 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:32:43,542 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:32:43,547 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:32:44,304 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363000] due to args.save_total_limit\n",
            "{'loss': 1.8032, 'learning_rate': 4.587971713042928e-05, 'epoch': 1.65}\n",
            "  8% 364500/4423240 [9:37:55<90:17:15, 12.49it/s][INFO|trainer.py:1937] 2021-09-13 19:33:29,355 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:33:29,363 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:33:29,557 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:33:29,563 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:33:29,568 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:33:30,707 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-363500] due to args.save_total_limit\n",
            "{'loss': 1.7582, 'learning_rate': 4.587406516490175e-05, 'epoch': 1.65}\n",
            "  8% 365000/4423240 [9:38:45<123:25:27,  9.13it/s][INFO|trainer.py:1937] 2021-09-13 19:34:18,996 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:34:19,007 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:34:19,197 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:34:19,219 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:34:19,224 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:34:20,013 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364000] due to args.save_total_limit\n",
            "{'loss': 1.7282, 'learning_rate': 4.5868413199374214e-05, 'epoch': 1.65}\n",
            "  8% 365500/4423240 [9:39:32<125:07:36,  9.01it/s][INFO|trainer.py:1937] 2021-09-13 19:35:06,573 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:35:06,597 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:35:06,794 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:35:06,800 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:35:06,806 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:35:07,550 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-364500] due to args.save_total_limit\n",
            "{'loss': 1.8166, 'learning_rate': 4.5862761233846685e-05, 'epoch': 1.65}\n",
            "  8% 366000/4423240 [9:40:23<107:14:36, 10.51it/s][INFO|trainer.py:1937] 2021-09-13 19:35:57,469 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:35:57,490 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:35:57,675 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:35:57,682 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:35:57,688 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:35:58,580 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365000] due to args.save_total_limit\n",
            "{'loss': 1.8158, 'learning_rate': 4.5857109268319156e-05, 'epoch': 1.66}\n",
            "  8% 366500/4423240 [9:41:12<118:40:15,  9.50it/s][INFO|trainer.py:1937] 2021-09-13 19:36:46,734 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:36:46,741 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:36:46,933 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:36:46,939 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:36:46,945 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:36:47,698 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-365500] due to args.save_total_limit\n",
            "{'loss': 1.7776, 'learning_rate': 4.585145730279162e-05, 'epoch': 1.66}\n",
            "  8% 367000/4423240 [9:41:58<98:07:41, 11.48it/s][INFO|trainer.py:1937] 2021-09-13 19:37:32,681 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:37:32,693 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:37:32,945 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:37:32,951 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:37:32,955 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:37:33,816 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366000] due to args.save_total_limit\n",
            "{'loss': 1.7008, 'learning_rate': 4.584580533726409e-05, 'epoch': 1.66}\n",
            "  8% 367500/4423240 [9:42:47<144:13:50,  7.81it/s][INFO|trainer.py:1937] 2021-09-13 19:38:21,424 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:38:21,432 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:38:21,633 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:38:21,639 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:38:21,644 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:38:23,113 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-366500] due to args.save_total_limit\n",
            "{'loss': 1.7202, 'learning_rate': 4.584015337173656e-05, 'epoch': 1.66}\n",
            "  8% 368000/4423240 [9:43:36<99:21:47, 11.34it/s][INFO|trainer.py:1937] 2021-09-13 19:39:10,602 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:39:10,610 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:39:10,806 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:39:10,813 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:39:10,819 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:39:11,715 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367000] due to args.save_total_limit\n",
            "{'loss': 1.7408, 'learning_rate': 4.583450140620902e-05, 'epoch': 1.67}\n",
            "  8% 368500/4423240 [9:44:26<127:26:44,  8.84it/s][INFO|trainer.py:1937] 2021-09-13 19:40:00,225 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:40:00,233 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:40:00,425 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:40:00,430 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:40:00,435 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:40:01,124 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-367500] due to args.save_total_limit\n",
            "{'loss': 1.683, 'learning_rate': 4.582884944068149e-05, 'epoch': 1.67}\n",
            "  8% 369000/4423240 [9:45:14<123:57:54,  9.08it/s][INFO|trainer.py:1937] 2021-09-13 19:40:48,016 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:40:48,023 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:40:48,213 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:40:48,235 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:40:48,240 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:40:48,918 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368000] due to args.save_total_limit\n",
            "{'loss': 1.667, 'learning_rate': 4.5823197475153964e-05, 'epoch': 1.67}\n",
            "  8% 369500/4423240 [9:46:01<97:03:58, 11.60it/s][INFO|trainer.py:1937] 2021-09-13 19:41:34,828 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:41:34,836 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:41:35,031 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:41:35,038 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:41:35,044 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:41:35,751 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-368500] due to args.save_total_limit\n",
            "{'loss': 1.7792, 'learning_rate': 4.581754550962643e-05, 'epoch': 1.67}\n",
            "  8% 370000/4423240 [9:46:50<100:11:54, 11.24it/s][INFO|trainer.py:1937] 2021-09-13 19:42:24,487 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:42:24,497 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:42:24,703 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:42:24,726 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:42:24,735 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:42:25,761 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369000] due to args.save_total_limit\n",
            "{'loss': 1.7909, 'learning_rate': 4.58118935440989e-05, 'epoch': 1.68}\n",
            "  8% 370500/4423240 [9:47:37<87:48:41, 12.82it/s][INFO|trainer.py:1937] 2021-09-13 19:43:10,803 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:43:10,812 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:43:11,073 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:43:11,083 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:43:11,090 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:43:12,017 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-369500] due to args.save_total_limit\n",
            "{'loss': 1.7851, 'learning_rate': 4.580624157857137e-05, 'epoch': 1.68}\n",
            "  8% 371000/4423240 [9:48:26<96:00:48, 11.72it/s][INFO|trainer.py:1937] 2021-09-13 19:44:00,484 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:44:00,492 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:44:00,684 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:44:00,690 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:44:00,695 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:44:01,413 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370000] due to args.save_total_limit\n",
            "{'loss': 1.7236, 'learning_rate': 4.580058961304383e-05, 'epoch': 1.68}\n",
            "  8% 371500/4423240 [9:49:11<104:23:09, 10.78it/s][INFO|trainer.py:1937] 2021-09-13 19:44:44,863 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:44:44,870 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:44:45,106 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:44:45,112 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:44:45,117 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:44:45,892 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-370500] due to args.save_total_limit\n",
            "{'loss': 1.7978, 'learning_rate': 4.57949376475163e-05, 'epoch': 1.68}\n",
            "  8% 372000/4423240 [9:50:00<113:36:57,  9.90it/s][INFO|trainer.py:1937] 2021-09-13 19:45:33,864 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:45:33,873 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:45:34,067 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:45:34,075 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:45:34,101 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:45:34,749 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371000] due to args.save_total_limit\n",
            "{'loss': 1.7591, 'learning_rate': 4.578928568198877e-05, 'epoch': 1.68}\n",
            "  8% 372500/4423240 [9:50:48<79:58:08, 14.07it/s][INFO|trainer.py:1937] 2021-09-13 19:46:22,128 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:46:22,135 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:46:22,347 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:46:22,354 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:46:22,361 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:46:23,353 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-371500] due to args.save_total_limit\n",
            "{'loss': 1.7154, 'learning_rate': 4.5783633716461236e-05, 'epoch': 1.69}\n",
            "  8% 373000/4423240 [9:51:34<93:30:46, 12.03it/s][INFO|trainer.py:1937] 2021-09-13 19:47:08,620 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:47:08,628 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:47:08,815 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:47:08,821 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:47:08,826 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:47:09,762 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372000] due to args.save_total_limit\n",
            "{'loss': 1.8076, 'learning_rate': 4.577798175093371e-05, 'epoch': 1.69}\n",
            "  8% 373500/4423240 [9:52:22<91:54:07, 12.24it/s][INFO|trainer.py:1937] 2021-09-13 19:47:56,571 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:47:56,579 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:47:56,759 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:47:56,765 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:47:56,770 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:47:57,726 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-372500] due to args.save_total_limit\n",
            "{'loss': 1.6391, 'learning_rate': 4.577232978540618e-05, 'epoch': 1.69}\n",
            "  8% 374000/4423240 [9:53:10<89:16:01, 12.60it/s][INFO|trainer.py:1937] 2021-09-13 19:48:43,930 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:48:43,938 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:48:44,141 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:48:44,147 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:48:44,152 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:48:44,802 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373000] due to args.save_total_limit\n",
            "{'loss': 1.7699, 'learning_rate': 4.5766677819878644e-05, 'epoch': 1.69}\n",
            "  8% 374500/4423240 [9:53:57<91:24:50, 12.30it/s][INFO|trainer.py:1937] 2021-09-13 19:49:31,386 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:49:31,393 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:49:31,608 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:49:31,615 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:49:31,621 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:49:32,268 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-373500] due to args.save_total_limit\n",
            "{'loss': 1.8109, 'learning_rate': 4.5761025854351115e-05, 'epoch': 1.7}\n",
            "  8% 375000/4423240 [9:54:45<105:17:20, 10.68it/s][INFO|trainer.py:1937] 2021-09-13 19:50:19,028 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:50:19,034 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:50:19,208 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:50:19,213 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:50:19,218 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:50:19,888 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374000] due to args.save_total_limit\n",
            "{'loss': 1.6925, 'learning_rate': 4.575537388882358e-05, 'epoch': 1.7}\n",
            "  8% 375500/4423240 [9:55:33<101:29:29, 11.08it/s][INFO|trainer.py:1937] 2021-09-13 19:51:07,311 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:51:07,329 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:51:07,513 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:51:07,519 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:51:07,524 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:51:08,529 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-374500] due to args.save_total_limit\n",
            "{'loss': 1.8189, 'learning_rate': 4.5749721923296044e-05, 'epoch': 1.7}\n",
            "  9% 376000/4423240 [9:56:23<113:18:28,  9.92it/s][INFO|trainer.py:1937] 2021-09-13 19:51:57,560 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:51:57,571 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:51:57,822 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:51:57,828 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:51:57,833 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:51:58,984 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375000] due to args.save_total_limit\n",
            "{'loss': 1.7015, 'learning_rate': 4.5744069957768515e-05, 'epoch': 1.7}\n",
            "  9% 376500/4423240 [9:57:10<104:02:15, 10.80it/s][INFO|trainer.py:1937] 2021-09-13 19:52:44,367 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:52:44,375 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:52:44,606 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:52:44,611 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:52:44,615 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:52:45,573 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-375500] due to args.save_total_limit\n",
            "{'loss': 1.6872, 'learning_rate': 4.573841799224099e-05, 'epoch': 1.7}\n",
            "  9% 377000/4423240 [9:58:01<107:51:48, 10.42it/s][INFO|trainer.py:1937] 2021-09-13 19:53:35,769 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:53:35,792 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:53:35,973 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:53:35,979 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:53:35,984 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:53:36,706 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376000] due to args.save_total_limit\n",
            "{'loss': 1.7469, 'learning_rate': 4.573276602671345e-05, 'epoch': 1.71}\n",
            "  9% 377500/4423240 [9:58:50<122:01:53,  9.21it/s][INFO|trainer.py:1937] 2021-09-13 19:54:24,431 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:54:24,438 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:54:24,623 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:54:24,635 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:54:24,644 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:54:25,317 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-376500] due to args.save_total_limit\n",
            "{'loss': 1.7466, 'learning_rate': 4.572711406118592e-05, 'epoch': 1.71}\n",
            "  9% 378000/4423240 [9:59:41<92:33:37, 12.14it/s][INFO|trainer.py:1937] 2021-09-13 19:55:15,059 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:55:15,067 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:55:15,255 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:55:15,261 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:55:15,267 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:55:16,086 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377000] due to args.save_total_limit\n",
            "{'loss': 1.6881, 'learning_rate': 4.572146209565839e-05, 'epoch': 1.71}\n",
            "  9% 378500/4423240 [10:00:29<112:46:09,  9.96it/s][INFO|trainer.py:1937] 2021-09-13 19:56:03,004 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:56:03,010 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:56:03,182 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:56:03,205 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:56:03,214 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:56:04,546 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-377500] due to args.save_total_limit\n",
            "{'loss': 1.7862, 'learning_rate': 4.571581013013085e-05, 'epoch': 1.71}\n",
            "  9% 379000/4423240 [10:01:18<112:05:51, 10.02it/s][INFO|trainer.py:1937] 2021-09-13 19:56:51,959 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:56:51,966 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:56:52,164 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:56:52,171 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:56:52,176 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:56:52,973 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378000] due to args.save_total_limit\n",
            "{'loss': 1.7433, 'learning_rate': 4.571015816460332e-05, 'epoch': 1.72}\n",
            "  9% 379500/4423240 [10:02:07<102:03:02, 11.01it/s][INFO|trainer.py:1937] 2021-09-13 19:57:41,195 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:57:41,202 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:57:41,406 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:57:41,417 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:57:41,428 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:57:42,301 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-378500] due to args.save_total_limit\n",
            "{'loss': 1.6566, 'learning_rate': 4.5704506199075794e-05, 'epoch': 1.72}\n",
            "  9% 380000/4423240 [10:02:54<102:13:08, 10.99it/s][INFO|trainer.py:1937] 2021-09-13 19:58:28,123 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:58:28,130 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:58:28,341 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:58:28,347 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:58:28,352 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:58:29,255 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379000] due to args.save_total_limit\n",
            "{'loss': 1.7527, 'learning_rate': 4.569885423354826e-05, 'epoch': 1.72}\n",
            "  9% 380500/4423240 [10:03:44<103:24:01, 10.86it/s][INFO|trainer.py:1937] 2021-09-13 19:59:18,123 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 19:59:18,130 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 19:59:18,314 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 19:59:18,320 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 19:59:18,326 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 19:59:19,129 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-379500] due to args.save_total_limit\n",
            "{'loss': 1.6604, 'learning_rate': 4.569320226802073e-05, 'epoch': 1.72}\n",
            "  9% 381000/4423240 [10:04:31<85:21:31, 13.15it/s][INFO|trainer.py:1937] 2021-09-13 20:00:05,093 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:00:05,100 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:00:05,298 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:00:05,305 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:00:05,310 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:00:06,571 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380000] due to args.save_total_limit\n",
            "{'loss': 1.6891, 'learning_rate': 4.56875503024932e-05, 'epoch': 1.72}\n",
            "  9% 381500/4423240 [10:05:20<84:47:09, 13.24it/s][INFO|trainer.py:1937] 2021-09-13 20:00:54,305 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:00:54,313 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:00:54,491 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:00:54,498 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:00:54,504 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:00:55,375 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-380500] due to args.save_total_limit\n",
            "{'loss': 1.8148, 'learning_rate': 4.5681898336965666e-05, 'epoch': 1.73}\n",
            "  9% 382000/4423240 [10:06:08<91:01:31, 12.33it/s][INFO|trainer.py:1937] 2021-09-13 20:01:42,126 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:01:42,135 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:01:42,324 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:01:42,332 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:01:42,336 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:01:43,307 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381000] due to args.save_total_limit\n",
            "{'loss': 1.6613, 'learning_rate': 4.567624637143814e-05, 'epoch': 1.73}\n",
            "  9% 382500/4423240 [10:06:56<97:35:35, 11.50it/s][INFO|trainer.py:1937] 2021-09-13 20:02:29,937 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:02:29,959 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:02:30,147 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:02:30,153 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:02:30,158 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:02:30,973 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-381500] due to args.save_total_limit\n",
            "{'loss': 1.704, 'learning_rate': 4.56705944059106e-05, 'epoch': 1.73}\n",
            "  9% 383000/4423240 [10:07:43<90:30:13, 12.40it/s][INFO|trainer.py:1937] 2021-09-13 20:03:17,083 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:03:17,091 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:03:17,279 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:03:17,285 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:03:17,291 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:03:18,036 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382000] due to args.save_total_limit\n",
            "{'loss': 1.7622, 'learning_rate': 4.566494244038307e-05, 'epoch': 1.73}\n",
            "  9% 383500/4423240 [10:08:31<89:40:18, 12.51it/s][INFO|trainer.py:1937] 2021-09-13 20:04:05,018 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:04:05,026 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:04:05,215 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:04:05,221 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:04:05,226 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:04:06,334 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-382500] due to args.save_total_limit\n",
            "{'loss': 1.801, 'learning_rate': 4.565929047485554e-05, 'epoch': 1.74}\n",
            "  9% 384000/4423240 [10:09:21<110:40:54, 10.14it/s][INFO|trainer.py:1937] 2021-09-13 20:04:55,663 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:04:55,672 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:04:55,851 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:04:55,857 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:04:55,862 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:04:57,129 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383000] due to args.save_total_limit\n",
            "{'loss': 1.6211, 'learning_rate': 4.565363850932801e-05, 'epoch': 1.74}\n",
            "  9% 384500/4423240 [10:10:06<114:01:45,  9.84it/s][INFO|trainer.py:1937] 2021-09-13 20:05:40,139 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:05:40,148 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:05:40,338 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:05:40,346 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:05:40,352 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:05:41,158 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-383500] due to args.save_total_limit\n",
            "{'loss': 1.788, 'learning_rate': 4.5647986543800474e-05, 'epoch': 1.74}\n",
            "  9% 385000/4423240 [10:10:56<104:47:37, 10.70it/s][INFO|trainer.py:1937] 2021-09-13 20:06:29,900 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:06:29,906 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:06:30,106 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:06:30,112 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:06:30,117 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:06:31,038 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384000] due to args.save_total_limit\n",
            "{'loss': 1.8421, 'learning_rate': 4.5642334578272945e-05, 'epoch': 1.74}\n",
            "  9% 385500/4423240 [10:11:44<99:49:18, 11.24it/s][INFO|trainer.py:1937] 2021-09-13 20:07:18,012 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:07:18,019 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:07:18,225 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:07:18,249 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:07:18,254 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:07:19,073 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-384500] due to args.save_total_limit\n",
            "{'loss': 1.7776, 'learning_rate': 4.563668261274541e-05, 'epoch': 1.75}\n",
            "  9% 386000/4423240 [10:12:36<109:18:14, 10.26it/s][INFO|trainer.py:1937] 2021-09-13 20:08:09,889 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:08:09,898 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:08:10,081 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:08:10,086 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:08:10,092 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:08:11,017 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385000] due to args.save_total_limit\n",
            "{'loss': 1.8026, 'learning_rate': 4.5631030647217874e-05, 'epoch': 1.75}\n",
            "  9% 386500/4423240 [10:13:25<94:13:21, 11.90it/s][INFO|trainer.py:1937] 2021-09-13 20:08:59,036 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:08:59,043 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:08:59,232 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:08:59,238 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:08:59,242 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:09:00,310 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-385500] due to args.save_total_limit\n",
            "{'loss': 1.6357, 'learning_rate': 4.5625378681690346e-05, 'epoch': 1.75}\n",
            "  9% 387000/4423240 [10:14:14<93:38:09, 11.97it/s][INFO|trainer.py:1937] 2021-09-13 20:09:48,302 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:09:48,310 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:09:48,518 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:09:48,528 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:09:48,536 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:09:49,736 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386000] due to args.save_total_limit\n",
            "{'loss': 1.6485, 'learning_rate': 4.561972671616282e-05, 'epoch': 1.75}\n",
            "  9% 387500/4423240 [10:15:00<103:40:34, 10.81it/s][INFO|trainer.py:1937] 2021-09-13 20:10:34,259 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:10:34,266 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:10:34,464 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:10:34,479 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:10:34,489 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:10:35,287 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-386500] due to args.save_total_limit\n",
            "{'loss': 1.794, 'learning_rate': 4.561407475063528e-05, 'epoch': 1.75}\n",
            "  9% 388000/4423240 [10:15:48<112:36:56,  9.95it/s][INFO|trainer.py:1937] 2021-09-13 20:11:22,070 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:11:22,079 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:11:22,293 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:11:22,299 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:11:22,304 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:11:23,387 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387000] due to args.save_total_limit\n",
            "{'loss': 1.7221, 'learning_rate': 4.560842278510775e-05, 'epoch': 1.76}\n",
            "  9% 388500/4423240 [10:16:39<107:08:42, 10.46it/s][INFO|trainer.py:1937] 2021-09-13 20:12:13,333 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:12:13,340 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:12:13,530 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:12:13,536 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:12:13,542 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:12:14,521 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-387500] due to args.save_total_limit\n",
            "{'loss': 1.7341, 'learning_rate': 4.560277081958022e-05, 'epoch': 1.76}\n",
            "  9% 389000/4423240 [10:17:28<94:47:19, 11.82it/s][INFO|trainer.py:1937] 2021-09-13 20:13:02,513 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:13:02,520 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:13:02,708 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:13:02,715 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:13:02,721 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:13:03,720 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388000] due to args.save_total_limit\n",
            "{'loss': 1.7155, 'learning_rate': 4.559711885405269e-05, 'epoch': 1.76}\n",
            "  9% 389500/4423240 [10:18:16<85:22:33, 13.12it/s][INFO|trainer.py:1937] 2021-09-13 20:13:50,341 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:13:50,350 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:13:50,537 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:13:50,544 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:13:50,549 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:13:51,620 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-388500] due to args.save_total_limit\n",
            "{'loss': 1.6565, 'learning_rate': 4.559146688852516e-05, 'epoch': 1.76}\n",
            "  9% 390000/4423240 [10:19:04<92:26:06, 12.12it/s][INFO|trainer.py:1937] 2021-09-13 20:14:38,292 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:14:38,301 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:14:38,489 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:14:38,506 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:14:38,516 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:14:39,341 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389000] due to args.save_total_limit\n",
            "{'loss': 1.6838, 'learning_rate': 4.5585814922997625e-05, 'epoch': 1.77}\n",
            "  9% 390500/4423240 [10:19:52<91:45:06, 12.21it/s][INFO|trainer.py:1937] 2021-09-13 20:15:26,658 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:15:26,667 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:15:26,845 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:15:26,854 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:15:26,860 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:15:27,685 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-389500] due to args.save_total_limit\n",
            "{'loss': 1.7811, 'learning_rate': 4.558016295747009e-05, 'epoch': 1.77}\n",
            "  9% 391000/4423240 [10:20:40<114:10:53,  9.81it/s][INFO|trainer.py:1937] 2021-09-13 20:16:14,722 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:16:14,729 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:16:14,918 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:16:14,939 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:16:14,946 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:16:15,861 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390000] due to args.save_total_limit\n",
            "{'loss': 1.7627, 'learning_rate': 4.557451099194256e-05, 'epoch': 1.77}\n",
            "  9% 391500/4423240 [10:21:30<98:19:32, 11.39it/s][INFO|trainer.py:1937] 2021-09-13 20:17:03,897 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:17:03,906 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:17:04,098 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:17:04,108 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:17:04,114 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:17:04,950 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-390500] due to args.save_total_limit\n",
            "{'loss': 1.7641, 'learning_rate': 4.5568859026415025e-05, 'epoch': 1.77}\n",
            "  9% 392000/4423240 [10:22:17<96:48:56, 11.57it/s][INFO|trainer.py:1937] 2021-09-13 20:17:51,617 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:17:51,631 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:17:51,834 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:17:51,839 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:17:51,845 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:17:52,928 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391000] due to args.save_total_limit\n",
            "{'loss': 1.7593, 'learning_rate': 4.5563207060887497e-05, 'epoch': 1.77}\n",
            "  9% 392500/4423240 [10:23:06<166:55:11,  6.71it/s][INFO|trainer.py:1937] 2021-09-13 20:18:40,057 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:18:40,064 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:18:40,256 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:18:40,267 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:18:40,276 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:18:41,019 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-391500] due to args.save_total_limit\n",
            "{'loss': 1.7089, 'learning_rate': 4.555755509535997e-05, 'epoch': 1.78}\n",
            "  9% 393000/4423240 [10:23:53<93:29:33, 11.97it/s][INFO|trainer.py:1937] 2021-09-13 20:19:27,396 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:19:27,406 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:19:27,595 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:19:27,602 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:19:27,609 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:19:28,307 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392000] due to args.save_total_limit\n",
            "{'loss': 1.7326, 'learning_rate': 4.555190312983243e-05, 'epoch': 1.78}\n",
            "  9% 393500/4423240 [10:24:40<110:47:31, 10.10it/s][INFO|trainer.py:1937] 2021-09-13 20:20:14,791 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:20:14,812 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:20:14,997 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:20:15,003 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:20:15,012 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:20:15,665 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-392500] due to args.save_total_limit\n",
            "{'loss': 1.7048, 'learning_rate': 4.55462511643049e-05, 'epoch': 1.78}\n",
            "  9% 394000/4423240 [10:25:28<111:24:17, 10.05it/s][INFO|trainer.py:1937] 2021-09-13 20:21:02,593 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:21:02,601 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:21:02,802 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:21:02,808 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:21:02,814 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:21:03,450 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393000] due to args.save_total_limit\n",
            "{'loss': 1.8299, 'learning_rate': 4.554059919877737e-05, 'epoch': 1.78}\n",
            "  9% 394500/4423240 [10:26:17<88:23:30, 12.66it/s][INFO|trainer.py:1937] 2021-09-13 20:21:51,053 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:21:51,060 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:21:51,242 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:21:51,248 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:21:51,253 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:21:51,944 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-393500] due to args.save_total_limit\n",
            "{'loss': 1.7199, 'learning_rate': 4.553494723324983e-05, 'epoch': 1.79}\n",
            "  9% 395000/4423240 [10:27:03<95:57:22, 11.66it/s][INFO|trainer.py:1937] 2021-09-13 20:22:37,569 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:22:37,580 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:22:37,792 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:22:37,799 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:22:37,805 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:22:38,487 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394000] due to args.save_total_limit\n",
            "{'loss': 1.7524, 'learning_rate': 4.5529295267722304e-05, 'epoch': 1.79}\n",
            "  9% 395500/4423240 [10:27:53<107:20:52, 10.42it/s][INFO|trainer.py:1937] 2021-09-13 20:23:27,307 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:23:27,314 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:23:27,489 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:23:27,495 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:23:27,502 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:23:28,166 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-394500] due to args.save_total_limit\n",
            "{'loss': 1.6914, 'learning_rate': 4.5523643302194776e-05, 'epoch': 1.79}\n",
            "  9% 396000/4423240 [10:28:39<101:51:30, 10.98it/s][INFO|trainer.py:1937] 2021-09-13 20:24:13,583 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:24:13,591 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:24:13,787 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:24:13,800 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:24:13,805 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:24:14,460 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395000] due to args.save_total_limit\n",
            "{'loss': 1.6844, 'learning_rate': 4.551799133666724e-05, 'epoch': 1.79}\n",
            "  9% 396500/4423240 [10:29:26<101:37:00, 11.01it/s][INFO|trainer.py:1937] 2021-09-13 20:25:00,244 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:25:00,251 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:25:00,423 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:25:00,452 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:25:00,456 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:25:01,083 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-395500] due to args.save_total_limit\n",
            "{'loss': 1.6734, 'learning_rate': 4.551233937113971e-05, 'epoch': 1.8}\n",
            "  9% 397000/4423240 [10:30:15<102:06:22, 10.95it/s][INFO|trainer.py:1937] 2021-09-13 20:25:49,678 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:25:49,686 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:25:49,897 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:25:49,903 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:25:49,908 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:25:50,592 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396000] due to args.save_total_limit\n",
            "{'loss': 1.7386, 'learning_rate': 4.550668740561218e-05, 'epoch': 1.8}\n",
            "  9% 397500/4423240 [10:31:03<88:26:42, 12.64it/s][INFO|trainer.py:1937] 2021-09-13 20:26:37,533 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:26:37,541 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:26:37,734 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:26:37,740 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:26:37,745 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:26:38,839 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-396500] due to args.save_total_limit\n",
            "{'loss': 1.691, 'learning_rate': 4.550103544008464e-05, 'epoch': 1.8}\n",
            "  9% 398000/4423240 [10:31:49<91:22:51, 12.24it/s][INFO|trainer.py:1937] 2021-09-13 20:27:23,259 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:27:23,266 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:27:23,473 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:27:23,478 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:27:23,483 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:27:24,282 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397000] due to args.save_total_limit\n",
            "{'loss': 1.7527, 'learning_rate': 4.549538347455711e-05, 'epoch': 1.8}\n",
            "  9% 398500/4423240 [10:32:39<93:12:52, 11.99it/s][INFO|trainer.py:1937] 2021-09-13 20:28:12,818 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:28:12,827 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:28:13,021 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:28:13,028 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:28:13,034 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:28:14,057 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-397500] due to args.save_total_limit\n",
            "{'loss': 1.7536, 'learning_rate': 4.548973150902958e-05, 'epoch': 1.8}\n",
            "  9% 399000/4423240 [10:33:26<113:17:28,  9.87it/s][INFO|trainer.py:1937] 2021-09-13 20:29:00,324 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:29:00,358 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:29:00,568 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:29:00,573 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:29:00,578 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:29:01,524 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398000] due to args.save_total_limit\n",
            "{'loss': 1.6388, 'learning_rate': 4.548407954350205e-05, 'epoch': 1.81}\n",
            "  9% 399500/4423240 [10:34:14<92:41:08, 12.06it/s][INFO|trainer.py:1937] 2021-09-13 20:29:47,848 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:29:47,856 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:29:48,048 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:29:48,056 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:29:48,062 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:29:49,112 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-398500] due to args.save_total_limit\n",
            "{'loss': 1.7785, 'learning_rate': 4.547842757797452e-05, 'epoch': 1.81}\n",
            "  9% 400000/4423240 [10:35:03<96:16:50, 11.61it/s][INFO|trainer.py:1937] 2021-09-13 20:30:37,353 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:30:37,361 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:30:37,554 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:30:37,560 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:30:37,565 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:30:38,329 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399000] due to args.save_total_limit\n",
            "{'loss': 1.8679, 'learning_rate': 4.547277561244699e-05, 'epoch': 1.81}\n",
            "  9% 400500/4423240 [10:35:52<138:02:05,  8.10it/s][INFO|trainer.py:1937] 2021-09-13 20:31:26,369 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:31:26,376 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:31:26,578 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:31:26,584 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:31:26,590 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:31:27,367 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-399500] due to args.save_total_limit\n",
            "{'loss': 1.6862, 'learning_rate': 4.5467123646919455e-05, 'epoch': 1.81}\n",
            "  9% 401000/4423240 [10:36:39<79:35:35, 14.04it/s][INFO|trainer.py:1937] 2021-09-13 20:32:13,370 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:32:13,379 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:32:13,585 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:32:13,597 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:32:13,608 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:32:14,477 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400000] due to args.save_total_limit\n",
            "{'loss': 1.7102, 'learning_rate': 4.546147168139192e-05, 'epoch': 1.82}\n",
            "  9% 401500/4423240 [10:37:27<124:46:59,  8.95it/s][INFO|trainer.py:1937] 2021-09-13 20:33:00,873 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:33:00,883 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:33:01,098 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:33:01,108 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:33:01,117 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:33:02,088 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-400500] due to args.save_total_limit\n",
            "{'loss': 1.7481, 'learning_rate': 4.545581971586439e-05, 'epoch': 1.82}\n",
            "  9% 402000/4423240 [10:38:15<113:31:34,  9.84it/s][INFO|trainer.py:1937] 2021-09-13 20:33:49,484 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:33:49,490 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:33:49,692 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:33:49,699 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:33:49,705 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:33:50,721 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401000] due to args.save_total_limit\n",
            "{'loss': 1.7329, 'learning_rate': 4.5450167750336856e-05, 'epoch': 1.82}\n",
            "  9% 402500/4423240 [10:39:04<146:11:32,  7.64it/s][INFO|trainer.py:1937] 2021-09-13 20:34:38,050 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:34:38,058 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:34:38,244 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:34:38,250 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:34:38,256 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:34:39,198 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-401500] due to args.save_total_limit\n",
            "{'loss': 1.6791, 'learning_rate': 4.544451578480933e-05, 'epoch': 1.82}\n",
            "  9% 403000/4423240 [10:39:51<101:24:32, 11.01it/s][INFO|trainer.py:1937] 2021-09-13 20:35:24,955 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:35:24,962 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:35:25,147 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:35:25,154 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:35:25,159 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:35:25,917 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402000] due to args.save_total_limit\n",
            "{'loss': 1.8187, 'learning_rate': 4.54388638192818e-05, 'epoch': 1.82}\n",
            "  9% 403500/4423240 [10:40:39<118:17:19,  9.44it/s][INFO|trainer.py:1937] 2021-09-13 20:36:13,601 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:36:13,608 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:36:13,818 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:36:13,824 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:36:13,829 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:36:14,752 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-402500] due to args.save_total_limit\n",
            "{'loss': 1.6706, 'learning_rate': 4.543321185375426e-05, 'epoch': 1.83}\n",
            "  9% 404000/4423240 [10:41:25<79:38:13, 14.02it/s][INFO|trainer.py:1937] 2021-09-13 20:36:59,170 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:36:59,179 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:36:59,367 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:36:59,373 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:36:59,378 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:37:00,323 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403000] due to args.save_total_limit\n",
            "{'loss': 1.6364, 'learning_rate': 4.5427559888226734e-05, 'epoch': 1.83}\n",
            "  9% 404500/4423240 [10:42:13<106:43:18, 10.46it/s][INFO|trainer.py:1937] 2021-09-13 20:37:47,161 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:37:47,169 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:37:47,425 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:37:47,432 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:37:47,437 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:37:48,233 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-403500] due to args.save_total_limit\n",
            "{'loss': 1.7207, 'learning_rate': 4.5421907922699206e-05, 'epoch': 1.83}\n",
            "  9% 405000/4423240 [10:43:02<84:38:29, 13.19it/s][INFO|trainer.py:1937] 2021-09-13 20:38:35,990 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:38:35,997 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:38:36,187 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:38:36,193 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:38:36,199 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:38:36,962 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404000] due to args.save_total_limit\n",
            "{'loss': 1.6544, 'learning_rate': 4.541625595717166e-05, 'epoch': 1.83}\n",
            "  9% 405500/4423240 [10:43:48<107:36:36, 10.37it/s][INFO|trainer.py:1937] 2021-09-13 20:39:22,001 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:39:22,009 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:39:22,243 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:39:22,250 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:39:22,255 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:39:23,209 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-404500] due to args.save_total_limit\n",
            "{'loss': 1.6843, 'learning_rate': 4.5410603991644135e-05, 'epoch': 1.84}\n",
            "  9% 406000/4423240 [10:44:34<99:24:01, 11.23it/s][INFO|trainer.py:1937] 2021-09-13 20:40:08,519 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:40:08,526 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:40:08,716 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:40:08,721 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:40:08,726 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:40:09,894 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405000] due to args.save_total_limit\n",
            "{'loss': 1.652, 'learning_rate': 4.5404952026116606e-05, 'epoch': 1.84}\n",
            "  9% 406500/4423240 [10:45:23<86:37:59, 12.88it/s][INFO|trainer.py:1937] 2021-09-13 20:40:57,462 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:40:57,471 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:40:57,685 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:40:57,692 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:40:57,698 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:40:58,505 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-405500] due to args.save_total_limit\n",
            "{'loss': 1.7507, 'learning_rate': 4.539930006058907e-05, 'epoch': 1.84}\n",
            "  9% 407000/4423240 [10:46:10<101:06:50, 11.03it/s][INFO|trainer.py:1937] 2021-09-13 20:41:44,299 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:41:44,307 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:41:44,547 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:41:44,553 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:41:44,559 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:41:45,561 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406000] due to args.save_total_limit\n",
            "{'loss': 1.6282, 'learning_rate': 4.539364809506154e-05, 'epoch': 1.84}\n",
            "  9% 407500/4423240 [10:47:01<109:49:10, 10.16it/s][INFO|trainer.py:1937] 2021-09-13 20:42:35,027 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:42:35,034 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:42:35,235 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:42:35,260 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:42:35,268 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:42:36,278 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-406500] due to args.save_total_limit\n",
            "{'loss': 1.6918, 'learning_rate': 4.538799612953401e-05, 'epoch': 1.84}\n",
            "  9% 408000/4423240 [10:47:49<104:09:50, 10.71it/s][INFO|trainer.py:1937] 2021-09-13 20:43:22,933 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:43:22,941 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:43:23,141 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:43:23,147 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:43:23,153 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:43:24,015 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407000] due to args.save_total_limit\n",
            "{'loss': 1.6764, 'learning_rate': 4.538234416400648e-05, 'epoch': 1.85}\n",
            "  9% 408500/4423240 [10:48:37<102:41:33, 10.86it/s][INFO|trainer.py:1937] 2021-09-13 20:44:11,208 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:44:11,218 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:44:11,397 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:44:11,403 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:44:11,409 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:44:12,492 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-407500] due to args.save_total_limit\n",
            "{'loss': 1.7912, 'learning_rate': 4.537669219847894e-05, 'epoch': 1.85}\n",
            "  9% 409000/4423240 [10:49:26<86:39:53, 12.87it/s][INFO|trainer.py:1937] 2021-09-13 20:44:59,866 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:44:59,873 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:45:00,074 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:45:00,080 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:45:00,085 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:45:00,916 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408000] due to args.save_total_limit\n",
            "{'loss': 1.649, 'learning_rate': 4.5371040232951414e-05, 'epoch': 1.85}\n",
            "  9% 409500/4423240 [10:50:13<86:21:49, 12.91it/s][INFO|trainer.py:1937] 2021-09-13 20:45:47,243 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:45:47,250 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:45:47,451 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:45:47,460 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:45:47,465 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:45:49,111 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-408500] due to args.save_total_limit\n",
            "{'loss': 1.6641, 'learning_rate': 4.536538826742388e-05, 'epoch': 1.85}\n",
            "  9% 410000/4423240 [10:51:03<103:47:49, 10.74it/s][INFO|trainer.py:1937] 2021-09-13 20:46:37,007 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:46:37,032 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:46:37,211 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:46:37,217 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:46:37,222 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:46:38,086 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409000] due to args.save_total_limit\n",
            "{'loss': 1.6518, 'learning_rate': 4.535973630189635e-05, 'epoch': 1.86}\n",
            "  9% 410500/4423240 [10:51:51<112:14:34,  9.93it/s][INFO|trainer.py:1937] 2021-09-13 20:47:25,335 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:47:25,343 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:47:25,588 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:47:25,593 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:47:25,598 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:47:26,696 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-409500] due to args.save_total_limit\n",
            "{'loss': 1.6909, 'learning_rate': 4.535408433636882e-05, 'epoch': 1.86}\n",
            "  9% 411000/4423240 [10:52:41<110:28:22, 10.09it/s][INFO|trainer.py:1937] 2021-09-13 20:48:15,791 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:48:15,799 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:48:15,970 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:48:15,976 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:48:15,982 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:48:17,039 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410000] due to args.save_total_limit\n",
            "{'loss': 1.7586, 'learning_rate': 4.5348432370841286e-05, 'epoch': 1.86}\n",
            "  9% 411500/4423240 [10:53:31<91:06:31, 12.23it/s][INFO|trainer.py:1937] 2021-09-13 20:49:05,095 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:49:05,103 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:49:05,278 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:49:05,284 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:49:05,289 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:49:06,122 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-410500] due to args.save_total_limit\n",
            "{'loss': 1.7212, 'learning_rate': 4.534278040531376e-05, 'epoch': 1.86}\n",
            "  9% 412000/4423240 [10:54:19<88:58:01, 12.52it/s][INFO|trainer.py:1937] 2021-09-13 20:49:53,023 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:49:53,031 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:49:53,204 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:49:53,210 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:49:53,216 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:49:54,302 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411000] due to args.save_total_limit\n",
            "{'loss': 1.8065, 'learning_rate': 4.533712843978622e-05, 'epoch': 1.87}\n",
            "  9% 412500/4423240 [10:55:07<107:31:22, 10.36it/s][INFO|trainer.py:1937] 2021-09-13 20:50:41,656 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:50:41,663 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:50:41,852 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:50:41,869 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:50:41,875 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:50:42,663 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-411500] due to args.save_total_limit\n",
            "{'loss': 1.8497, 'learning_rate': 4.5331476474258686e-05, 'epoch': 1.87}\n",
            "  9% 413000/4423240 [10:55:57<123:35:05,  9.01it/s][INFO|trainer.py:1937] 2021-09-13 20:51:31,038 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:51:31,047 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:51:31,227 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:51:31,248 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:51:31,268 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:51:32,048 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412000] due to args.save_total_limit\n",
            "{'loss': 1.7069, 'learning_rate': 4.532582450873116e-05, 'epoch': 1.87}\n",
            "  9% 413500/4423240 [10:56:45<96:14:50, 11.57it/s][INFO|trainer.py:1937] 2021-09-13 20:52:18,812 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:52:18,821 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:52:19,010 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:52:19,015 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:52:19,020 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:52:19,869 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-412500] due to args.save_total_limit\n",
            "{'loss': 1.6765, 'learning_rate': 4.532017254320363e-05, 'epoch': 1.87}\n",
            "  9% 414000/4423240 [10:57:30<95:28:51, 11.66it/s][INFO|trainer.py:1937] 2021-09-13 20:53:04,599 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:53:04,606 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:53:04,821 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:53:04,827 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:53:04,833 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:53:06,063 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413000] due to args.save_total_limit\n",
            "{'loss': 1.7407, 'learning_rate': 4.531452057767609e-05, 'epoch': 1.87}\n",
            "  9% 414500/4423240 [10:58:18<87:23:00, 12.74it/s][INFO|trainer.py:1937] 2021-09-13 20:53:52,138 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:53:52,146 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:53:52,340 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:53:52,346 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:53:52,351 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:53:53,106 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-413500] due to args.save_total_limit\n",
            "{'loss': 1.6291, 'learning_rate': 4.5308868612148565e-05, 'epoch': 1.88}\n",
            "  9% 415000/4423240 [10:59:06<106:27:55, 10.46it/s][INFO|trainer.py:1937] 2021-09-13 20:54:40,513 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:54:40,522 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:54:40,700 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:54:40,706 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:54:40,711 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:54:41,652 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414000] due to args.save_total_limit\n",
            "{'loss': 1.7869, 'learning_rate': 4.5303216646621036e-05, 'epoch': 1.88}\n",
            "  9% 415500/4423240 [10:59:54<107:14:38, 10.38it/s][INFO|trainer.py:1937] 2021-09-13 20:55:27,965 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:55:27,985 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:55:28,162 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:55:28,167 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:55:28,172 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:55:29,164 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-414500] due to args.save_total_limit\n",
            "{'loss': 1.7181, 'learning_rate': 4.52975646810935e-05, 'epoch': 1.88}\n",
            "  9% 416000/4423240 [11:00:42<95:02:06, 11.71it/s][INFO|trainer.py:1937] 2021-09-13 20:56:16,424 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:56:16,432 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:56:16,606 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:56:16,612 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:56:16,617 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:56:17,438 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415000] due to args.save_total_limit\n",
            "{'loss': 1.6588, 'learning_rate': 4.5291912715565965e-05, 'epoch': 1.88}\n",
            "  9% 416500/4423240 [11:01:31<132:16:59,  8.41it/s][INFO|trainer.py:1937] 2021-09-13 20:57:05,246 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:57:05,253 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:57:05,460 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:57:05,479 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:57:05,484 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:57:06,278 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-415500] due to args.save_total_limit\n",
            "{'loss': 1.6302, 'learning_rate': 4.5286260750038436e-05, 'epoch': 1.89}\n",
            "  9% 417000/4423240 [11:02:18<79:46:41, 13.95it/s][INFO|trainer.py:1937] 2021-09-13 20:57:52,123 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417000\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:57:52,132 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417000/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:57:52,333 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:57:52,344 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:57:52,350 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417000/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:57:53,582 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416000] due to args.save_total_limit\n",
            "{'loss': 1.7414, 'learning_rate': 4.52806087845109e-05, 'epoch': 1.89}\n",
            "  9% 417500/4423240 [11:03:05<83:34:59, 13.31it/s][INFO|trainer.py:1937] 2021-09-13 20:58:39,722 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417500\n",
            "[INFO|configuration_utils.py:404] 2021-09-13 20:58:39,730 >> Configuration saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417500/config.json\n",
            "[INFO|modeling_utils.py:1001] 2021-09-13 20:58:39,942 >> Model weights saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2022] 2021-09-13 20:58:39,947 >> tokenizer config file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2028] 2021-09-13 20:58:39,955 >> Special tokens file saved in /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-417500/special_tokens_map.json\n",
            "[INFO|trainer.py:2013] 2021-09-13 20:58:41,180 >> Deleting older checkpoint [/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-416500] due to args.save_total_limit\n",
            "{'loss': 1.7044, 'learning_rate': 4.527495681898337e-05, 'epoch': 1.89}\n",
            "  9% 418000/4423240 [11:03:54<108:11:48, 10.28it/s][INFO|trainer.py:1937] 2021-09-13 20:59:28,322 >> Saving model checkpoint to /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model/checkpoint-418000\n",
            "OSError: [Errno 28] No space left on device\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 550, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 501, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1342, in train\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1451, in _maybe_log_save_evaluate\n",
            "    self._save_checkpoint(model, trial, metrics=metrics)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1511, in _save_checkpoint\n",
            "    self.save_model(output_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1903, in save_model\n",
            "    self._save(output_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1951, in _save\n",
            "    self.model.save_pretrained(output_dir, state_dict=state_dict)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 987, in save_pretrained\n",
            "    model_to_save.config.save_pretrained(save_directory)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 403, in save_pretrained\n",
            "    self.to_json_file(output_config_file, use_diff=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 725, in to_json_file\n",
            "    writer.write(self.to_json_string(use_diff=use_diff))\n",
            "OSError: [Errno 28] No space left on device\n",
            "  9% 418000/4423240 [11:03:54<106:01:33, 10.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QGY7F6YUQPe"
      },
      "source": [
        "# language model 만들었으면... 여기서 부터~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj-fQYF-L4gB"
      },
      "source": [
        "model_name = 'albert-base-v2'\n",
        "#model_name = 'albert-large-v2'\n",
        "#model_name = 'albert-xlarge-v2'\n",
        "#model_name = 'albert-xxlarge-v2'\n",
        "#model_name = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhnMlOQ-9YiH",
        "outputId": "07de0db0-68d9-419c-fb52-95c89b574cfa"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "op = tokenizer(\"ais (xx) australian digital aeronautical flight information file edition 8 (ausdafif ed 8) withdrawn. replacement version australian digital aeronautical flight information file edition 8 version 2 (ausdafif ed 8 v2) available via ais-af intranet. mil use only.\", return_tensors=\"pt\")\n",
        "print(op)\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in op['input_ids'].tolist()[0]]))\n",
        "print(\"Tokens (int)      : {}\".format(op['input_ids'].tolist()[0]))\n",
        "print(\"Tokens (attn_mask): {}\\n\".format(op['attention_mask'].tolist()[0]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    2,    21,   403,    13,     5,  8962,     6,   933,  1888, 26976,\n",
            "          1690,   676,  3893,  1322,   469,    13,     5,  1346,    18,    43,\n",
            "          2565,   821,    13,    69,   469,     6,  9682,     9,  4610,   615,\n",
            "           933,  1888, 26976,  1690,   676,  3893,  1322,   469,   615,   172,\n",
            "            13,     5,  1346,    18,    43,  2565,   821,    13,    69,   469,\n",
            "           566,   135,     6,   904,  1197,    21,   403,     8,  2565, 14369,\n",
            "          2328,     9,  7097,   275,   104,     9,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tokens (str)      : ['[CLS]', '▁a', 'is', '▁', '(', 'xx', ')', '▁australian', '▁digital', '▁aeronautical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', '(', 'au', 's', 'd', 'af', 'if', '▁', 'ed', '▁8', ')', '▁withdrawn', '.', '▁replacement', '▁version', '▁australian', '▁digital', '▁aeronautical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁version', '▁2', '▁', '(', 'au', 's', 'd', 'af', 'if', '▁', 'ed', '▁8', '▁v', '2', ')', '▁available', '▁via', '▁a', 'is', '-', 'af', '▁intra', 'net', '.', '▁mil', '▁use', '▁only', '.', '[SEP]']\n",
            "Tokens (int)      : [2, 21, 403, 13, 5, 8962, 6, 933, 1888, 26976, 1690, 676, 3893, 1322, 469, 13, 5, 1346, 18, 43, 2565, 821, 13, 69, 469, 6, 9682, 9, 4610, 615, 933, 1888, 26976, 1690, 676, 3893, 1322, 469, 615, 172, 13, 5, 1346, 18, 43, 2565, 821, 13, 69, 469, 566, 135, 6, 904, 1197, 21, 403, 8, 2565, 14369, 2328, 9, 7097, 275, 104, 9, 3]\n",
            "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4sl25hxXQPx"
      },
      "source": [
        "# Albert NOTAM Classification Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqdLO2agXWXN"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
        "\n",
        "\n",
        "class Notam_classification_model:\n",
        "    def __init__(self,pre_trained_model_name='albert-base-v2'):\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained(pre_trained_model_name)\n",
        "        self.model  = AlbertForSequenceClassification.from_pretrained(\n",
        "                                    pre_trained_model_name, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "                                    num_labels = 5, # The number of output labels--2 for binary classification.\n",
        "                                                    # You can increase this for multi-class tasks.   \n",
        "                                    output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "                                )\n",
        "        # If there's a GPU available...\n",
        "        if torch.cuda.is_available():    \n",
        "\n",
        "            # Tell PyTorch to use the GPU.    \n",
        "            self.device = torch.device(\"cuda\")\n",
        "\n",
        "            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "            print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "        # If not...\n",
        "        else:\n",
        "            print('No GPU available, using the CPU instead.')\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "    def __clean_text(self,txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        #txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        #txt = txt.replace(',','')\n",
        "        #txt = txt.replace('..','')\n",
        "        #txt = txt.replace('...','')\n",
        "        #txt = txt.replace('.','. ')\n",
        "        #txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')             \n",
        "        return txt.strip()        \n",
        "\n",
        "    # Function to calculate the accuracy of our predictions vs labels\n",
        "    def __flat_accuracy(self,preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "    def __format_time(self,elapsed):\n",
        "        '''\n",
        "        Takes a time in seconds and returns a string hh:mm:ss\n",
        "        '''\n",
        "        # Round to the nearest second.\n",
        "        elapsed_rounded = int(round((elapsed)))\n",
        "        \n",
        "        # Format as hh:mm:ss\n",
        "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "    def save_notam_model(self, output_dir = '/content/drive/MyDrive/NOTAM/notam_model'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    def load_notam_model(self,input_dir = '/content/drive/MyDrive/NOTAM/notam_model/For_E_Section'):\n",
        "        print('Loading Albert notam model...')\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained(input_dir)\n",
        "        self.model = AlbertForSequenceClassification.from_pretrained(input_dir)\n",
        "        #self.tokenizer.to(self.device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_train_dataset(self,data_frame,section='E',batch_size=96):\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        labels = []\n",
        "        # For every sentence...\n",
        "        for row in data_frame.iterrows():\n",
        "\n",
        "            if section=='E':\n",
        "                # E section\n",
        "                sent = self.__clean_text(str(row[1][11]).lower()) \n",
        "                # label for E Section\n",
        "                lb = row[1][3]-1\n",
        "            elif section=='AE':\n",
        "                # A section\n",
        "                sent = self.__clean_text(str(row[1][7]).lower())     \n",
        "                # E section\n",
        "                sent = sent + ' [SEP] ' + self.__clean_text(str(row[1][11]).lower()) \n",
        "                # label for AE_score\n",
        "                lb = row[1][4]-1                \n",
        "\n",
        "            labels.append(lb)\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 128,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        # Print sentence 0, now as a list of IDs.\n",
        "        print(\"Tokens (str)      : {}\".format([self.tokenizer.convert_ids_to_tokens(s) for s in input_ids[0].tolist()]))\n",
        "        print('Token IDs:', input_ids[0])\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        #batch_size = 64+32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = batch_size # Evaluate with this batch size.\n",
        "                )\n",
        "        return train_dataloader,validation_dataloader        \n",
        "\n",
        "\n",
        "    def train(self,epochs=1,train_dataloader=None,validation_dataloader=None,output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section'):\n",
        "\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.model.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.model.named_parameters())\n",
        "\n",
        "        print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        "        \n",
        "        print(self.model)\n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        optimizer = AdamW(self.model.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.model.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = self.__format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(self.device)\n",
        "                b_input_mask = batch[1].to(self.device)\n",
        "                b_labels = batch[2].to(self.device)\n",
        "\n",
        "                #print(b_input_ids)\n",
        "                #print(b_input_mask)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.model.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                '''\n",
        "                loss, logits = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                '''\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                loss = outputs[0]\n",
        "                logits = outputs[1]\n",
        "                \n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = self.__format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "            avg_val_loss, avg_val_accuracy, validation_time = self.validate(validation_dataloader)\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "        self.save_notam_model(output_dir)\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(self.__format_time(time.time()-total_t0)))\n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def validate(self,validation_dataloader=None):          \n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tracking variables \n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            \n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "            # the `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(self.device)\n",
        "            b_input_mask = batch[1].to(self.device)\n",
        "            b_labels = batch[2].to(self.device)\n",
        "            for l in batch[2]:\n",
        "                true_labels.append(l.item())        \n",
        "            # Tell pytorch not to bother with constructing the compute graph during\n",
        "            # the forward pass, since this is only needed for backprop (training).\n",
        "            with torch.no_grad():        \n",
        "\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                # values prior to applying an activation function like the softmax.\n",
        "                outputs = self.model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]            \n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            for li in logits:\n",
        "                pred_labels.append(np.argmax(li))\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += self.__flat_accuracy(logits, label_ids)\n",
        "            \n",
        "        print(\"  classification_report   \")    \n",
        "        print(classification_report(true_labels,pred_labels))\n",
        "        # Report the final accuracy for this validation run.\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "        \n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = self.__format_time(time.time() - t0)\n",
        "        \n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        return avg_val_loss, avg_val_accuracy, validation_time\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5XcNIyCaq7V",
        "outputId": "db41725f-a86a-49d0-dbb7-2d79d6b8c31f"
      },
      "source": [
        "ancm = Notam_classification_model(pre_trained_model_name='albert-base-v2')\n",
        "train_dataloader,validation_dataloader = ancm.get_train_dataset(df1)\n",
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Tokens (str)      : ['[CLS]', '▁south', '▁terminal', '▁ramp', '▁ice', '▁and', '▁1', '/8', 'in', '▁dry', '▁sn', '▁over', '▁compact', 'ed', '▁sn', '▁ob', 's', '▁at', '▁1902', '140', '120', '.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    2,   180,  3855,  9960,  1392,    17,   137, 21701,   108,  2273,\n",
            "         8912,    84,  8285,    69,  8912,  5122,    18,    35,  5401, 14331,\n",
            "        12054,     9,     3,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "13,496 training samples\n",
            "1,500 validation samples\n",
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.72\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.87      0.82       251\n",
            "           1       0.90      0.76      0.83       348\n",
            "           2       0.84      0.88      0.86       301\n",
            "           3       0.88      0.81      0.84       263\n",
            "           4       0.84      0.93      0.88       337\n",
            "\n",
            "    accuracy                           0.85      1500\n",
            "   macro avg       0.85      0.85      0.85      1500\n",
            "weighted avg       0.85      0.85      0.85      1500\n",
            "\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       251\n",
            "           1       0.94      0.87      0.90       348\n",
            "           2       0.92      0.86      0.89       301\n",
            "           3       0.83      0.92      0.87       263\n",
            "           4       0.94      0.92      0.93       337\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.89      0.90      0.89      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.32\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.86      0.90       251\n",
            "           1       0.88      0.91      0.89       348\n",
            "           2       0.86      0.92      0.89       301\n",
            "           3       0.96      0.82      0.88       263\n",
            "           4       0.89      0.96      0.92       337\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.90      0.89      0.90      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.33\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.92      0.91       251\n",
            "           1       0.93      0.89      0.91       348\n",
            "           2       0.91      0.86      0.88       301\n",
            "           3       0.86      0.91      0.89       263\n",
            "           4       0.93      0.94      0.94       337\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.90      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.29\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.89       251\n",
            "           1       0.89      0.89      0.89       348\n",
            "           2       0.90      0.87      0.89       301\n",
            "           3       0.90      0.90      0.90       263\n",
            "           4       0.95      0.92      0.94       337\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.90      0.90      0.90      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.32\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       251\n",
            "           1       0.91      0.88      0.90       348\n",
            "           2       0.88      0.90      0.89       301\n",
            "           3       0.91      0.89      0.90       263\n",
            "           4       0.94      0.94      0.94       337\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.90      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.31\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89       251\n",
            "           1       0.92      0.87      0.89       348\n",
            "           2       0.87      0.90      0.88       301\n",
            "           3       0.89      0.90      0.89       263\n",
            "           4       0.95      0.93      0.94       337\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.90      0.90      0.90      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.33\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89       251\n",
            "           1       0.92      0.89      0.90       348\n",
            "           2       0.89      0.88      0.88       301\n",
            "           3       0.86      0.90      0.88       263\n",
            "           4       0.96      0.92      0.94       337\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.90      0.90      0.90      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.34\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89       251\n",
            "           1       0.91      0.88      0.89       348\n",
            "           2       0.88      0.89      0.89       301\n",
            "           3       0.89      0.89      0.89       263\n",
            "           4       0.95      0.93      0.94       337\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.90      0.90      0.90      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       251\n",
            "           1       0.91      0.89      0.90       348\n",
            "           2       0.89      0.88      0.89       301\n",
            "           3       0.90      0.90      0.90       263\n",
            "           4       0.95      0.93      0.94       337\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.90      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:00:06\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_E_Section\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:28:05 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 0.7205787495518408,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8498697916666667,\n",
              "  'Valid. Loss': 0.44691540487110615,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 0.32573863361955535,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8979166666666667,\n",
              "  'Valid. Loss': 0.3235158659517765,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 0.2449863871149983,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8981770833333335,\n",
              "  'Valid. Loss': 0.33011379186064005,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.20047799571820185,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.9069010416666666,\n",
              "  'Valid. Loss': 0.2903577387332916,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.16196355007007612,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.9003906250000001,\n",
              "  'Valid. Loss': 0.32191570289433,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.132580993261109,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.90625,\n",
              "  'Valid. Loss': 0.3123427862301469,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.10563267083798951,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.9036458333333334,\n",
              "  'Valid. Loss': 0.32876985520124435,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.08752634964143553,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.9016927083333334,\n",
              "  'Valid. Loss': 0.3393236706033349,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.07250159950464541,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.9027343750000001,\n",
              "  'Valid. Loss': 0.34795965161174536,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.057457896689256875,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.9066406249999999,\n",
              "  'Valid. Loss': 0.3476506555452943,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTJ_imIKu757",
        "outputId": "e1207611-089e-4d54-ed90-f7663706c57f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "del ancm\n",
        "ancm = Notam_classification_model(pre_trained_model_name='albert-base-v2')\n",
        "train_dataloader,validation_dataloader = ancm.get_train_dataset(df1,section='AE')\n",
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_AE_Section')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Tokens (str)      : ['[CLS]', '▁pa', 'fa', '[SEP]', '▁south', '▁terminal', '▁ramp', '▁ice', '▁and', '▁1', '/8', 'in', '▁dry', '▁sn', '▁over', '▁compact', 'ed', '▁sn', '▁ob', 's', '▁at', '▁1902', '140', '120', '.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    2,  1562,  1473,     3,   180,  3855,  9960,  1392,    17,   137,\n",
            "        21701,   108,  2273,  8912,    84,  8285,    69,  8912,  5122,    18,\n",
            "           35,  5401, 14331, 12054,     9,     3,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "13,496 training samples\n",
            "1,500 validation samples\n",
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:19.\n",
            "\n",
            "  Average training loss: 0.93\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.63      0.72       337\n",
            "           1       0.67      0.78      0.72       475\n",
            "           2       0.47      0.37      0.41       187\n",
            "           3       0.59      0.54      0.56       236\n",
            "           4       0.62      0.78      0.69       265\n",
            "\n",
            "    accuracy                           0.66      1500\n",
            "   macro avg       0.64      0.62      0.62      1500\n",
            "weighted avg       0.66      0.66      0.65      1500\n",
            "\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 0.93\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.66      0.77       337\n",
            "           1       0.81      0.90      0.85       475\n",
            "           2       0.68      0.80      0.73       187\n",
            "           3       0.78      0.83      0.80       236\n",
            "           4       0.87      0.88      0.87       265\n",
            "\n",
            "    accuracy                           0.82      1500\n",
            "   macro avg       0.81      0.81      0.81      1500\n",
            "weighted avg       0.83      0.82      0.82      1500\n",
            "\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.77      0.83       337\n",
            "           1       0.86      0.91      0.88       475\n",
            "           2       0.77      0.74      0.76       187\n",
            "           3       0.83      0.81      0.82       236\n",
            "           4       0.81      0.94      0.87       265\n",
            "\n",
            "    accuracy                           0.85      1500\n",
            "   macro avg       0.84      0.83      0.83      1500\n",
            "weighted avg       0.85      0.85      0.84      1500\n",
            "\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       337\n",
            "           1       0.90      0.89      0.89       475\n",
            "           2       0.85      0.73      0.78       187\n",
            "           3       0.81      0.86      0.84       236\n",
            "           4       0.86      0.94      0.89       265\n",
            "\n",
            "    accuracy                           0.86      1500\n",
            "   macro avg       0.85      0.85      0.85      1500\n",
            "weighted avg       0.86      0.86      0.86      1500\n",
            "\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.42\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:19.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       337\n",
            "           1       0.92      0.87      0.90       475\n",
            "           2       0.82      0.73      0.77       187\n",
            "           3       0.84      0.90      0.87       236\n",
            "           4       0.89      0.94      0.91       265\n",
            "\n",
            "    accuracy                           0.87      1500\n",
            "   macro avg       0.86      0.86      0.86      1500\n",
            "weighted avg       0.87      0.87      0.87      1500\n",
            "\n",
            "  Accuracy: 0.87\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.82      0.86       337\n",
            "           1       0.88      0.92      0.90       475\n",
            "           2       0.77      0.84      0.80       187\n",
            "           3       0.90      0.87      0.88       236\n",
            "           4       0.92      0.91      0.91       265\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.87      0.87      0.87      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.37\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       337\n",
            "           1       0.91      0.89      0.90       475\n",
            "           2       0.80      0.83      0.82       187\n",
            "           3       0.89      0.90      0.90       236\n",
            "           4       0.92      0.91      0.91       265\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.88      0.88      0.88      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.34\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       337\n",
            "           1       0.92      0.88      0.90       475\n",
            "           2       0.82      0.80      0.81       187\n",
            "           3       0.86      0.92      0.89       236\n",
            "           4       0.91      0.91      0.91       265\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.87      0.87      0.87      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.37\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       337\n",
            "           1       0.93      0.87      0.90       475\n",
            "           2       0.82      0.80      0.81       187\n",
            "           3       0.85      0.92      0.88       236\n",
            "           4       0.92      0.90      0.91       265\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.87      0.87      0.87      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.87\n",
            "  Validation Loss: 0.38\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    141.    Elapsed: 0:01:32.\n",
            "  Batch   120  of    141.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       337\n",
            "           1       0.91      0.89      0.90       475\n",
            "           2       0.84      0.81      0.83       187\n",
            "           3       0.87      0.91      0.89       236\n",
            "           4       0.90      0.91      0.91       265\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.87      0.88      0.88      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.37\n",
            "  Validation took: 0:00:06\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_AE_Section\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:28:06 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 0.9304313837213719,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.6537760416666666,\n",
              "  'Valid. Loss': 0.9328352734446526,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 0.6426334398012634,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8143229166666668,\n",
              "  'Valid. Loss': 0.5385270863771439,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 0.45979171293847104,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8446614583333333,\n",
              "  'Valid. Loss': 0.4472093414515257,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.36082044888472725,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8598958333333333,\n",
              "  'Valid. Loss': 0.4233703427016735,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.29603947757195076,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8688802083333335,\n",
              "  'Valid. Loss': 0.39885253738611937,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.2553773353814233,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.875390625,\n",
              "  'Valid. Loss': 0.367082710377872,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.21474051401547506,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8798177083333332,\n",
              "  'Valid. Loss': 0.34404883440583944,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.1723791834310437,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8761718750000002,\n",
              "  'Valid. Loss': 0.36775026749819517,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.1399984416809488,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8748697916666668,\n",
              "  'Valid. Loss': 0.38000182900577784,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.11629775742478404,\n",
              "  'Training Time': '0:02:42',\n",
              "  'Valid. Accur.': 0.8781250000000002,\n",
              "  'Valid. Loss': 0.3734021047130227,\n",
              "  'Validation Time': '0:00:06',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcslLRH_vPci",
        "outputId": "719a9b5c-738c-4836-b645-800ea80f4063",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "del ancm\n",
        "ancm = Notam_classification_model(pre_trained_model_name='/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model')\n",
        "train_dataloader,validation_dataloader = ancm.get_train_dataset(df1,section='E')\n",
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section_nl')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['classifier.bias', 'albert.pooler.bias', 'classifier.weight', 'albert.pooler.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Tokens (str)      : ['[CLS]', '▁south', '▁terminal', '▁ramp', '▁ice', '▁and', '▁1/8', 'in', '▁dry', '▁sn', '▁over', '▁compact', 'ed', '▁sn', '▁obs', '▁at', '▁1902140', '120.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    5,   461,   695,   465,   470,   132,   337,   232,   313,   214,\n",
            "          376,   431,   274,   214,   185,   152, 13815,  7579,     6, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000])\n",
            "13,496 training samples\n",
            "1,500 validation samples\n",
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(32101, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 1.08\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.71      0.75       258\n",
            "           1       0.77      0.91      0.83       334\n",
            "           2       0.87      0.85      0.86       265\n",
            "           3       0.90      0.80      0.85       292\n",
            "           4       0.85      0.87      0.86       351\n",
            "\n",
            "    accuracy                           0.83      1500\n",
            "   macro avg       0.84      0.83      0.83      1500\n",
            "weighted avg       0.84      0.83      0.83      1500\n",
            "\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.49\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.76      0.82       258\n",
            "           1       0.82      0.94      0.88       334\n",
            "           2       0.89      0.89      0.89       265\n",
            "           3       0.92      0.83      0.87       292\n",
            "           4       0.89      0.94      0.92       351\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.88      0.87      0.88      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.36\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.84      0.87       258\n",
            "           1       0.87      0.94      0.90       334\n",
            "           2       0.89      0.89      0.89       265\n",
            "           3       0.93      0.88      0.90       292\n",
            "           4       0.92      0.93      0.93       351\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.90      0.90      0.90      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.30\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.90      0.89       258\n",
            "           1       0.89      0.95      0.92       334\n",
            "           2       0.88      0.90      0.89       265\n",
            "           3       0.95      0.89      0.92       292\n",
            "           4       0.96      0.92      0.94       351\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.29\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       258\n",
            "           1       0.89      0.95      0.92       334\n",
            "           2       0.89      0.89      0.89       265\n",
            "           3       0.94      0.88      0.91       292\n",
            "           4       0.95      0.93      0.94       351\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       258\n",
            "           1       0.90      0.95      0.92       334\n",
            "           2       0.89      0.88      0.88       265\n",
            "           3       0.93      0.89      0.91       292\n",
            "           4       0.93      0.94      0.94       351\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91       258\n",
            "           1       0.90      0.95      0.93       334\n",
            "           2       0.89      0.88      0.89       265\n",
            "           3       0.94      0.89      0.92       292\n",
            "           4       0.95      0.95      0.95       351\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.92      0.92      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.27\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.91       258\n",
            "           1       0.90      0.95      0.93       334\n",
            "           2       0.89      0.89      0.89       265\n",
            "           3       0.94      0.89      0.91       292\n",
            "           4       0.94      0.94      0.94       351\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.92      0.92      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.91       258\n",
            "           1       0.90      0.96      0.93       334\n",
            "           2       0.89      0.89      0.89       265\n",
            "           3       0.94      0.89      0.91       292\n",
            "           4       0.95      0.94      0.94       351\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.92      0.92      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.91      0.91       258\n",
            "           1       0.90      0.96      0.93       334\n",
            "           2       0.89      0.89      0.89       265\n",
            "           3       0.94      0.89      0.91       292\n",
            "           4       0.95      0.94      0.95       351\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.92      0.92      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:03\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_E_Section_nl\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:12:41 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 1.075042577288675,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8330729166666667,\n",
              "  'Valid. Loss': 0.48677776008844376,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 0.3756597836812337,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8772135416666665,\n",
              "  'Valid. Loss': 0.3564295740798116,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 0.2761236278089226,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8984375000000001,\n",
              "  'Valid. Loss': 0.30447534564882517,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.22542424542261352,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.9122395833333333,\n",
              "  'Valid. Loss': 0.28700374998152256,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.20067367105619283,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.912890625,\n",
              "  'Valid. Loss': 0.2767715398222208,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.18361326235722988,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.9108072916666667,\n",
              "  'Valid. Loss': 0.27938066702336073,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.1700251963755763,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.91875,\n",
              "  'Valid. Loss': 0.2739845188334584,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.1603089930317926,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.916015625,\n",
              "  'Valid. Loss': 0.2777406917884946,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.15254244776376596,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.916015625,\n",
              "  'Valid. Loss': 0.27924797125160694,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.14667481612017813,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.918359375,\n",
              "  'Valid. Loss': 0.2784544602036476,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7jSD-m4vgrb",
        "outputId": "5217aadc-ebdb-480a-c9fb-a6e59178075a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "del ancm\n",
        "ancm = Notam_classification_model(pre_trained_model_name='/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model')\n",
        "train_dataloader,validation_dataloader = ancm.get_train_dataset(df1,section='AE')\n",
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_AE_Section_nl')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['classifier.bias', 'albert.pooler.bias', 'classifier.weight', 'albert.pooler.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Tokens (str)      : ['[CLS]', '▁paf', 'a', '[SEP]', '▁south', '▁terminal', '▁ramp', '▁ice', '▁and', '▁1/8', 'in', '▁dry', '▁sn', '▁over', '▁compact', 'ed', '▁sn', '▁obs', '▁at', '▁1902140', '120.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    5,   466,   122,     6,   461,   695,   465,   470,   132,   337,\n",
            "          232,   313,   214,   376,   431,   274,   214,   185,   152, 13815,\n",
            "         7579,     6, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
            "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000])\n",
            "13,496 training samples\n",
            "1,500 validation samples\n",
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "AlbertForSequenceClassification(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(32101, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 1.20\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.65      0.71       306\n",
            "           1       0.76      0.84      0.80       483\n",
            "           2       0.53      0.47      0.50       182\n",
            "           3       0.65      0.62      0.63       250\n",
            "           4       0.78      0.87      0.82       279\n",
            "\n",
            "    accuracy                           0.73      1500\n",
            "   macro avg       0.70      0.69      0.69      1500\n",
            "weighted avg       0.72      0.73      0.72      1500\n",
            "\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 0.73\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.72      0.79       306\n",
            "           1       0.82      0.90      0.86       483\n",
            "           2       0.64      0.60      0.62       182\n",
            "           3       0.73      0.71      0.72       250\n",
            "           4       0.84      0.91      0.87       279\n",
            "\n",
            "    accuracy                           0.80      1500\n",
            "   macro avg       0.78      0.77      0.77      1500\n",
            "weighted avg       0.80      0.80      0.79      1500\n",
            "\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.77      0.82       306\n",
            "           1       0.84      0.93      0.88       483\n",
            "           2       0.75      0.63      0.68       182\n",
            "           3       0.76      0.78      0.77       250\n",
            "           4       0.86      0.89      0.88       279\n",
            "\n",
            "    accuracy                           0.83      1500\n",
            "   macro avg       0.82      0.80      0.81      1500\n",
            "weighted avg       0.83      0.83      0.83      1500\n",
            "\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.50\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.78      0.82       306\n",
            "           1       0.83      0.94      0.88       483\n",
            "           2       0.75      0.73      0.74       182\n",
            "           3       0.86      0.80      0.83       250\n",
            "           4       0.90      0.88      0.89       279\n",
            "\n",
            "    accuracy                           0.85      1500\n",
            "   macro avg       0.84      0.82      0.83      1500\n",
            "weighted avg       0.85      0.85      0.84      1500\n",
            "\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       306\n",
            "           1       0.88      0.92      0.89       483\n",
            "           2       0.74      0.77      0.76       182\n",
            "           3       0.87      0.82      0.84       250\n",
            "           4       0.89      0.91      0.90       279\n",
            "\n",
            "    accuracy                           0.86      1500\n",
            "   macro avg       0.85      0.84      0.85      1500\n",
            "weighted avg       0.86      0.86      0.86      1500\n",
            "\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.41\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.85       306\n",
            "           1       0.88      0.92      0.90       483\n",
            "           2       0.76      0.76      0.76       182\n",
            "           3       0.85      0.84      0.84       250\n",
            "           4       0.91      0.91      0.91       279\n",
            "\n",
            "    accuracy                           0.87      1500\n",
            "   macro avg       0.86      0.85      0.85      1500\n",
            "weighted avg       0.87      0.87      0.86      1500\n",
            "\n",
            "  Accuracy: 0.86\n",
            "  Validation Loss: 0.41\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.82      0.85       306\n",
            "           1       0.89      0.92      0.91       483\n",
            "           2       0.77      0.77      0.77       182\n",
            "           3       0.86      0.86      0.86       250\n",
            "           4       0.90      0.93      0.92       279\n",
            "\n",
            "    accuracy                           0.87      1500\n",
            "   macro avg       0.86      0.86      0.86      1500\n",
            "weighted avg       0.87      0.87      0.87      1500\n",
            "\n",
            "  Accuracy: 0.87\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.83      0.85       306\n",
            "           1       0.89      0.92      0.91       483\n",
            "           2       0.78      0.76      0.77       182\n",
            "           3       0.86      0.86      0.86       250\n",
            "           4       0.92      0.93      0.92       279\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.87      0.86      0.86      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.84      0.86       306\n",
            "           1       0.89      0.93      0.91       483\n",
            "           2       0.78      0.78      0.78       182\n",
            "           3       0.88      0.86      0.87       250\n",
            "           4       0.92      0.92      0.92       279\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.87      0.87      0.87      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    141.    Elapsed: 0:00:21.\n",
            "  Batch    80  of    141.    Elapsed: 0:00:42.\n",
            "  Batch   120  of    141.    Elapsed: 0:01:03.\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:01:13\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.84      0.86       306\n",
            "           1       0.89      0.93      0.91       483\n",
            "           2       0.79      0.79      0.79       182\n",
            "           3       0.88      0.86      0.87       250\n",
            "           4       0.92      0.91      0.92       279\n",
            "\n",
            "    accuracy                           0.88      1500\n",
            "   macro avg       0.87      0.87      0.87      1500\n",
            "weighted avg       0.88      0.88      0.88      1500\n",
            "\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.40\n",
            "  Validation took: 0:00:03\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_AE_Section_nl\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:12:41 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 1.2043506291741175,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.7231770833333334,\n",
              "  'Valid. Loss': 0.729401808232069,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 0.5891011748330813,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.7973958333333333,\n",
              "  'Valid. Loss': 0.5610933862626553,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 0.47209966077026744,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.827734375,\n",
              "  'Valid. Loss': 0.49674419686198235,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.40013058314509425,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8446614583333333,\n",
              "  'Valid. Loss': 0.4483519736677408,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.35701900033663353,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8584635416666667,\n",
              "  'Valid. Loss': 0.41387889534235,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.3225482592345975,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8649739583333332,\n",
              "  'Valid. Loss': 0.4119705930352211,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.29931062734718866,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8721354166666665,\n",
              "  'Valid. Loss': 0.39778896421194077,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.2851327265619386,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.875390625,\n",
              "  'Valid. Loss': 0.3974014725536108,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.27385886922372993,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8779947916666666,\n",
              "  'Valid. Loss': 0.39568910002708435,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.2624833873928861,\n",
              "  'Training Time': '0:01:13',\n",
              "  'Valid. Accur.': 0.8779947916666667,\n",
              "  'Valid. Loss': 0.39622843638062477,\n",
              "  'Validation Time': '0:00:03',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbIYET37BXQd"
      },
      "source": [
        "# albert_base_v2 + notam languge mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPd5nO9xBeJq"
      },
      "source": [
        "from transformers import AlbertModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AL_NL_Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        al_pre_trained_model = 'albert-base-v2'\n",
        "        nl_pre_trained_model = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model'\n",
        "        \n",
        "        #self.al_tokenizer = AlbertTokenizer.from_pretrained(al_pre_trained_model)\n",
        "        #self.nl_tokenizer = AlbertTokenizer.from_pretrained(nl_pre_trained_model)\n",
        "        self.al_bert = AlbertModel.from_pretrained(al_pre_trained_model, return_dict=True)\n",
        "        self.nl_bert = AlbertModel.from_pretrained(nl_pre_trained_model, return_dict=True)\n",
        "        \n",
        "        self.classifier = nn.Linear(self.al_bert.config.hidden_size + self.nl_bert.config.hidden_size, 5)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        #BCEWithLogitsLoss\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,n_input_ids, n_attention_mask,labels):\n",
        "        #ta = torch.tensor(np.eye(self.a_mapping_classes, dtype='uint8')[a_mapping])\n",
        "        #ta = F.one_hot(a_mapping, num_classes=self.a_mapping_classes).float()\n",
        "        output1 = self.al_bert(input_ids, attention_mask=attention_mask)\n",
        "        output2 = self.nl_bert(n_input_ids, attention_mask=n_attention_mask)\n",
        "        \n",
        "        output = self.classifier(torch.cat((output1.pooler_output,output2.pooler_output),1))\n",
        "        #output = torch.softmax(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return [loss, output]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VssU_mcLEMRV",
        "outputId": "4d669f83-56d8-4842-bcd0-fd8ce20ded59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mm = AL_NL_Model()\n",
        "print(mm)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AL_NL_Model(\n",
            "  (al_bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (nl_bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(32101, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (classifier): Linear(in_features=1536, out_features=5, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo6OvYhEEo2z"
      },
      "source": [
        "\n",
        "class Notam_classification_model2:\n",
        "    def __init__(self):\n",
        "        al_pre_trained_model = 'albert-base-v2'\n",
        "        nl_pre_trained_model = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model'\n",
        "        \n",
        "        self.al_tokenizer = AlbertTokenizer.from_pretrained(al_pre_trained_model)\n",
        "        self.nl_tokenizer = AlbertTokenizer.from_pretrained(nl_pre_trained_model)\n",
        "        self.model  = AL_NL_Model()\n",
        "        # If there's a GPU available...\n",
        "        if torch.cuda.is_available():    \n",
        "\n",
        "            # Tell PyTorch to use the GPU.    \n",
        "            self.device = torch.device(\"cuda\")\n",
        "\n",
        "            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "            print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "        # If not...\n",
        "        else:\n",
        "            print('No GPU available, using the CPU instead.')\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "    def __clean_text(self,txt):\n",
        "        txt = txt.replace('\\n',' ')\n",
        "        txt = txt.replace('\\r',' ')    \n",
        "        #txt = txt.replace('=','')\n",
        "        txt = txt.replace('\\\"','')   \n",
        "        txt = txt.replace('\\'','')\n",
        "        #txt = txt.replace(',','')\n",
        "        #txt = txt.replace('..','')\n",
        "        #txt = txt.replace('...','')\n",
        "        #txt = txt.replace('.','. ')\n",
        "        #txt = txt.replace('.','. ')\n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')           \n",
        "        txt = txt.replace('  ',' ')\n",
        "        txt = txt.replace('  ',' ')    \n",
        "        txt = txt.replace('  ',' ')   \n",
        "        txt = txt.replace('  ',' ')             \n",
        "        return txt.strip()        \n",
        "\n",
        "    # Function to calculate the accuracy of our predictions vs labels\n",
        "    def __flat_accuracy(self,preds, labels):\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        labels_flat = labels.flatten()\n",
        "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "    def __format_time(self,elapsed):\n",
        "        '''\n",
        "        Takes a time in seconds and returns a string hh:mm:ss\n",
        "        '''\n",
        "        # Round to the nearest second.\n",
        "        elapsed_rounded = int(round((elapsed)))\n",
        "        \n",
        "        # Format as hh:mm:ss\n",
        "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "    def save_notam_model(self, output_dir = '/content/drive/MyDrive/NOTAM/notam_model'):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model  # Take care of distributed/parallel training\n",
        "        #model_to_save.save_pretrained(output_dir)\n",
        "        #self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    def load_notam_model(self,input_dir = '/content/drive/MyDrive/NOTAM/notam_model/For_E_Section'):\n",
        "        print('Loading dual notam model...')\n",
        "        al_pre_trained_model = 'albert-base-v2'\n",
        "        nl_pre_trained_model = '/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model'\n",
        "        \n",
        "        self.al_tokenizer = AlbertTokenizer.from_pretrained(al_pre_trained_model)\n",
        "        self.nl_tokenizer = AlbertTokenizer.from_pretrained(nl_pre_trained_model)\n",
        "\n",
        "        self.model = AL_NL_Model.from_pretrained(input_dir)\n",
        "        #self.tokenizer.to(self.device)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_train_dataset(self,data_frame,section='E',batch_size=32):\n",
        "        # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        n_input_ids = []\n",
        "        n_attention_masks = []        \n",
        "        labels = []\n",
        "        # For every sentence...\n",
        "        for row in data_frame.iterrows():\n",
        "\n",
        "            if section=='E':\n",
        "                # E section\n",
        "                sent = self.__clean_text(str(row[1][11]).lower()) \n",
        "                # label for E Section\n",
        "                lb = row[1][3]-1\n",
        "            elif section=='AE':\n",
        "                # A section\n",
        "                sent = self.__clean_text(str(row[1][7]).lower())     \n",
        "                # E section\n",
        "                sent = sent + ' [SEP] ' + self.__clean_text(str(row[1][11]).lower()) \n",
        "                # label for AE_score\n",
        "                lb = row[1][4]-1                \n",
        "\n",
        "            labels.append(lb)\n",
        "            # `encode_plus` will:\n",
        "            #   (1) Tokenize the sentence.\n",
        "            #   (2) Prepend the `[CLS]` token to the start.\n",
        "            #   (3) Append the `[SEP]` token to the end.\n",
        "            #   (4) Map tokens to their IDs.\n",
        "            #   (5) Pad or truncate the sentence to `max_length`\n",
        "            #   (6) Create attention masks for [PAD] tokens.\n",
        "            encoded_dict = self.al_tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 128,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            input_ids.append(encoded_dict['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "            encoded_dict2 = self.nl_tokenizer.encode_plus(\n",
        "                                sent,                      # Sentence to encode.\n",
        "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                max_length = 128,           # Pad & truncate all sentences.\n",
        "                                pad_to_max_length = True,\n",
        "                                return_attention_mask = True,   # Construct attn. masks.\n",
        "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                truncation = True,\n",
        "                        )\n",
        "            \n",
        "            # Add the encoded sentence to the list.    \n",
        "            n_input_ids.append(encoded_dict2['input_ids'])\n",
        "            \n",
        "            # And its attention mask (simply differentiates padding from non-padding).\n",
        "            n_attention_masks.append(encoded_dict2['attention_mask'])\n",
        "\n",
        "        # Convert the lists into tensors.\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        n_input_ids = torch.cat(n_input_ids, dim=0)\n",
        "        n_attention_masks = torch.cat(n_attention_masks, dim=0)\n",
        "\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "\n",
        "        # Training & Validation Split\n",
        "        # Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "        # Combine the training inputs into a TensorDataset.\n",
        "        dataset = TensorDataset(input_ids, attention_masks,n_input_ids, n_attention_masks, labels)\n",
        "\n",
        "        # Create a 90-10 train-validation split.\n",
        "\n",
        "        # Calculate the number of samples to include in each set.\n",
        "        train_size = int(0.9 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        print('{:>5,} training samples'.format(train_size))\n",
        "        print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "        # The DataLoader needs to know our batch size for training, so we specify it \n",
        "        # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "        # size of 16 or 32.\n",
        "        #batch_size = 64+32\n",
        "\n",
        "        # Create the DataLoaders for our training and validation sets.\n",
        "        # We'll take training samples in random order. \n",
        "        train_dataloader = DataLoader(\n",
        "                    train_dataset,  # The training samples.\n",
        "                    sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "                    batch_size = batch_size # Trains with this batch size.\n",
        "                )\n",
        "\n",
        "        # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "        validation_dataloader = DataLoader(\n",
        "                    val_dataset, # The validation samples.\n",
        "                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "                    batch_size = batch_size # Evaluate with this batch size.\n",
        "                )\n",
        "        return train_dataloader,validation_dataloader        \n",
        "\n",
        "\n",
        "    def train(self,epochs=1,train_dataloader=None,validation_dataloader=None,output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section'):\n",
        "\n",
        "        # Tell pytorch to run this model on the GPU.\n",
        "        self.model.cuda()\n",
        "\n",
        "        # Get all of the model's parameters as a list of tuples.\n",
        "        params = list(self.model.named_parameters())\n",
        "\n",
        "        print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        "        \n",
        "        print(self.model)\n",
        "\n",
        "        # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "        # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "        optimizer = AdamW(self.model.parameters(),\n",
        "                        lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                        eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                        )\n",
        "\n",
        "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "        # training data.\n",
        "        #epochs = 2\n",
        "\n",
        "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "        # (Note that this is not the same as the number of training samples).\n",
        "        total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "        # Create the learning rate scheduler.\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                    num_training_steps = total_steps)\n",
        "            \n",
        "        # This training code is based on the `run_glue.py` script here:\n",
        "        # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "        # Set the seed value all over the place to make this reproducible.\n",
        "        seed_val = 42\n",
        "\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # We'll store a number of quantities such as training and validation loss, \n",
        "        # validation accuracy, and timings.\n",
        "        training_stats = []\n",
        "\n",
        "        # Measure the total training time for the whole run.\n",
        "        total_t0 = time.time()\n",
        "\n",
        "        # For each epoch...\n",
        "        for epoch_i in range(0, epochs):\n",
        "            \n",
        "            # ========================================\n",
        "            #               Training\n",
        "            # ========================================\n",
        "            \n",
        "            # Perform one full pass over the training set.\n",
        "\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "\n",
        "            # Measure how long the training epoch takes.\n",
        "            t0 = time.time()\n",
        "\n",
        "            # Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "\n",
        "            # Put the model into training mode. Don't be mislead--the call to \n",
        "            # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "            # `dropout` and `batchnorm` layers behave differently during training\n",
        "            # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "            self.model.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                # Progress update every 40 batches.\n",
        "                if step % 40 == 0 and not step == 0:\n",
        "                    # Calculate elapsed time in minutes.\n",
        "                    elapsed = self.__format_time(time.time() - t0)\n",
        "                    \n",
        "                    # Report progress.\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "                # Unpack this training batch from our dataloader. \n",
        "                #\n",
        "                # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "                # `to` method.\n",
        "                #\n",
        "                # `batch` contains three pytorch tensors:\n",
        "                #   [0]: input ids \n",
        "                #   [1]: attention masks\n",
        "                #   [2]: labels \n",
        "                b_input_ids = batch[0].to(self.device)\n",
        "                b_input_mask = batch[1].to(self.device)\n",
        "                b_n_input_ids = batch[2].to(self.device)\n",
        "                b_n_input_mask = batch[3].to(self.device)                \n",
        "                b_labels = batch[4].to(self.device)\n",
        "\n",
        "                #print(b_input_ids)\n",
        "                #print(b_input_mask)\n",
        "\n",
        "                # Always clear any previously calculated gradients before performing a\n",
        "                # backward pass. PyTorch doesn't do this automatically because \n",
        "                # accumulating the gradients is \"convenient while training RNNs\". \n",
        "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "                self.model.zero_grad()        \n",
        "\n",
        "                # Perform a forward pass (evaluate the model on this training batch).\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # It returns different numbers of parameters depending on what arguments\n",
        "                # arge given and what flags are set. For our useage here, it returns\n",
        "                # the loss (because we provided labels) and the \"logits\"--the model\n",
        "                # outputs prior to activation.\n",
        "\n",
        "                outputs = self.model(b_input_ids,b_input_mask,b_n_input_ids,b_n_input_mask,b_labels)\n",
        "                '''\n",
        "                loss, logits = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask, \n",
        "                                    labels=b_labels)\n",
        "                '''\n",
        "                # Accumulate the training loss over all of the batches so that we can\n",
        "                # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "                # single value; the `.item()` function just returns the Python value \n",
        "                # from the tensor.\n",
        "                loss = outputs[0]\n",
        "                logits = outputs[1]\n",
        "                \n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate the gradients.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0.\n",
        "                # This is to help prevent the \"exploding gradients\" problem.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and take a step using the computed gradient.\n",
        "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "                # modified based on their gradients, the learning rate, etc.\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the learning rate.\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "            \n",
        "            # Measure how long this epoch took.\n",
        "            training_time = self.__format_time(time.time() - t0)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "            print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "            avg_val_loss, avg_val_accuracy, validation_time = self.validate(validation_dataloader)\n",
        "            # Record all statistics from this epoch.\n",
        "            training_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch_i + 1,\n",
        "                    'Training Loss': avg_train_loss,\n",
        "                    'Valid. Loss': avg_val_loss,\n",
        "                    'Valid. Accur.': avg_val_accuracy,\n",
        "                    'Training Time': training_time,\n",
        "                    'Validation Time': validation_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "        self.save_notam_model(output_dir)\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(self.__format_time(time.time()-total_t0)))\n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def validate(self,validation_dataloader=None):          \n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        self.model.eval()\n",
        "\n",
        "        # Tracking variables \n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            \n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "            # the `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(self.device)\n",
        "            b_input_mask = batch[1].to(self.device)\n",
        "            b_n_input_ids = batch[2].to(self.device)\n",
        "            b_n_input_mask = batch[3].to(self.device)             \n",
        "            b_labels = batch[4].to(self.device)\n",
        "            for l in batch[4]:\n",
        "                true_labels.append(l.item())        \n",
        "            # Tell pytorch not to bother with constructing the compute graph during\n",
        "            # the forward pass, since this is only needed for backprop (training).\n",
        "            with torch.no_grad():        \n",
        "\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                # The documentation for this `model` function is here: \n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "                # values prior to applying an activation function like the softmax.\n",
        "                outputs = self.model(b_input_ids,b_input_mask,b_n_input_ids,b_n_input_mask,b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]            \n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            for li in logits:\n",
        "                pred_labels.append(np.argmax(li))\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += self.__flat_accuracy(logits, label_ids)\n",
        "            \n",
        "        print(\"  classification_report   \")    \n",
        "        print(classification_report(true_labels,pred_labels))\n",
        "        # Report the final accuracy for this validation run.\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "        \n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = self.__format_time(time.time() - t0)\n",
        "        \n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        return avg_val_loss, avg_val_accuracy, validation_time\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtZ81KFsHzYt",
        "outputId": "42081f9e-373a-44b2-b53d-394beedd104e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "del ancm\n",
        "ancm = Notam_classification_model2()\n",
        "train_dataloader,validation_dataloader = ancm.get_train_dataset(df1)\n",
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section_dual')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "13,496 training samples\n",
            "1,500 validation samples\n",
            "The Albert model has 52 different named parameters.\n",
            "\n",
            "AL_NL_Model(\n",
            "  (al_bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (nl_bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(32101, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (classifier): Linear(in_features=1536, out_features=5, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:31.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       254\n",
            "           1       0.93      0.94      0.93       359\n",
            "           2       0.90      0.90      0.90       294\n",
            "           3       0.87      0.83      0.85       259\n",
            "           4       0.90      0.93      0.91       334\n",
            "\n",
            "    accuracy                           0.90      1500\n",
            "   macro avg       0.89      0.89      0.89      1500\n",
            "weighted avg       0.90      0.90      0.90      1500\n",
            "\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.32\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:31.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.85      0.89       254\n",
            "           1       0.94      0.95      0.95       359\n",
            "           2       0.87      0.90      0.89       294\n",
            "           3       0.92      0.84      0.88       259\n",
            "           4       0.87      0.97      0.92       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.90      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.29\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:31.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.90       254\n",
            "           1       0.94      0.95      0.95       359\n",
            "           2       0.92      0.88      0.90       294\n",
            "           3       0.89      0.89      0.89       259\n",
            "           4       0.92      0.95      0.94       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.27\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:31.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.87      0.90       254\n",
            "           1       0.93      0.97      0.95       359\n",
            "           2       0.89      0.92      0.90       294\n",
            "           3       0.93      0.85      0.89       259\n",
            "           4       0.91      0.94      0.93       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.91      0.91      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.29\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:31.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.86      0.90       254\n",
            "           1       0.95      0.95      0.95       359\n",
            "           2       0.90      0.90      0.90       294\n",
            "           3       0.92      0.87      0.89       259\n",
            "           4       0.88      0.98      0.93       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.91      0.91      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.31\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89       254\n",
            "           1       0.95      0.94      0.94       359\n",
            "           2       0.91      0.89      0.90       294\n",
            "           3       0.91      0.87      0.89       259\n",
            "           4       0.90      0.96      0.93       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.31\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.86      0.90       254\n",
            "           1       0.92      0.96      0.94       359\n",
            "           2       0.92      0.90      0.91       294\n",
            "           3       0.91      0.90      0.90       259\n",
            "           4       0.91      0.95      0.93       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.91      0.91      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.33\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.87      0.89       254\n",
            "           1       0.94      0.94      0.94       359\n",
            "           2       0.92      0.89      0.91       294\n",
            "           3       0.91      0.90      0.90       259\n",
            "           4       0.90      0.96      0.93       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.91      0.91      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.36\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90       254\n",
            "           1       0.93      0.94      0.94       359\n",
            "           2       0.91      0.90      0.91       294\n",
            "           3       0.92      0.89      0.91       259\n",
            "           4       0.92      0.96      0.94       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.92      0.92      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.35\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89       254\n",
            "           1       0.94      0.94      0.94       359\n",
            "           2       0.91      0.90      0.90       294\n",
            "           3       0.91      0.90      0.90       259\n",
            "           4       0.91      0.96      0.93       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.92      0.91      0.91      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.37\n",
            "  Validation took: 0:00:09\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_E_Section_dual\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:42:41 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 0.5314241004533959,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.8979863221884499,\n",
              "  'Valid. Loss': 0.3185532769941269,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 0.25895189402086477,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9086246200607904,\n",
              "  'Valid. Loss': 0.2927304589130143,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 0.21172192734246853,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.916033434650456,\n",
              "  'Valid. Loss': 0.27086311071477037,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.172960882902728,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.916033434650456,\n",
              "  'Valid. Loss': 0.2915981403611442,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.14973988628646104,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.916033434650456,\n",
              "  'Valid. Loss': 0.31425855681300163,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.12553932757770103,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9133738601823709,\n",
              "  'Valid. Loss': 0.3144508606575905,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.10939338241711533,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9173632218844985,\n",
              "  'Valid. Loss': 0.3279030025401648,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.09243383576102572,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.916033434650456,\n",
              "  'Valid. Loss': 0.35748543461507304,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.07653444795731358,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9200227963525837,\n",
              "  'Valid. Loss': 0.34926670516266467,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.0639699241229205,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9173632218844985,\n",
              "  'Valid. Loss': 0.3695231918245554,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkL4zp2LXGVD",
        "outputId": "a9b7504c-f44d-411f-f5b8-7358a72e45e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ancm.train(epochs=10, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, output_dir='/content/drive/MyDrive/NOTAM/notam_model/For_E_Section_dual')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Albert model has 52 different named parameters.\n",
            "\n",
            "AL_NL_Model(\n",
            "  (al_bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (nl_bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(32101, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (classifier): Linear(in_features=1536, out_features=5, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.87      0.89       254\n",
            "           1       0.95      0.94      0.94       359\n",
            "           2       0.89      0.89      0.89       294\n",
            "           3       0.91      0.90      0.90       259\n",
            "           4       0.91      0.96      0.94       334\n",
            "\n",
            "    accuracy                           0.92      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.92      0.92      0.92      1500\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.88       254\n",
            "           1       0.91      0.96      0.93       359\n",
            "           2       0.89      0.91      0.90       294\n",
            "           3       0.92      0.87      0.89       259\n",
            "           4       0.91      0.95      0.93       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.49\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.89       254\n",
            "           1       0.94      0.95      0.94       359\n",
            "           2       0.91      0.89      0.90       294\n",
            "           3       0.88      0.90      0.89       259\n",
            "           4       0.93      0.93      0.93       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.48\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89       254\n",
            "           1       0.94      0.94      0.94       359\n",
            "           2       0.87      0.90      0.88       294\n",
            "           3       0.91      0.89      0.90       259\n",
            "           4       0.93      0.95      0.94       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:33.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:04:06\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90       254\n",
            "           1       0.94      0.94      0.94       359\n",
            "           2       0.90      0.88      0.89       294\n",
            "           3       0.89      0.90      0.89       259\n",
            "           4       0.92      0.95      0.94       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:33.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:04:06\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.89       254\n",
            "           1       0.95      0.92      0.94       359\n",
            "           2       0.88      0.87      0.88       294\n",
            "           3       0.88      0.89      0.89       259\n",
            "           4       0.92      0.95      0.94       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.57\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:34.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:04:06\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90       254\n",
            "           1       0.94      0.93      0.94       359\n",
            "           2       0.88      0.89      0.88       294\n",
            "           3       0.90      0.88      0.89       259\n",
            "           4       0.91      0.95      0.93       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:33.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:04:07\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.89       254\n",
            "           1       0.94      0.93      0.94       359\n",
            "           2       0.90      0.88      0.89       294\n",
            "           3       0.89      0.89      0.89       259\n",
            "           4       0.91      0.95      0.93       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.58\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:33.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:04:06\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89       254\n",
            "           1       0.94      0.93      0.94       359\n",
            "           2       0.89      0.89      0.89       294\n",
            "           3       0.89      0.90      0.90       259\n",
            "           4       0.92      0.95      0.93       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.59\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    422.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    422.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    422.    Elapsed: 0:01:10.\n",
            "  Batch   160  of    422.    Elapsed: 0:01:33.\n",
            "  Batch   200  of    422.    Elapsed: 0:01:57.\n",
            "  Batch   240  of    422.    Elapsed: 0:02:20.\n",
            "  Batch   280  of    422.    Elapsed: 0:02:44.\n",
            "  Batch   320  of    422.    Elapsed: 0:03:07.\n",
            "  Batch   360  of    422.    Elapsed: 0:03:30.\n",
            "  Batch   400  of    422.    Elapsed: 0:03:54.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:04:06\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.87      0.89       254\n",
            "           1       0.95      0.94      0.94       359\n",
            "           2       0.90      0.88      0.89       294\n",
            "           3       0.89      0.90      0.89       259\n",
            "           4       0.91      0.95      0.93       334\n",
            "\n",
            "    accuracy                           0.91      1500\n",
            "   macro avg       0.91      0.91      0.91      1500\n",
            "weighted avg       0.91      0.91      0.91      1500\n",
            "\n",
            "  Accuracy: 0.91\n",
            "  Validation Loss: 0.59\n",
            "  Validation took: 0:00:09\n",
            "Saving model to /content/drive/MyDrive/NOTAM/notam_model/For_E_Section_dual\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:42:40 (h:mm:ss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Training Loss': 0.06546054635660654,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9152735562310031,\n",
              "  'Valid. Loss': 0.4517825287370447,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 1},\n",
              " {'Training Loss': 0.061316355764471385,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9119490881458967,\n",
              "  'Valid. Loss': 0.4892597906192408,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 2},\n",
              " {'Training Loss': 0.05553854526185692,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9139437689969605,\n",
              "  'Valid. Loss': 0.47997649917577173,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 3},\n",
              " {'Training Loss': 0.04785426815135721,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9127089665653496,\n",
              "  'Valid. Loss': 0.539570204477678,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 4},\n",
              " {'Training Loss': 0.04777612292975167,\n",
              "  'Training Time': '0:04:06',\n",
              "  'Valid. Accur.': 0.9140387537993921,\n",
              "  'Valid. Loss': 0.5380393766960317,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 5},\n",
              " {'Training Loss': 0.04053093064532594,\n",
              "  'Training Time': '0:04:06',\n",
              "  'Valid. Accur.': 0.9086246200607904,\n",
              "  'Valid. Loss': 0.5676709355791396,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 6},\n",
              " {'Training Loss': 0.03847350194845823,\n",
              "  'Training Time': '0:04:06',\n",
              "  'Valid. Accur.': 0.9107142857142858,\n",
              "  'Valid. Loss': 0.5631117476955393,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 7},\n",
              " {'Training Loss': 0.03752420538484938,\n",
              "  'Training Time': '0:04:07',\n",
              "  'Valid. Accur.': 0.9112841945288754,\n",
              "  'Valid. Loss': 0.580092128016986,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 8},\n",
              " {'Training Loss': 0.03665363393956451,\n",
              "  'Training Time': '0:04:06',\n",
              "  'Valid. Accur.': 0.9112841945288754,\n",
              "  'Valid. Loss': 0.5889588600517488,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 9},\n",
              " {'Training Loss': 0.03356435891455735,\n",
              "  'Training Time': '0:04:06',\n",
              "  'Valid. Accur.': 0.9120440729483283,\n",
              "  'Valid. Loss': 0.5895670855942955,\n",
              "  'Validation Time': '0:00:09',\n",
              "  'epoch': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjxTaVBdxNev"
      },
      "source": [
        "# BERT에서 multi input을 활용하는 방법들...\n",
        "\n",
        "Combining Categorical and Numerical Features with Text in BERT\n",
        "\n",
        "https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
        "\n",
        "Combine multiple sentences together during tokenization\n",
        "\n",
        "https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430\n",
        "\n",
        "\n",
        "How to add a CNN layer on top of BERT?\n",
        "\n",
        "https://datascience.stackexchange.com/questions/54412/how-to-add-a-cnn-layer-on-top-of-bert\n",
        "\n",
        "\n",
        "Multi-label Text Classification with BERT and PyTorch Lightning\n",
        "\n",
        "https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YZNBfQFYafc"
      },
      "source": [
        "from transformers import AlbertModel\n",
        "\n",
        "class Mi_model(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, n_classes):\n",
        "        super().__init__()\n",
        "        self.bert = AlbertModel.from_pretrained(model_name, return_dict=True)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,labels):\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        output = self.classifier(output.pooler_output)\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return [loss, output]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNPckRSncqgt",
        "outputId": "ee879144-4612-41fa-c5ee-e0e5c9dbc74f"
      },
      "source": [
        "mm = Mi_model(model_name='/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model',n_classes=6)\n",
        "print(mm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mi_model(\n",
            "  (bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(22089, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uiQsoUdddRV",
        "outputId": "f8e6b739-e575-493e-f193-4b3f28f6b681"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def __flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def __format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcLKFjenc9SR",
        "outputId": "810aec7f-4812-4f86-9c86-dad5f9e3b2af"
      },
      "source": [
        "\n",
        "epochs = 1\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "mm.cuda()\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(mm.named_parameters())\n",
        "\n",
        "print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(mm.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "#epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "    \n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    mm.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = __format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        #print(b_input_ids)\n",
        "        #print(b_input_mask)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        mm.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "\n",
        "        outputs = mm(b_input_ids,attention_mask=b_input_mask,labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(mm.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = __format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    mm.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        for l in batch[2]:\n",
        "            true_labels.append(l.item())        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = mm(b_input_ids, attention_mask=b_input_mask,labels=b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        for li in logits:\n",
        "            pred_labels.append(np.argmax(li))\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += __flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    print(\"  classification_report   \")    \n",
        "    print(classification_report(true_labels,pred_labels))\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = __format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(__format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 27 different named parameters.\n",
            "\n",
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:36.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:51.\n",
            "\n",
            "  Average training loss: 1.35\n",
            "  Training epcoh took: 0:00:53\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        30\n",
            "           1       0.51      0.63      0.56       181\n",
            "           2       0.57      0.34      0.43        99\n",
            "           3       0.83      0.14      0.24       109\n",
            "           4       0.89      0.81      0.85       175\n",
            "           5       0.70      0.90      0.79       448\n",
            "\n",
            "    accuracy                           0.68      1042\n",
            "   macro avg       0.58      0.47      0.48      1042\n",
            "weighted avg       0.68      0.68      0.64      1042\n",
            "\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 1.33\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:55 (h:mm:ss)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HzDAad-jvVe"
      },
      "source": [
        "class EmbeddingMapping():\n",
        "    \"\"\"\n",
        "    Helper class for handling categorical variables\n",
        "    An instance of this class should be defined for each categorical variable we want to use.\n",
        "    \"\"\"\n",
        "    def __init__(self, series):\n",
        "        # get a list of unique values\n",
        "        values = series.unique().tolist()\n",
        "        \n",
        "        # Set a dictionary mapping from values to integer value\n",
        "        # In our example this will be {'Mercaz': 1, 'Old North': 2, 'Florentine': 3}\n",
        "        self.embedding_dict = {value: int_value+1 for int_value, value in enumerate(values)}\n",
        "        \n",
        "        # The num_values will be used as the input_dim when defining the embedding layer. \n",
        "        # It will also be returned for unseen values \n",
        "        self.num_values = len(values) + 1\n",
        "\n",
        "    def get_mapping(self, value):\n",
        "        # If the value was seen in the training set, return its integer mapping\n",
        "        if value in self.embedding_dict:\n",
        "            return self.embedding_dict[value]\n",
        "        \n",
        "        # Else, return the same integer for unseen values\n",
        "        else:\n",
        "            return self.num_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot6rbRc0kIJX"
      },
      "source": [
        "  # code mapping\n",
        "  AS_mapping = EmbeddingMapping(df['A_LINE'])\n",
        "  df = df.assign(AS_mapping=df['A_LINE'].apply(AS_mapping.get_mapping))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "id": "BU4911lHkZxv",
        "outputId": "e5f3e846-2c57-42a8-ec7b-17d48747d090"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSG_ID</th>\n",
              "      <th>UPDATED_BY</th>\n",
              "      <th>UPDATED_DATE</th>\n",
              "      <th>E_SCORE</th>\n",
              "      <th>AE_SCORE</th>\n",
              "      <th>ALL_SCORE</th>\n",
              "      <th>Q_LINE</th>\n",
              "      <th>A_LINE</th>\n",
              "      <th>B_LINE</th>\n",
              "      <th>C_LINE</th>\n",
              "      <th>D_LINE</th>\n",
              "      <th>E_LINE</th>\n",
              "      <th>AS_mapping</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6824038</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:08</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6824038</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>YUXX/QOAXX/IV/BO/E/000/999/2537S13421E005</td>\n",
              "      <td>YBBB/YMMM</td>\n",
              "      <td>1905221600</td>\n",
              "      <td>1906191600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AIS (XX)\\nAUSTRALIAN DIGITAL AERONAUTICAL FLIG...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6816851</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:09</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6816851</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-07-01 8:18</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>LECM/QRDCA/IV/BO /W /000/100/3635N00636W022</td>\n",
              "      <td>LECM</td>\n",
              "      <td>1905270600</td>\n",
              "      <td>1905301559</td>\n",
              "      <td>27-28 0600-1900, 30 1201-1559</td>\n",
              "      <td>LED128 ACTIVATED. LOWER AND UPPER VERTICAL LIM...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6816333</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-06-08 9:11</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>KZOB/QMXLC/IV/BO/A/000/999/4213N08321W005</td>\n",
              "      <td>KDTW</td>\n",
              "      <td>1905211704</td>\n",
              "      <td>1905211800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY K5 CLSD</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10410</th>\n",
              "      <td>15966850</td>\n",
              "      <td>parkij</td>\n",
              "      <td>2021-08-01 10:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZNY/QMXLC/IV/BO/A/000/999/4038N07347W005</td>\n",
              "      <td>KJFK</td>\n",
              "      <td>1912311421</td>\n",
              "      <td>1912311600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY C BTN TWY C1 AND TWY V CLSD</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10411</th>\n",
              "      <td>15947262</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:06</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>LCCC/QWFLW/IV/M  /W /210/230/3335N03202E133</td>\n",
              "      <td>LCCC</td>\n",
              "      <td>2001040820</td>\n",
              "      <td>2001051150</td>\n",
              "      <td>0820-1150</td>\n",
              "      <td>AIR REFUELLING WILL TAKE PLACE ALONG \\nSUVAS-K...</td>\n",
              "      <td>468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10412</th>\n",
              "      <td>15962821</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-10 16:48</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>KZAU/QMRLC/IV/NBO/A/000/999/4257N08754W005</td>\n",
              "      <td>KMKE</td>\n",
              "      <td>1912310929</td>\n",
              "      <td>2001012359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RWY 01L/19R CLSD</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10413</th>\n",
              "      <td>15965583</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-06 13:52</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>2001071500</td>\n",
              "      <td>2001072000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY H(BTN E AND W) H1 H2 R(BTN H AND G)-CLSD D...</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10414</th>\n",
              "      <td>15962570</td>\n",
              "      <td>shbl1021</td>\n",
              "      <td>2021-08-08 14:49</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>RJJJ/QMXLC/IV/M/A/000/999/3533N13947E005</td>\n",
              "      <td>RJTT</td>\n",
              "      <td>1912311430</td>\n",
              "      <td>1912312100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TWY B1 B10 B11 B13 B14 T1 T11 T14-CLSD DUE TO ...</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10415 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         MSG_ID  ... AS_mapping\n",
              "0       6824038  ...          1\n",
              "1       6824038  ...          1\n",
              "2       6816851  ...          2\n",
              "3       6816851  ...          2\n",
              "4       6816333  ...          3\n",
              "...         ...  ...        ...\n",
              "10410  15966850  ...         72\n",
              "10411  15947262  ...        468\n",
              "10412  15962821  ...        108\n",
              "10413  15965583  ...         91\n",
              "10414  15962570  ...         91\n",
              "\n",
              "[10415 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI434HmpkgXd",
        "outputId": "e863ccad-446c-460b-da67-282ee53307e1"
      },
      "source": [
        "np.max(df['AS_mapping'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2-6MBDnvurg",
        "outputId": "2c3ee123-4fb2-48f4-b63e-a926469df093"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "x = torch.tensor([1, 0])\n",
        "F.one_hot(x, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNRobYmZv1od",
        "outputId": "b1bac44c-a077-4d09-ebf1-a528ddf6e060"
      },
      "source": [
        "x = torch.tensor([0,1,1,0,0,1,0])\n",
        "F.one_hot(x, num_classes=2).float()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9999, 0.0000],\n",
              "        [0.0000, 0.9999],\n",
              "        [0.0000, 0.9999],\n",
              "        [0.9999, 0.0000],\n",
              "        [0.9999, 0.0000],\n",
              "        [0.0000, 0.9999],\n",
              "        [0.9999, 0.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFz1FO-xxAFv",
        "outputId": "dd2300e2-6c27-4243-f4f9-2d3f92044fc3"
      },
      "source": [
        "x = torch.tensor([0,1,1,0,0,1,0])\n",
        "a = F.one_hot(x, num_classes=2).float()\n",
        "print(a)\n",
        "b = torch.tensor([[0],[0],[0],[1],[1],[1],[0]])\n",
        "print(b)\n",
        "torch.cat((a,b),1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.]])\n",
            "tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 0., 1.],\n",
              "        [0., 1., 1.],\n",
              "        [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ6tSkhfijYi",
        "outputId": "d8a343e3-7e3c-4d72-db4d-823f35112eba"
      },
      "source": [
        "num_classes = 4\n",
        "embedding_size = 10\n",
        "\n",
        "embedding = nn.Embedding(num_classes, embedding_size)\n",
        "\n",
        "class_vector = torch.tensor([1, 0, 3, 3, 2])\n",
        "\n",
        "embedded_classes = embedding(class_vector)\n",
        "embedded_classes.size() # => torch.Size([5, 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHde8s8ZluI_"
      },
      "source": [
        "# A Section for categorical, E Section for Text -> ACC:56%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t9gssQGwNC6"
      },
      "source": [
        "### embedding version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XNL_wk2l1la"
      },
      "source": [
        "from transformers import AlbertModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Mi_a_model(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, a_mapping_classes, n_classes):\n",
        "        super().__init__()\n",
        "        self.a_mapping_classes = a_mapping_classes\n",
        "        self.bert = AlbertModel.from_pretrained(model_name, return_dict=True)\n",
        "        #self.hidden = nn.Linear(self.a_mapping_classes,8)    \n",
        "        self.embedding = nn.Embedding(self.a_mapping_classes, 256)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size + 256, n_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,a_mapping,labels):\n",
        "        #ta = torch.tensor(np.eye(self.a_mapping_classes, dtype='uint8')[a_mapping])\n",
        "        #ta = F.one_hot(a_mapping, num_classes=self.a_mapping_classes).float()\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        map_output = self.embedding(a_mapping)\n",
        "        #print(output.pooler_output)\n",
        "        #print(map_output)\n",
        "        #print(torch.cat((output.pooler_output,map_output),1))\n",
        "        \n",
        "        output = self.classifier(torch.cat((output.pooler_output,map_output),1))\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return [loss, output]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMoj7pj6wR8Y"
      },
      "source": [
        "### one_hot version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu935W-SwWHT"
      },
      "source": [
        "from transformers import AlbertModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Mi_a_model(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, a_mapping_classes, n_classes):\n",
        "        super().__init__()\n",
        "        self.a_mapping_classes = a_mapping_classes\n",
        "        self.bert = AlbertModel.from_pretrained(model_name, return_dict=True)\n",
        "        self.hidden = nn.Linear(self.a_mapping_classes,128)\n",
        "        #self.ReLU = nn.ReLU()\n",
        "        #self.hidden2 = nn.Linear(self.a_mapping_classes*2,32)\n",
        "        \n",
        "        #self.embedding = nn.Embedding(self.a_mapping_classes, 256)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size + 128, n_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,a_mapping,labels):\n",
        "        #ta = torch.tensor(np.eye(self.a_mapping_classes, dtype='uint8')[a_mapping])\n",
        "        ta = F.one_hot(a_mapping, num_classes=self.a_mapping_classes).float()\n",
        "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        map_output = self.hidden(ta)\n",
        "        #map_output = self.ReLU(map_output)\n",
        "        #map_output = self.hidden2(map_output)\n",
        "        #print(output.pooler_output)\n",
        "        #print(map_output)\n",
        "        #print(torch.cat((output.pooler_output,map_output),1))\n",
        "        \n",
        "        output = self.classifier(torch.cat((output.pooler_output,map_output),1))\n",
        "        output = torch.sigmoid(output)\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(output, labels)\n",
        "        return [loss, output]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3taqxCPoVyC",
        "outputId": "dabca9ea-baf2-4a42-b643-b17e1732f778"
      },
      "source": [
        "mam = Mi_a_model(model_name='/content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model',a_mapping_classes=np.max(df['AS_mapping'])+1,n_classes=6)\n",
        "print(mam)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/NOTAM/albert_tokenizer_model_special/model and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mi_a_model(\n",
            "  (bert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(22089, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(514, 128)\n",
            "      (token_type_embeddings): Embedding(1, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (pooler_activation): Tanh()\n",
            "  )\n",
            "  (hidden): Linear(in_features=590, out_features=128, bias=True)\n",
            "  (classifier): Linear(in_features=896, out_features=6, bias=True)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usdn0MYootq2"
      },
      "source": [
        "# AE Transet,Valicationset을 다시 만든다.\n",
        "\n",
        "A Section만 Mapping value로..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SQ__mX2otQ5",
        "outputId": "9e16b09b-98c2-490e-e2f7-79d6e9de88bd"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "a_mappings = []\n",
        "labels = []\n",
        "# For every sentence...\n",
        "for row in df.iterrows():\n",
        "    # A Mapping value\n",
        "    A_mapping = row[1][12]    \n",
        "    # E section\n",
        "    sent_E = clean_text(str(row[1][11]).lower()) \n",
        "    # label for E Section\n",
        "    lb = row[1][4]+0\n",
        "    if lb == 6:\n",
        "        lb = 5\n",
        "    labels.append(lb)\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent_E,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        truncation = True,\n",
        "                )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    a_mappings.append(A_mapping)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "a_mappings = torch.tensor(a_mappings)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in input_ids[0].tolist()]))\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Training & Validation Split\n",
        "# Divide up our training set to use 90% for training and 10% for validation.\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, a_mappings, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2187: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokens (str)      : ['[CLS]', '▁ais', '▁(', 'xx', ')', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8)', '▁withdrawn', '.', '▁replacemen', 't', '▁', 'version', '▁australia', 'n', '▁dig', 'ital', '▁aerona', 'u', 'tical', '▁flight', '▁information', '▁file', '▁edition', '▁8', '▁', 'version', '▁2', '▁(', 'aus', 'da', 'fif', '▁', 'ed', '▁8', '▁v', '2)', '▁available', '▁via', '▁ais', '-', 'af', '▁intran', 'et', '.', '▁mil', '▁use', '▁only', '.', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Token IDs: tensor([    5,  3011,   158,   325,   150,   948,   121,  6600,  7011,  4561,\n",
            "          282,  5641,   228,   541,   794,  2656,   741,   158,  4850,  1537,\n",
            "        11342,   124,   156,   481,  1539,   123,  3107,   177,   124,  9695,\n",
            "          948,   121,  6600,  7011,  4561,   282,  5641,   228,   541,   794,\n",
            "         2656,   741,   124,  9695,   239,   158,  4850,  1537, 11342,   124,\n",
            "          156,   741,   324,   776,   546,   231,  3011,   133,  3859, 16232,\n",
            "         1786,   123,   491,   296,   218,   123,     6, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988,\n",
            "        21988, 21988, 21988, 21988, 21988, 21988, 21988, 21988])\n",
            "9,373 training samples\n",
            "1,042 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qjAOu3Dpydj"
      },
      "source": [
        "# A mapping, E Text에 대한 학습..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94t82bLip4TO",
        "outputId": "79e3d867-84ee-413b-b741-20dd2d28d4ab"
      },
      "source": [
        "\n",
        "epochs = 2\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "mam.cuda()\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(mam.named_parameters())\n",
        "\n",
        "print('The Albert model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(mam.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "#epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "    \n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    mam.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = __format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_a_mapping = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        #print(b_input_ids)\n",
        "        #print(b_input_mask)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        mam.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "\n",
        "        outputs = mam(b_input_ids,attention_mask=b_input_mask,a_mapping=b_a_mapping,labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(mam.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = __format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    mam.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_a_mapping = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        for l in batch[3]:\n",
        "            true_labels.append(l.item())        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = mam(b_input_ids, attention_mask=b_input_mask,a_mapping=b_a_mapping,labels=b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        for li in logits:\n",
        "            pred_labels.append(np.argmax(li))\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += __flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    print(\"  classification_report   \")    \n",
        "    print(classification_report(true_labels,pred_labels))\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = __format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(__format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Albert model has 29 different named parameters.\n",
            "\n",
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:45.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 1.56\n",
            "  Training epcoh took: 0:00:55\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.36      0.38       201\n",
            "           1       0.44      0.43      0.43       200\n",
            "           2       0.20      0.09      0.13        99\n",
            "           3       0.45      0.06      0.10        87\n",
            "           4       0.48      0.58      0.53       154\n",
            "           5       0.55      0.77      0.64       301\n",
            "\n",
            "    accuracy                           0.48      1042\n",
            "   macro avg       0.42      0.38      0.37      1042\n",
            "weighted avg       0.45      0.48      0.44      1042\n",
            "\n",
            "  Accuracy: 0.48\n",
            "  Validation Loss: 1.52\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    293.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    293.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    293.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    293.    Elapsed: 0:00:30.\n",
            "  Batch   200  of    293.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    293.    Elapsed: 0:00:45.\n",
            "  Batch   280  of    293.    Elapsed: 0:00:52.\n",
            "\n",
            "  Average training loss: 1.48\n",
            "  Training epcoh took: 0:00:55\n",
            "\n",
            "Running Validation...\n",
            "  classification_report   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.49      0.46       201\n",
            "           1       0.45      0.46      0.45       200\n",
            "           2       0.24      0.10      0.14        99\n",
            "           3       0.40      0.05      0.08        87\n",
            "           4       0.48      0.60      0.53       154\n",
            "           5       0.61      0.75      0.68       301\n",
            "\n",
            "    accuracy                           0.50      1042\n",
            "   macro avg       0.44      0.41      0.39      1042\n",
            "weighted avg       0.47      0.50      0.47      1042\n",
            "\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.49\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:54 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}